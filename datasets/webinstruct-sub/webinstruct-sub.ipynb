{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess WebInstructSub\n",
    "\n",
    "https://huggingface.co/datasets/TIGER-Lab/WebInstructSub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "DATASET_DIR = \".\"\n",
    "local_dir =  \"data\" \n",
    "DATA_CACHE_DIR = os.path.join(DATASET_DIR, local_dir)\n",
    "os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# download the dataset\n",
    "dataset = load_dataset(\"TIGER-Lab/WebInstructSub\", split=\"train\", cache_dir=DATA_CACHE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at a few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  What is an activated complex?\n",
      "Response length: 1366\n",
      "Assistant:\n",
      " The activated complex is formed when the reactants collide with each other and begin to rearrange\n",
      "their atoms and bonds to form the products. This process requires energy, which is why the activated\n",
      "complex has a higher energy than the reactants. The energy required to reach the activated complex\n",
      "is called the activation energy.  Once the activated complex is formed, it can either decompose back\n",
      "into the reactants or proceed to form the products. The probability of the activated complex\n",
      "decomposing back into the reactants is determined by the activation energy. If the activation energy\n",
      "is high, then the activated complex is more likely to decompose back into the reactants. If the\n",
      "activation energy is low, then the activated complex is more likely to proceed to form the products.\n",
      "The activated complex is a key concept in understanding chemical reactions. It helps to explain why\n",
      "some reactions occur quickly and others occur slowly. It also helps to explain why some reactions\n",
      "require a catalyst to occur. #### An activated complex is a high-energy, unstable intermediate state\n",
      "that forms during a chemical reaction. It is the transition state between the reactants and the\n",
      "products. The activated complex has a higher energy than both the reactants and the products, and it\n",
      "represents the maximum energy that must be overcome for the reaction to occur.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "User:  Why does gravity cause planets to be round?\n",
      "Response length: 1275\n",
      "Assistant:\n",
      " The gravitational force of a planet pulls matter towards its center. This force is strongest at the\n",
      "center of the planet and weakest at the surface. As a result, matter is pulled towards the center of\n",
      "the planet, causing it to take on a spherical shape.  A sphere is the shape with the lowest surface\n",
      "area for a given volume. This means that a planet with a spherical shape has the least amount of\n",
      "potential energy. Potential energy is the energy that an object has due to its position or\n",
      "condition. In the case of a planet, its potential energy is due to its gravitational force.  The\n",
      "lower the potential energy of a planet, the more stable it is. This is because a planet with a lower\n",
      "potential energy is less likely to change its shape. As a result, planets tend to be spherical in\n",
      "shape. #### Gravity causes planets to be round because a sphere is the shape with the lowest surface\n",
      "area for a given volume. This means that a planet with a spherical shape has the least amount of\n",
      "potential energy, which is the energy that an object has due to its position or condition. Gravity\n",
      "pulls matter towards the center of a planet, and this force is strongest at the center. As a result,\n",
      "matter is pulled towards the center of the planet, causing it to take on a spherical shape.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "User:  How many carbon atoms are present in a 5.85-gram sample of carbon tetrabromide (CBr4)?\n",
      "Response length: 765\n",
      "Assistant:\n",
      "There are approximately \\(1 \\times 10^{22}\\) carbon atoms in the given sample.  Explanation: To\n",
      "determine the number of carbon atoms, we first need to calculate the molar amount of CBr4. The molar\n",
      "mass of CBr4 is 331.63 g/mol. So, we have:  \\[ \\frac{5.85\\ g}{331.63\\ g/mol} = 0.0176\\ mol \\]  Since\n",
      "one molecule of CBr4 contains one carbon atom and four bromine atoms, there are:  \\[ 1 \\times\n",
      "0.0176\\ mol = 0.0176\\ mol\\ of\\ carbon\\ atoms \\]  Now, multiplying the molar quantity by Avogadro's\n",
      "number (6.022 Ã— 10^23 mol^(-1)) gives us the number of individual carbon atoms:  \\[ 0.0176\\ mol\n",
      "\\times 6.022 \\times 10^{23}\\ mol^{-1} = 1.06 \\times 10^{22}\\ carbon\\ atoms \\]  Therefore, there are\n",
      "approximately \\(1 \\times 10^{22}\\) carbon atoms in a 5.85-gram sample of CBr4.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "User:  Describe the properties and uses of the chemical compound with the formula #K_2CrO_4#.\n",
      "Response length: 154\n",
      "Assistant:\n",
      "Potassium chromate is a bright yellow salt with a transition metal oxidation state of +VI. It is\n",
      "commonly used as an oxidizing agent in organic chemistry.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "User:  How can the derivative of the function \\( y = (1+4x)^5(3+x-x^2)^8 \\) be found using the chain rule and the product rule?\n",
      "Response length: 872\n",
      "Assistant:\n",
      "The derivative \\( y' \\) can be calculated as follows:  1. Identify the two functions being\n",
      "multiplied: \\( f(x) = (1+4x)^5 \\) and \\( g(x) = (3+x-x^2)^8 \\). 2. Apply the chain rule to each\n",
      "function:    - \\( f'(x) = 5(1+4x)^4 \\cdot 4 \\) (since the derivative of \\( (1+4x)^5 \\) with respect\n",
      "to \\( 1+4x \\) is \\( 5(1+4x)^4 \\) and then multiply by the derivative of \\( 1+4x \\) with respect to\n",
      "\\( x \\), which is 4).    - \\( g'(x) = 8(3+x-x^2)^7 \\cdot (1-2x) \\) (using similar logic). 3. Apply\n",
      "the product rule, \\( (fg)' = f'g + fg' \\):    - \\( y' = f'(x)g(x) + f(x)g'(x) \\).    - \\( y' =\n",
      "[5(1+4x)^4 \\cdot 4] \\cdot (3+x-x^2)^8 + (1+4x)^5 \\cdot [8(3+x-x^2)^7 \\cdot (1-2x)] \\). 4. Simplify\n",
      "the expression:    - \\( y' = 20(1+4x)^4(3+x-x^2)^8 + 8(1+4x)^5(3+x-x^2)^7(1-2x) \\).  Thus, the\n",
      "derivative of the given function is \\( y' = 20(1+4x)^4(3+x-x^2)^8 + 8(1+4x)^5(3+x-x^2)^7(1-2x) \\).\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "it = iter(dataset)\n",
    "for i in range(5):\n",
    "    item = next(it)\n",
    "    print(f\"User: \", item['question'])\n",
    "    ans = item['answer']\n",
    "    print(f\"Response length: {len(ans)}\")\n",
    "    print(\"Assistant:\")\n",
    "    print(textwrap.fill(ans, width=100))\n",
    "    print(100*\"-\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sytem statistics:\n",
      "-----------------\n",
      "cpu count: 12\n",
      "\n",
      "dataset statistics\n",
      "------------------\n",
      "documents: 2,335,220\n",
      "docs_per_cpu: 194,602\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "num_cpus = os.cpu_count()\n",
    "print(f\"\"\"\n",
    "sytem statistics:\n",
    "-----------------\n",
    "cpu count: {num_cpus}\"\"\")\n",
    "total_docs = len(dataset)\n",
    "docs_per_cpu = int(math.ceil(total_docs/num_cpus))\n",
    "print(f\"\"\"\n",
    "dataset statistics\n",
    "------------------\n",
    "documents: {total_docs:,}\n",
    "docs_per_cpu: {docs_per_cpu:,}\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Preprocessing Operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 1,160,000\r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from functools import partial\n",
    "from multiprocessing import get_context\n",
    "\n",
    "import concurrent.futures as cf\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "enc = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n",
    "\n",
    "def count_tokens(dataset, enc, idx):\n",
    "    token_count = len(enc.encode(dataset[idx]['question']))\n",
    "    token_count += len(enc.encode(dataset[idx]['answer']))\n",
    "    return token_count\n",
    " \n",
    "ctx = get_context(\"fork\")\n",
    "with cf.ProcessPoolExecutor(max_workers = num_cpus - 2, mp_context=ctx) as ex:\n",
    "    start = time.time()\n",
    "    documents = 0\n",
    "    tokens = 0\n",
    "   \n",
    "    f = partial(count_tokens, dataset, enc)\n",
    "\n",
    "\n",
    "    for result in ex.map(f, range(len(dataset)), chunksize=docs_per_cpu//10):\n",
    "        documents += 1\n",
    "        tokens += result\n",
    "        documents % 1e4 == 0 and print(f\"processed {documents:,}\", end=\"\\r\")\n",
    "        \n",
    "    print(f\"processed documents in {time.time()-start:0.2f} seconds\")\n",
    "    print(f\"total tokens: {tokens:,}\")\n",
    "    print(f\"total documents: {documents:,}\")   \n",
    "    assert(documents == total_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "BLOCK_SIZE=2048\n",
    "\n",
    "def format_and_tokenize(dataset, tokenizer, idx,\n",
    "                        block_size=BLOCK_SIZE,\n",
    "                        system_prompt=\"You are a helpful assistant.\"):\n",
    "    question = dataset[idx][\"question\"].strip()\n",
    "    answer = dataset[idx][\"answer\"].strip()\n",
    "    \n",
    "    x = [\n",
    "        \"<|im_start|>system\\n\" + system_prompt + \"<|im_end|>\\n\",\n",
    "        \"<|im_start|>user\\n\" + question + \"<|im_end|>\\n\",\n",
    "        \"<|im_start|>assistant\\n\" \n",
    "        ] \n",
    "    x = \"\".join(x)\n",
    "    y = answer + \"<|im_end|>\"\n",
    "    tok_x = np.array(tokenizer.encode(x), dtype=np.uint16)\n",
    "    tok_y = np.array(tokenizer.encode(y), dtype=np.uint16)\n",
    "\n",
    "    # skip oversized examples\n",
    "    if len(tok_x) + len(tok_y) > BLOCK_SIZE:\n",
    "        return None\n",
    "\n",
    "    tokens = np.concatenate([\n",
    "        tok_x,\n",
    "        tok_y,\n",
    "        np.zeros(block_size - (len(tok_x)+len(tok_y)), dtype=np.uint16)\n",
    "    ])\n",
    "    attn_mask = np.concatenate([\n",
    "        np.ones(len(tok_x)+len(tok_y), dtype=np.uint16),\n",
    "        np.zeros(block_size - (len(tok_x)+len(tok_y)), dtype=np.uint16)\n",
    "    ])\n",
    "    loss_mask = np.concatenate([\n",
    "        np.zeros_like(tok_x, dtype=np.uint16),\n",
    "        np.ones_like(tok_y, dtype=np.uint16),\n",
    "        np.zeros(block_size - (len(tok_x)+len(tok_y)), dtype=np.uint16)\n",
    "    ])\n",
    "    result = np.stack([\n",
    "        tokens,\n",
    "        attn_mask,\n",
    "        loss_mask\n",
    "    ], axis=0)\n",
    "\n",
    "    return result\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1, 9690,  198, ...,    0,    0,    0],\n",
       "       [   1,    1,    1, ...,    0,    0,    0],\n",
       "       [   0,    0,    0, ...,    0,    0,    0]], dtype=uint16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = format_and_tokenize(dataset, enc, 5)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Preprocessing Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shard size: 48,650 documents, 298,905,600 tokens\n"
     ]
    }
   ],
   "source": [
    "TOTAL_SHARDS = 48\n",
    "SHARD_SIZE_DOCS = documents // TOTAL_SHARDS\n",
    "TOKENS_PER_DOC = 3 * BLOCK_SIZE\n",
    "SHARD_SIZE_TOKENS = SHARD_SIZE_DOCS * TOKENS_PER_DOC\n",
    "print(f\"shard size: {SHARD_SIZE_DOCS:,} documents, {SHARD_SIZE_TOKENS:,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 443,000 documents | generated 1,329,000 tokens | wrote 9 shards\r"
     ]
    }
   ],
   "source": [
    "output_dir = \"processed\"\n",
    "os.makedirs(os.path.join(DATASET_DIR, output_dir), exist_ok=True)\n",
    "\n",
    "def write_shard(shard, shard_idx, split=\"train\"):\n",
    "    f_path = os.path.join(DATA_CACHE_DIR, output_dir, f\"fineweb_edu_{split}_{shard_idx}\")\n",
    "    np.savez(f_path, shard)\n",
    "\n",
    "f = partial(format_and_tokenize, dataset, enc)\n",
    "\n",
    "ctx = get_context(\"fork\")\n",
    "with cf.ProcessPoolExecutor(max_workers = num_cpus, mp_context=ctx) as ex:\n",
    "    start = time.time()\n",
    "    \n",
    "    docs_processed = 0\n",
    "    shards_written = 0\n",
    "    tokens_generated = 0\n",
    "    shard_docs = 0\n",
    "\n",
    "    shard = np.empty((SHARD_SIZE_DOCS, 3, BLOCK_SIZE), dtype=np.uint16)\n",
    "    \n",
    "    for tokens in ex.map(f, range(len(dataset)), chunksize=docs_per_cpu//100):\n",
    "        if tokens is None:\n",
    "            continue\n",
    "\n",
    "        docs_processed += 1\n",
    "        tokens_generated += len(tokens)\n",
    "\n",
    "        if docs_processed % 1e3 == 0:\n",
    "            print(f\"processed {docs_processed:,} documents | generated {tokens_generated:,} tokens | wrote {shards_written} shards\", end=\"\\r\")\n",
    "\n",
    "        if shard_docs < SHARD_SIZE_DOCS:\n",
    "            shard[shard_docs, :, :] = tokens \n",
    "            shard_docs += 1\n",
    "        else:\n",
    "            write_shard(shard, shards_written)\n",
    "            shards_written += 1\n",
    "            shard_docs = 0\n",
    "    \n",
    "    write_shard(shard, shards_written) #write the final shard\n",
    "    shards_written += 1\n",
    "    print(f\"processed {docs_processed:,} documents | generated {tokens_generated:,} tokens | wrote {shards_written} shards\", end=\"\\r\")        \n",
    "    print(f\"finished in {time.time()-start:.2f} seconds\")\n",
    "    assert(docs_processed == total_docs)\n",
    "    print(f\"total shards written: {shards_written:,}\")\n",
    "    print(f\"total tokens: {tokens_generated:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
