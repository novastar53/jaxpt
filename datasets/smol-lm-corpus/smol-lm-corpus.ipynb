{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1952d75f",
   "metadata": {},
   "source": [
    "# Preprocess the SmolLM Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581df3eb",
   "metadata": {},
   "source": [
    "## Load the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce15e81-6f0d-4942-9ef9-e74a36361a2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf5213a3a70494289277dbdecfdff30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1979062df34b4febb3a64dd2286594d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5bcb2b691348359da417b639bfb14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/423 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f01dfe157e4e1188f6f287e7a1567f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436d0d6bfef94c8f8b1e247325c6715a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2361b9819f1440d91678e886db4f22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f38af5e14a94cb79dbc8d6def56481e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset, interleave_datasets\n",
    "\n",
    "dataset_paths=[\"HuggingFaceTB/smollm-corpus\",\n",
    "                \"HuggingFaceTB/smollm-corpus\",\n",
    "                \"HuggingFaceTB/smollm-corpus\"]\n",
    "dataset_names=[\"cosmopedia-v2\",\n",
    "                \"python-edu\",\n",
    "                \"fineweb-edu-dedup\"]\n",
    "probabilities=[0.111, 0.016 , 0.873]\n",
    "\n",
    "local_dir =  \"train-gpt2-data\" \n",
    "DATA_CACHE_DIR = os.path.join(\"/lambda/nfs\", local_dir)\n",
    "os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "dataset_objs = []\n",
    "for ds_path, ds_name in zip(dataset_paths, dataset_names, strict=False):\n",
    "    dataset_objs.append(\n",
    "        load_dataset(ds_path, ds_name, split=\"train\", cache_dir=DATA_CACHE_DIR)\n",
    "    )\n",
    "#ds = interleave_datasets(\n",
    "#    dataset_objs, probabilities=probabilities, seed=1337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ba93e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'text', 'token_length', 'audience', 'format', 'seed_data'],\n",
      "    num_rows: 39134000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['blob_id', 'repo_name', 'path', 'length_bytes', 'score', 'int_score'],\n",
      "    num_rows: 7678448\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'id', 'metadata'],\n",
      "    num_rows: 190168005\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "for dataset in dataset_objs:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26ffc11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blob_id': '55884a59514464a78f8002779532a7eb01b8331c',\n",
       " 'repo_name': 'sudajzp/jzp-s-python',\n",
       " 'path': '/FBNQ_py/Fib_circle.py',\n",
       " 'length_bytes': 854,\n",
       " 'score': 3.84375,\n",
       " 'int_score': 4}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_objs[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7822d0",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fad3ed75-c2ed-4544-b2e9-0ec28799af4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7462f736eb1b41c791603b3e56d9ed21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a610ff44844159b88c68e9b2300cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc94beea626450c861db3479979ed5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a82f4987b5145de9697e8888728b7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6f849df7214e82b0a16e6d3a808228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sytem statistics:\n",
      "-----------------\n",
      "cpu count: 88\n",
      "\n",
      "dataset statistics\n",
      "------------------\n",
      "documents: 570,504,015\n",
      "docs_per_cpu: 6,483,001\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "from functools import partial\n",
    "import concurrent.futures as cf\n",
    "\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#import tiktoken\n",
    "#enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-360M\")\n",
    "\n",
    "num_cpus = os.cpu_count()\n",
    "print(f\"\"\"\n",
    "sytem statistics:\n",
    "-----------------\n",
    "cpu count: {num_cpus}\"\"\")\n",
    "total_docs = sum([len(dataset) for datasset in dataset_objs])\n",
    "docs_per_cpu = int(math.ceil(total_docs/num_cpus))\n",
    "print(f\"\"\"\n",
    "dataset statistics\n",
    "------------------\n",
    "documents: {total_docs:,}\n",
    "docs_per_cpu: {docs_per_cpu:,}\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c20d4",
   "metadata": {},
   "source": [
    "## Dummy Preprocessing Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caa2a0c4-85d4-445a-856b-3abce9a13653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 39,134,000\r"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/concurrent/futures/process.py\", line 264, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/concurrent/futures/process.py\", line 213, in _process_chunk\n    return [fn(*args) for args in chunk]\n            ^^^^^^^^^\n  File \"/tmp/ipykernel_55426/1527579439.py\", line 2, in count_tokens\n    tokens = _tokenizer.encode(dataset[idx]['text'])\n                               ~~~~~~~~~~~~^^^^^^^^\nKeyError: 'text'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m dataset_objs: \n\u001b[32m     12\u001b[39m     f = partial(count_tokens, dataset, tokenizer)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocs_per_cpu\u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/concurrent/futures/process.py:636\u001b[39m, in \u001b[36m_chain_from_iterable_of_lists\u001b[39m\u001b[34m(iterable)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[32m    631\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[33;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[33;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[32m    634\u001b[39m \u001b[33;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[32m    635\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43melement\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'text'"
     ]
    }
   ],
   "source": [
    "def count_tokens(dataset, _tokenizer, idx):\n",
    "    tokens = _tokenizer.encode(dataset[idx]['text'])\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "with cf.ProcessPoolExecutor(max_workers = num_cpus) as ex:\n",
    "    start = time.time()\n",
    "    documents = 0\n",
    "    tokens = 0\n",
    "\n",
    "    for dataset in dataset_objs: \n",
    "        f = partial(count_tokens, dataset, tokenizer)\n",
    "        for result in ex.map(f, range(len(dataset)), chunksize=docs_per_cpu//100):\n",
    "            documents += 1\n",
    "            tokens += result\n",
    "            documents % 1e3 == 0 and print(f\"processed {documents:,}\", end=\"\\r\")\n",
    "            \n",
    "    print(f\"processed documents in {time.time()-start:0.2f} seconds\")\n",
    "    print(f\"total tokens: {tokens:,}\")\n",
    "    print(f\"total documents: {documents:,}\")   \n",
    "    assert(documents == total_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd2e88",
   "metadata": {},
   "source": [
    "## Actual Preprocessing Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9802fa-cb02-4f92-a39c-0a0d0cadd11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARD_SIZE = int(1e8)\n",
    "output_dir = \"processed\"\n",
    "os.makedirs(os.path.join(DATA_CACHE_DIR, output_dir), exist_ok=True)\n",
    "\n",
    "def write_shard(shard, shard_idx):\n",
    "    if shard_idx % 100 == 0:\n",
    "        split = \"valid\"\n",
    "    else:\n",
    "        split = \"train\"\n",
    "    \n",
    "    \n",
    "    f_path = os.path.join(DATA_CACHE_DIR, output_dir, f\"smol_lm_corpus_{split}_{shard_idx}\")\n",
    "    np.savez(f_path, shard)\n",
    "\n",
    "def tokenize(dataset, encoder, idx):\n",
    "    eot = encoder._special_tokens['<|endoftext|>']\n",
    "    tokens = [eot] + encoder.encode(dataset[idx]['text'])\n",
    "    return tokens\n",
    "\n",
    "f = partial(tokenize, dataset, enc)\n",
    "\n",
    "with cf.ProcessPoolExecutor(max_workers = num_cpus) as ex:\n",
    "    start = time.time()\n",
    "    \n",
    "    docs_processed = 0\n",
    "    shards_written = 0\n",
    "    tokens_generated = 0\n",
    "    shard_token_count = 0\n",
    "\n",
    "    shard = np.empty((SHARD_SIZE,), dtype=np.uint16)\n",
    "    \n",
    "    for tokens in ex.map(f, range(len(dataset)), chunksize=docs_per_cpu//100):\n",
    "        docs_processed += 1\n",
    "        tokens_generated += len(tokens)\n",
    "\n",
    "        if docs_processed % 1e4 == 0:\n",
    "            print(f\"processed {docs_processed:,} documents | generated {tokens_generated:,} tokens | wrote {shards_written} shards\", end=\"\\r\")\n",
    "\n",
    "        if shard_token_count + len(tokens) < SHARD_SIZE:\n",
    "            shard[shard_token_count:shard_token_count + len(tokens)] = tokens \n",
    "            shard_token_count += len(tokens)\n",
    "        else:\n",
    "            remainder = SHARD_SIZE - shard_token_count\n",
    "            shard[shard_token_count:shard_token_count + remainder] = tokens[:remainder]\n",
    "            write_shard(shard, shards_written)\n",
    "            shards_written += 1\n",
    "            \n",
    "            shard[:len(tokens) - remainder] = tokens[remainder:]\n",
    "            shard_token_count = len(tokens) - remainder\n",
    "    \n",
    "    write_shard(shard, shards_written) #write the final shard\n",
    "    shards_written += 1\n",
    "    print(f\"processed {docs_processed:,} documents | generated {tokens_generated:,} tokens | wrote {shards_written} shards\", end=\"\\r\")        \n",
    "    print(f\"finished in {time.time()-start:.2f} seconds\")\n",
    "    assert(docs_processed == total_docs)\n",
    "    print(f\"total shards written: {shards_written:,}\")\n",
    "    print(f\"total tokens: {tokens_generated:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
