{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1952d75f",
   "metadata": {},
   "source": [
    "# Preprocess the SmolLM Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581df3eb",
   "metadata": {},
   "source": [
    "## Load the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce15e81-6f0d-4942-9ef9-e74a36361a2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset, interleave_datasets\n",
    "\n",
    "dataset_paths=[\"HuggingFaceTB/smollm-corpus\",\n",
    "                \"HuggingFaceTB/smollm-corpus\",\n",
    "                \"HuggingFaceTB/smollm-corpus\"]\n",
    "dataset_names=[\"cosmopedia-v2\",\n",
    "                \"python-edu\",\n",
    "                \"fineweb-edu-dedup\"]\n",
    "probabilities=[0.111, 0.016 , 0.873]\n",
    "\n",
    "local_dir =  \"datasets\" \n",
    "DATA_CACHE_DIR = os.path.join(\"/Users/vikram/dev/jaxpt\", local_dir)\n",
    "os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "dataset_objs = []\n",
    "for ds_path, ds_name in zip(dataset_paths, dataset_names, strict=False):\n",
    "    dataset_objs.append(\n",
    "        load_dataset(ds_path, ds_name, split=\"train\", cache_dir=DATA_CACHE_DIR)\n",
    "    )\n",
    "ds = interleave_datasets(\n",
    "    dataset_objs, probabilities=probabilities, seed=1337)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7822d0",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3ed75-c2ed-4544-b2e9-0ec28799af4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method PreTrainedTokenizerBase.encode of GPT2TokenizerFast(name_or_path='HuggingFaceTB/SmolLM-360M', vocab_size=49152, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<|endoftext|>', '<|im_start|>', '<|im_end|>', '<repo_name>', '<reponame>', '<file_sep>', '<filename>', '<gh_stars>', '<issue_start>', '<issue_comment>', '<issue_closed>', '<jupyter_start>', '<jupyter_text>', '<jupyter_code>', '<jupyter_output>', '<jupyter_script>', '<empty_output>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<repo_name>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"<reponame>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t5: AddedToken(\"<file_sep>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t6: AddedToken(\"<filename>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t7: AddedToken(\"<gh_stars>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t8: AddedToken(\"<issue_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t9: AddedToken(\"<issue_comment>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t10: AddedToken(\"<issue_closed>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t11: AddedToken(\"<jupyter_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t12: AddedToken(\"<jupyter_text>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t13: AddedToken(\"<jupyter_code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t14: AddedToken(\"<jupyter_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t15: AddedToken(\"<jupyter_script>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t16: AddedToken(\"<empty_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")>\n",
      "\n",
      "sytem statistics:\n",
      "-----------------\n",
      "cpu count: 12\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     14\u001b[39m num_cpus = os.cpu_count()\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[33msytem statistics:\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[33m-----------------\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[33mcpu count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cpus\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\"\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m total_docs = \u001b[38;5;28mlen\u001b[39m(\u001b[43mdataset\u001b[49m)\n\u001b[32m     20\u001b[39m docs_per_cpu = \u001b[38;5;28mint\u001b[39m(math.ceil(total_docs/num_cpus))\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[33mdataset statistics\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[33m------------------\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[33mdocuments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_docs\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33mdocs_per_cpu: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocs_per_cpu\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\"\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "from functools import partial\n",
    "import concurrent.futures as cf\n",
    "\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#import tiktoken\n",
    "#enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-360M\")\n",
    "\n",
    "num_cpus = os.cpu_count()\n",
    "print(f\"\"\"\n",
    "sytem statistics:\n",
    "-----------------\n",
    "cpu count: {num_cpus}\"\"\")\n",
    "total_docs = len(dataset)\n",
    "docs_per_cpu = int(math.ceil(total_docs/num_cpus))\n",
    "print(f\"\"\"\n",
    "dataset statistics\n",
    "------------------\n",
    "documents: {total_docs:,}\n",
    "docs_per_cpu: {docs_per_cpu:,}\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c20d4",
   "metadata": {},
   "source": [
    "## Dummy Preprocessing Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2a0c4-85d4-445a-856b-3abce9a13653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(dataset, enc, idx):\n",
    "    tokens = enc.encode(dataset[idx]['text'])\n",
    "    return len(tokens)\n",
    "\n",
    "f = partial(count_tokens, dataset, enc)\n",
    "\n",
    "with cf.ProcessPoolExecutor(max_workers = num_cpus) as ex:\n",
    "    start = time.time()\n",
    "    documents = 0\n",
    "    tokens = 0\n",
    "    \n",
    "    for result in ex.map(f, range(len(dataset)), chunksize=docs_per_cpu//10):\n",
    "        documents += 1\n",
    "        tokens += result\n",
    "        documents % 1e4 == 0 and print(f\"processed {documents:,}\", end=\"\\r\")\n",
    "        \n",
    "    print(f\"processed documents in {time.time()-start:0.2f} seconds\")\n",
    "    print(f\"total tokens: {tokens:,}\")\n",
    "    print(f\"total documents: {documents:,}\")   \n",
    "    assert(documents == total_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd2e88",
   "metadata": {},
   "source": [
    "## Actual Preprocessing Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9802fa-cb02-4f92-a39c-0a0d0cadd11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHARD_SIZE = int(1e8)\n",
    "output_dir = \"processed\"\n",
    "os.makedirs(os.path.join(DATA_CACHE_DIR, output_dir), exist_ok=True)\n",
    "\n",
    "def write_shard(shard, shard_idx):\n",
    "    if shard_idx % 100 == 0:\n",
    "        split = \"valid\"\n",
    "    else:\n",
    "        split = \"train\"\n",
    "    \n",
    "    \n",
    "    f_path = os.path.join(DATA_CACHE_DIR, output_dir, f\"smol_lm_corpus_{split}_{shard_idx}\")\n",
    "    np.savez(f_path, shard)\n",
    "\n",
    "def tokenize(dataset, encoder, idx):\n",
    "    eot = encoder._special_tokens['<|endoftext|>']\n",
    "    tokens = [eot] + encoder.encode(dataset[idx]['text'])\n",
    "    return tokens\n",
    "\n",
    "f = partial(tokenize, dataset, enc)\n",
    "\n",
    "with cf.ProcessPoolExecutor(max_workers = num_cpus) as ex:\n",
    "    start = time.time()\n",
    "    \n",
    "    docs_processed = 0\n",
    "    shards_written = 0\n",
    "    tokens_generated = 0\n",
    "    shard_token_count = 0\n",
    "\n",
    "    shard = np.empty((SHARD_SIZE,), dtype=np.uint16)\n",
    "    \n",
    "    for tokens in ex.map(f, range(len(dataset)), chunksize=docs_per_cpu//100):\n",
    "        docs_processed += 1\n",
    "        tokens_generated += len(tokens)\n",
    "\n",
    "        if docs_processed % 1e4 == 0:\n",
    "            print(f\"processed {docs_processed:,} documents | generated {tokens_generated:,} tokens | wrote {shards_written} shards\", end=\"\\r\")\n",
    "\n",
    "        if shard_token_count + len(tokens) < SHARD_SIZE:\n",
    "            shard[shard_token_count:shard_token_count + len(tokens)] = tokens \n",
    "            shard_token_count += len(tokens)\n",
    "        else:\n",
    "            remainder = SHARD_SIZE - shard_token_count\n",
    "            shard[shard_token_count:shard_token_count + remainder] = tokens[:remainder]\n",
    "            write_shard(shard, shards_written)\n",
    "            shards_written += 1\n",
    "            \n",
    "            shard[:len(tokens) - remainder] = tokens[remainder:]\n",
    "            shard_token_count = len(tokens) - remainder\n",
    "    \n",
    "    write_shard(shard, shards_written) #write the final shard\n",
    "    shards_written += 1\n",
    "    print(f\"processed {docs_processed:,} documents | generated {tokens_generated:,} tokens | wrote {shards_written} shards\", end=\"\\r\")        \n",
    "    print(f\"finished in {time.time()-start:.2f} seconds\")\n",
    "    assert(docs_processed == total_docs)\n",
    "    print(f\"total shards written: {shards_written:,}\")\n",
    "    print(f\"total tokens: {tokens_generated:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
