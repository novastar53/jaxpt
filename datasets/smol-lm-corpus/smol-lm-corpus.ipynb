{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1952d75f",
   "metadata": {},
   "source": [
    "# Preprocess the SmolLM Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581df3eb",
   "metadata": {},
   "source": [
    "## Load the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ce15e81-6f0d-4942-9ef9-e74a36361a2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5351cceff23a4a55af609ce7ea903e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2acfc92469ea4ca59d1362c06e26fde4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b08a483229a4e06abcf4837c1e9e3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/423 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bcd67de66d949c59a1a6be33597c965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858f862aafa843c2a26aff788440d2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f343cbcc46416293c6c0a522e2c2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/1906 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset, interleave_datasets\n",
    "\n",
    "dataset_paths=[\"HuggingFaceTB/smollm-corpus\",\n",
    "                \"HuggingFaceTB/smollm-corpus\"]\n",
    "dataset_names=[\"cosmopedia-v2\",\n",
    "                \"fineweb-edu-dedup\"]\n",
    "\n",
    "# cosmopedia-v2, python-edu, fineweb-edu-dedup\n",
    "#probabilities=[0.111, 0.016 , 0.873]\n",
    "\n",
    "local_dir =  \"train-gpt2-data\" \n",
    "DATA_CACHE_DIR = os.path.join(\"/lambda/nfs\", local_dir)\n",
    "os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "dataset_objs = []\n",
    "for ds_path, ds_name in zip(dataset_paths, dataset_names, strict=False):\n",
    "    dataset_objs.append(\n",
    "        load_dataset(ds_path, ds_name, split=\"train\", cache_dir=DATA_CACHE_DIR)\n",
    "    )\n",
    "#ds = interleave_datasets(\n",
    "#    dataset_objs, probabilities=probabilities, seed=1337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ffc11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe74bc47530848f4859b001a5fc46f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b326eeaf8c147238568016770d5912d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=80):   0%|          | 0/7678448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: content/39c3e5b85cc678d1d54b4d93a55271c51d54126c\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61177acdab52432fb20443c0589bd447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7678448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blob_id': '55884a59514464a78f8002779532a7eb01b8331c', 'repo_name': 'sudajzp/jzp-s-python', 'path': '/FBNQ_py/Fib_circle.py', 'length_bytes': 854, 'score': 3.84375, 'int_score': 4, 'text': \"#coding utf-8\\n'''\\n斐波那契数列-循环法\\n'''\\ndef Fib_circle():\\n    while True:   # 去掉while循环，只用for循环\\n        num_1 = 0\\n        num_2 = 1\\n        fib_array = [0] # 用于存储计算出的FB数列值\\n        m = input('你想要查找的起始项：')\\n        n = input('你想要查找的结束项：')\\n        if m.isdigit() and n.isdigit():   # 在这个实现函数中，不要进行检验。每个函数只做一个事情\\n            m = int(m) # 将输入化为整数型\\n            n = int(n)\\n            for i in range(n):\\n                num_1, num_2 = num_2, num_1 + num_2\\n                fib_array.append(num_1)\\n            print(f'你要查找的数列为{list(enumerate(fib_array[m:], m))}')\\n            break\\n        else:\\n            print('请输入有效的正整数')\\n\\nif __name__ == '__main__':\\n    Fib_circle()\\n\", 'download_success': True}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import gzip\n",
    "from datasets import load_dataset\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=\"AKIAYRLTTJU6SKOWZEOE\",\n",
    "    aws_secret_access_key=\"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\")\n",
    "s3 = session.client(\"s3\")\n",
    "num_proc = 80\n",
    "bucket_name = \"softwareheritage\"\n",
    "\n",
    "def download_contents(blob_id):\n",
    "    key = f\"content/{blob_id}\"\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "        with gzip.GzipFile(fileobj=obj['Body']) as fin:\n",
    "            content = fin.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "        return {\"text\": content, \"download_success\": True}\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'NoSuchKey':\n",
    "            print(f\"File not found: {key}\")\n",
    "            return {\"text\": \"\", \"download_success\": False}\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "#ds = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"python-edu\", split=\"train\", num_proc=num_proc, cache_dir=DATA_CACHE_DIR)\n",
    "#ds = ds.map(download_contents, input_columns=\"blob_id\", num_proc=num_proc, batched=False)\n",
    "\n",
    "# Filter out failed downloads\n",
    "ds = ds.filter(lambda x: x['download_success'])\n",
    "\n",
    "# Optionally, print the first example to verify the data\n",
    "print(ds[0])\n",
    "\n",
    "dataset_objs.append(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0c15fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f917e787dc345c58b390ce8115e2f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/27 shards):   0%|          | 0/7678447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.save_to_disk('/lambda/nfs/train-gpt2-data/smollm-corpus/python-edu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "066642dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538f4795adaf473bb6bfee265ec294ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk('/lambda/nfs/train-gpt2-data/smollm-corpus/python-edu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7822d0",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3ed75-c2ed-4544-b2e9-0ec28799af4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sytem statistics:\n",
      "-----------------\n",
      "cpu count: 88\n",
      "\n",
      "dataset statistics\n",
      "------------------\n",
      "docs_per_dataset: [39134000, 190168005, 7678447]\n",
      "documents: 236,980,452\n",
      "docs_per_cpu: 2,692,960\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "from functools import partial\n",
    "import concurrent.futures as cf\n",
    "\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#import tiktoken\n",
    "#enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-360M\")\n",
    "\n",
    "num_cpus = os.cpu_count()\n",
    "print(f\"\"\"\n",
    "sytem statistics:\n",
    "-----------------\n",
    "cpu count: {num_cpus}\"\"\")\n",
    "docs = [len(dataset) for dataset in dataset_objs]\n",
    "total_docs = sum(docs)\n",
    "docs_per_cpu = int(math.ceil(total_docs/num_cpus))\n",
    "print(f\"\"\"\n",
    "dataset statistics\n",
    "------------------\n",
    "docs_per_dataset: {docs}\n",
    "documents: {total_docs:,}\n",
    "docs_per_cpu: {docs_per_cpu:,}\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c20d4",
   "metadata": {},
   "source": [
    "## Dummy Preprocessing Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa2a0c4-85d4-445a-856b-3abce9a13653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed documents in 22565.92 seconds79\n",
      "total tokens: 227,557,089,774\n",
      "total documents: 236,980,452\n"
     ]
    }
   ],
   "source": [
    "def count_tokens(dataset, _tokenizer, idx):\n",
    "    tokens = _tokenizer.encode(dataset[idx]['text'])\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "with cf.ProcessPoolExecutor(max_workers = num_cpus) as ex:\n",
    "    start = time.time()\n",
    "    documents = 0\n",
    "    tokens = 0\n",
    "\n",
    "    for dataset in dataset_objs: \n",
    "        f = partial(count_tokens, dataset, tokenizer)\n",
    "        for result in ex.map(f, range(len(dataset)), chunksize=docs_per_cpu//10):\n",
    "            documents += 1\n",
    "            tokens += result\n",
    "            elapsed = time.time() - start\n",
    "            documents % 1e3 == 0 and (\n",
    "                print(f\"processed {documents:,} | docs/s {documents/elapsed:0.4f}\", end=\"\\r\")\n",
    "            )\n",
    "            \n",
    "    print(f\"processed documents in {time.time()-start:0.2f} seconds\")\n",
    "    print(f\"total tokens: {tokens:,}\")\n",
    "    print(f\"total documents: {documents:,}\")   \n",
    "    assert(documents == total_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd2e88",
   "metadata": {},
   "source": [
    "## Actual Preprocessing Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482b7b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(tokenizer.additional_special_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9802fa-cb02-4f92-a39c-0a0d0cadd11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOT_TOKEN = 0\n",
    "SHARD_SIZE = int(1e8)\n",
    "output_dir = \"processed\"\n",
    "os.makedirs(os.path.join(DATA_CACHE_DIR, output_dir), exist_ok=True)\n",
    "\n",
    "def write_shard(d_idx, shard, shard_idx):\n",
    "    if shard_idx % 100 == 0:\n",
    "        split = \"valid\"\n",
    "    else:\n",
    "        split = \"train\"\n",
    "    f_path = os.path.join(DATA_CACHE_DIR, output_dir, f\"smol_lm_corpus_{d_idx}_{split}_{shard_idx}\")\n",
    "    np.savez(f_path, shard)\n",
    "\n",
    "\n",
    "def tokenize(dataset, encoder, idx):\n",
    "    tokens = [EOT_TOKEN] + encoder.encode(dataset[idx]['text'])\n",
    "    return tokens\n",
    "\n",
    "\n",
    "for d_idx, dataset in enumerate(dataset_objs):\n",
    "    f = partial(tokenize, dataset, tokenizer)\n",
    "\n",
    "    with cf.ProcessPoolExecutor(max_workers = num_cpus) as ex:\n",
    "        start = time.time()\n",
    "        \n",
    "        docs_processed = 0\n",
    "        shards_written = 0\n",
    "        tokens_generated = 0\n",
    "        shard_token_count = 0\n",
    "\n",
    "        shard = np.empty((SHARD_SIZE,), dtype=np.uint16)\n",
    "        \n",
    "        for tokens in ex.map(f, range(len(dataset)), chunksize=docs_per_cpu//200):\n",
    "            docs_processed += 1\n",
    "            tokens_generated += len(tokens)\n",
    "\n",
    "            if docs_processed % 1e4 == 0:\n",
    "                print(f\"processed {docs_processed:,} documents | generated {tokens_generated:,} tokens | wrote {shards_written} shards\", end=\"\\r\")\n",
    "\n",
    "            if shard_token_count + len(tokens) < SHARD_SIZE:\n",
    "                shard[shard_token_count:shard_token_count + len(tokens)] = tokens \n",
    "                shard_token_count += len(tokens)\n",
    "            else:\n",
    "                remainder = SHARD_SIZE - shard_token_count\n",
    "                shard[shard_token_count:shard_token_count + remainder] = tokens[:remainder]\n",
    "                write_shard(d_idx, shard, shards_written)\n",
    "                shards_written += 1\n",
    "                \n",
    "                shard[:len(tokens) - remainder] = tokens[remainder:]\n",
    "                shard_token_count = len(tokens) - remainder\n",
    "        \n",
    "        write_shard(d_idx, shard, shards_written) #write the final shard\n",
    "        shards_written += 1\n",
    "        print(f\"processed {docs_processed:,} documents | generated {tokens_generated:,} tokens | wrote {shards_written} shards\", end=\"\\r\")        \n",
    "        print(f\"finished in {time.time()-start:.2f} seconds\")\n",
    "        assert(docs_processed == total_docs)\n",
    "        print(f\"total shards written: {shards_written:,}\")\n",
    "        print(f\"total tokens: {tokens_generated:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.8 (jaxpt)",
   "language": "python",
   "name": "jaxpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
