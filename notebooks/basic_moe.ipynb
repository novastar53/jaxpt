{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Dense Mixture of Experts Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "from typing import Any\n",
    "\n",
    "class Router(nnx.Module):\n",
    "    def __init__(self, dim: int, num_experts: int, *, rngs: nnx.Rngs):\n",
    "        self.w1 = nnx.Linear(dim, num_experts, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        return self.w1(x)\n",
    "\n",
    "class Expert(nnx.Module):\n",
    "    def __init__(self, dim: int, *, rngs: nnx.Rngs):\n",
    "        self.linear = nnx.Linear(dim, dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        return self.linear(x)\n",
    "\n",
    "class SimpleMoE(nnx.Module):\n",
    "    def __init__(self, dim: int, *, rngs: nnx.Rngs):\n",
    "        num_experts = 8\n",
    "        self.router = Router(dim, num_experts=num_experts, rngs=rngs)\n",
    "        self.experts = [\n",
    "            Expert(dim, rngs=rngs)\n",
    "            for _ in range(num_experts)\n",
    "        ]\n",
    "        self.top_k = 8\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        gate_logits = self.router(x)       \n",
    "        top_k_logits, expert_indices = jax.lax.top_k(gate_logits, self.top_k)\n",
    "        zeros = jnp.full_like(gate_logits, float('-inf'))\n",
    "        sparse_logits = jnp.put_along_axis(\n",
    "            zeros, expert_indices, top_k_logits, axis=-1, inplace=False\n",
    "        )\n",
    "        expert_weights = jax.nn.softmax(sparse_logits, axis=-1)\n",
    "\n",
    "        mean_gates = jnp.mean(gate_logits, axis=0)\n",
    "        lb_loss = gate_logits.shape[1] * jnp.sum(mean_gates ** 2)\n",
    "\n",
    "        outputs = [ e(x) for e in self.experts ]\n",
    "\n",
    "        result = jnp.zeros_like(x)\n",
    "\n",
    "        for i, o in enumerate(outputs):\n",
    "            result += (o * expert_weights[:, i:i+1])\n",
    "           \n",
    "        return result, lb_loss, expert_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(D):\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m         loss, gates, grads = \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m1000\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     39\u001b[39m             \u001b[38;5;28mprint\u001b[39m(i, loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/jaxpt/.venv/lib/python3.12/site-packages/flax/nnx/transforms/compilation.py:350\u001b[39m, in \u001b[36mjit.<locals>.jit_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m graph.update_context(jit_wrapper):\n\u001b[32m    341\u001b[39m   pure_args, pure_kwargs = extract.to_tree(\n\u001b[32m    342\u001b[39m     (args, kwargs),\n\u001b[32m    343\u001b[39m     prefix=(in_shardings, kwarg_shardings)\n\u001b[32m   (...)\u001b[39m\u001b[32m    348\u001b[39m     ctxtag=jit_wrapper,\n\u001b[32m    349\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m   pure_args_out, pure_kwargs_out, pure_out = \u001b[43mjitted_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43mpure_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpure_kwargs\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    353\u001b[39m   _args_out, _kwargs_out, out = extract.from_tree(\n\u001b[32m    354\u001b[39m     (pure_args_out, pure_kwargs_out, pure_out),\n\u001b[32m    355\u001b[39m     merge_fn=_jit_merge_fn,\n\u001b[32m    356\u001b[39m     is_inner=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    357\u001b[39m     ctxtag=jit_wrapper,\n\u001b[32m    358\u001b[39m   )\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "    \u001b[31m[... skipping hidden 13 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/jaxpt/.venv/lib/python3.12/site-packages/flax/nnx/transforms/compilation.py:129\u001b[39m, in \u001b[36mJitFn.__call__\u001b[39m\u001b[34m(self, *pure_args, **pure_kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *pure_args, **pure_kwargs):\n\u001b[32m    122\u001b[39m   args, kwargs = extract.from_tree(\n\u001b[32m    123\u001b[39m     (pure_args, pure_kwargs),\n\u001b[32m    124\u001b[39m     merge_fn=_jit_merge_fn,\n\u001b[32m    125\u001b[39m     ctxtag=\u001b[38;5;28mself\u001b[39m.ctxtag,\n\u001b[32m    126\u001b[39m     is_inner=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    127\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m   out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m   args_out, kwargs_out = extract.clear_non_graph_nodes((args, kwargs))\n\u001b[32m    132\u001b[39m   pure_args_out, pure_kwargs_out, pure_out = extract.to_tree(\n\u001b[32m    133\u001b[39m     (args_out, kwargs_out, out),\n\u001b[32m    134\u001b[39m     prefix=(\u001b[38;5;28mself\u001b[39m.in_shardings, \u001b[38;5;28mself\u001b[39m.kwarg_shardings, \u001b[38;5;28mself\u001b[39m.out_shardings),\n\u001b[32m    135\u001b[39m     ctxtag=\u001b[38;5;28mself\u001b[39m.ctxtag,\n\u001b[32m    136\u001b[39m     split_fn=_jit_split_fn,\n\u001b[32m    137\u001b[39m   )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mstep\u001b[39m\u001b[34m(state, x, y)\u001b[39m\n\u001b[32m     28\u001b[39m (loss, gates), grads = nnx.value_and_grad(loss_fn, has_aux=\u001b[38;5;28;01mTrue\u001b[39;00m)(state.model, x, y)\n\u001b[32m     29\u001b[39m state.update(grads)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss, gates, \u001b[43mgrad\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'grad' is not defined"
     ]
    }
   ],
   "source": [
    "import optax \n",
    "\n",
    "D, B, C = 10000, 16, 8\n",
    "\n",
    "model = SimpleMoE(dim=8, rngs=nnx.Rngs(0))\n",
    "tx = optax.adam(1e-3)\n",
    "state = nnx.Optimizer(model, tx)\n",
    "\n",
    "x = jax.random.normal(jax.random.key(1000), (D * B, C))\n",
    "\n",
    "expert_ids = (x[:, 0] > 0).astype(jnp.int32)\n",
    "t = [\n",
    "    jax.random.normal(jax.random.key(2000), (C, C)),\n",
    "    jax.random.normal(jax.random.key(3000), (C, C)),\n",
    "]\n",
    "def transform(xi, eid):\n",
    "    return jnp.where(eid == 1, xi @ t[0], xi @ t[1])\n",
    "\n",
    "y = jax.vmap(lambda xi, ei: transform(xi, ei))(x, expert_ids)\n",
    "\n",
    "def loss_fn(model, x, y):\n",
    "    y_pred, lb_loss, gates = model(x)\n",
    "    loss = jnp.mean((y - y_pred)**2) # + lb_loss\n",
    "    return loss, gates\n",
    "\n",
    "@nnx.jit\n",
    "def step(state, x, y):\n",
    "    (loss, gates), grads = nnx.value_and_grad(loss_fn, has_aux=True)(state.model, x, y)\n",
    "    state.update(grads)\n",
    "    return loss, gates, grad\n",
    "\n",
    "x = x.reshape(D, B, C)\n",
    "y = y.reshape(D, B, C)\n",
    "\n",
    "for e in range(10):\n",
    "    for i in range(D):\n",
    "        loss, gates, grads = step(state, x[i], y[i])\n",
    "        if i % 1000 == 0:\n",
    "            print(i, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Mixture of Experts Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1.9036549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object JaxprTrace.default_process_primitive.<locals>.<genexpr> at 0x3970ddf20>\n",
      "Traceback (most recent call last):\n",
      "  File \"<stringsource>\", line 69, in cfunc.to_py.__Pyx_CFunc_4904d5__29_pydevd_sys_monitoring_cython_object__lParen__etc_to_py_4code_11instruction_3exc.wrap\n",
      "  File \"_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx\", line 920, in _pydevd_sys_monitoring_cython._unwind_event\n",
      "  File \"_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx\", line 198, in _pydevd_sys_monitoring_cython._get_unhandled_exception_frame\n",
      "  File \"<frozen posixpath>\", line 117, in splitext\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import flax.nnx as nnx\n",
    "import optax\n",
    "\n",
    "from jaxpt.modules.config import Config\n",
    "\n",
    "\n",
    "@dataclass(unsafe_hash=True)\n",
    "class GLU_Config(Config):\n",
    "    top_k = 4\n",
    "    load_factor = 1.00\n",
    "    n_experts = 4\n",
    "    n_embed = 3\n",
    "    n_mlp_hidden = 6\n",
    "    mlp_bias = True\n",
    "    dtype = jax.numpy.float32\n",
    "\n",
    "config = GLU_Config()\n",
    "\n",
    "\n",
    "class Expert(nnx.Module):\n",
    "    def __init__(self, config, rngs):\n",
    "        init = nnx.initializers.normal(stddev=0.02)\n",
    "        self.w1 = nnx.Param(init(rngs.normal.key.value,\n",
    "            (\n",
    "                config.n_experts,\n",
    "                config.n_embed,\n",
    "                config.n_embed\n",
    "            )\n",
    "        ))\n",
    "\n",
    "    def __call__(self, x, expert_idx):\n",
    "        w1 = self.w1[expert_idx] \n",
    "        x = x @ w1\n",
    "        return x\n",
    "\n",
    "\n",
    "class MOE(nnx.Module):\n",
    "    def __init__(self, config: Config, rngs: nnx.Rngs):\n",
    "        self.router_gate = nnx.Linear(\n",
    "            config.n_embed,\n",
    "            config.n_experts,\n",
    "            kernel_init=nnx.initializers.normal(stddev=0.02),\n",
    "            bias_init=nnx.initializers.zeros, \n",
    "            use_bias=config.mlp_bias,\n",
    "            dtype=config.dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.expert = Expert(config, rngs)        \n",
    "        self.top_k = config.top_k\n",
    "        self.n_experts = config.n_experts\n",
    "        self.load_factor = config.load_factor\n",
    "        self.add_noise = False\n",
    "        self.rngs = rngs\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, C = x.shape\n",
    "        logits = self.router_gate(x) # B, n_experts\n",
    "        #if self.add_noise:\n",
    "        #    logits += jax.random.normal(key=self.rngs.gate_noise(), shape=logits.shape)\n",
    "        top_k_logits, expert_indices = jax.lax.top_k(logits, self.top_k) # B, top_k\n",
    "\n",
    "        zeros = jnp.full_like(logits, float('-inf')) # B, n_experts\n",
    "        sparse_logits = jnp.put_along_axis(\n",
    "                zeros, expert_indices, top_k_logits, axis=-1, inplace=False) # b, n_experts\n",
    "        expert_weights = jax.nn.softmax(sparse_logits, axis=-1) # B, n_experts\n",
    "\n",
    "        max_tokens_per_expert = int(self.top_k * (self.load_factor * B) // self.n_experts)\n",
    "        expert_inputs = jnp.zeros((self.n_experts, max_tokens_per_expert, C))\n",
    "        input_counters = jnp.zeros((self.n_experts,), dtype=jnp.uint8)\n",
    "\n",
    "        def update_expert_inputs(i, carry):\n",
    "            expert_inputs, counters = carry\n",
    "            for j in range(self.top_k):\n",
    "                expert_idx = expert_indices[i, j]\n",
    "                token_pos = counters[expert_idx]\n",
    "                expert_inputs = expert_inputs.at[expert_idx, token_pos].set(x[i])\n",
    "                counters = counters.at[expert_idx].add(1)\n",
    "\n",
    "            return expert_inputs, counters\n",
    "        \n",
    "        expert_inputs, input_counters = jax.lax.fori_loop(\n",
    "            0, B, update_expert_inputs, (\n",
    "                expert_inputs,\n",
    "                input_counters\n",
    "            )\n",
    "        )\n",
    "\n",
    "        expert_outputs = jnp.zeros_like(expert_inputs)\n",
    "        for i in range(self.n_experts):\n",
    "            expert_outputs = expert_outputs.at[i].set(\n",
    "                self.expert(expert_inputs[i], i))\n",
    "\n",
    "        output_counters = jnp.zeros((self.n_experts,), dtype=jnp.uint8)\n",
    "        #y = jnp.zeros((B,))\n",
    "        y = jnp.zeros_like(x)\n",
    "        def update_expert_outputs(i, carry):\n",
    "            y, counters = carry\n",
    "            for j in range(self.top_k):\n",
    "                expert_idx = expert_indices[i, j]\n",
    "                token_pos = counters[expert_idx]\n",
    "                y = y.at[i].add(\n",
    "                    expert_outputs[expert_idx, token_pos] * expert_weights[i, expert_idx])\n",
    "                counters = counters.at[expert_idx].add(1)\n",
    "\n",
    "            return y, counters\n",
    "\n",
    "        y, output_counters = jax.lax.fori_loop(\n",
    "            0, B, update_expert_outputs, (\n",
    "                y,\n",
    "                output_counters\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return y\n",
    "\n",
    "def loss_fn(model, x, y):\n",
    "    y_pred  = model(x)\n",
    "    loss = jnp.mean((y - y_pred)**2)\n",
    "    return loss, y_pred\n",
    "\n",
    "@nnx.jit\n",
    "def step(state, x, y):\n",
    "    (loss, y_pred), grads = nnx.value_and_grad(\n",
    "        loss_fn, has_aux=True)(state.model, x, y)\n",
    "    state.update(grads)\n",
    "    return loss, grads, y_pred\n",
    "\n",
    "D, B, C = config.n_experts * 1000, 2, config.n_embed \n",
    "   \n",
    "default = jax.random.key(0)\n",
    "gate_noise = jax.random.key(42)\n",
    "rngs = nnx.Rngs(default=default, gate_noise=gate_noise)\n",
    "\n",
    "model = MOE(config, rngs)\n",
    "model.train(add_noise=True)\n",
    "tx = optax.adam(1e-3)\n",
    "state = nnx.Optimizer(model, tx)\n",
    "\n",
    "x = jax.random.normal(jax.random.key(1000), (D * B, C))\n",
    "\n",
    "expert_ids = (x[:, 0] > 0).astype(jnp.int32)\n",
    "t = [\n",
    "    jax.random.normal(jax.random.key(2000), (C, C)),\n",
    "    jax.random.normal(jax.random.key(3000), (C, C)),\n",
    "]\n",
    "def transform(xi, eid):\n",
    "    return jnp.where(eid == 1, xi @ t[0], xi @ t[1])\n",
    "\n",
    "y = jax.vmap(lambda xi, ei: transform(xi, ei))(x, expert_ids)\n",
    "\n",
    "x = x.reshape(D//config.n_experts, config.n_experts * B, C)\n",
    "y = y.reshape(D//config.n_experts, config.n_experts * B, C)\n",
    "\n",
    "indices = list(range(D//config.n_experts))\n",
    "for e in range(100):\n",
    "    for i in indices:\n",
    "        loss, grads, y_pred = step(state, x[i], y[i])\n",
    "        if i % 1000 == 0:\n",
    "            print(e, i, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred\n",
    "Array([[ 5.6378189e-03,  7.2841058e-03,  1.4552707e-02],\n",
    "       [-6.3558687e-03, -7.7637648e-03, -1.3705203e-03],\n",
    "       [ 7.5572776e-03,  1.0502232e-02,  2.9031349e-02],\n",
    "       [-5.8859895e-04,  2.0565042e-04,  2.5836997e-02],\n",
    "       [-3.2433361e-04,  2.6398519e-04,  2.6083879e-02],\n",
    "       [-2.0684090e-03,  7.0836810e-05,  2.4707634e-02],\n",
    "       [-1.0950858e-03,  3.7319329e-05,  2.5395298e-02],\n",
    "       [-1.3602758e-03,  1.8746815e-04,  2.5280533e-02]], dtype=float32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
