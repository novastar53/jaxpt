{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Dense Mixture of Experts Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "from typing import Any\n",
    "\n",
    "class Router(nnx.Module):\n",
    "    def __init__(self, dim: int, num_experts: int, *, rngs: nnx.Rngs):\n",
    "        self.w1 = nnx.Linear(dim, num_experts, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        return self.w1(x)\n",
    "\n",
    "class Expert(nnx.Module):\n",
    "    def __init__(self, dim: int, *, rngs: nnx.Rngs):\n",
    "        self.linear = nnx.Linear(dim, dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        return self.linear(x)\n",
    "\n",
    "class SimpleMoE(nnx.Module):\n",
    "    def __init__(self, dim: int, *, rngs: nnx.Rngs):\n",
    "        num_experts = 8\n",
    "        self.router = Router(dim, num_experts=num_experts, rngs=rngs)\n",
    "        self.experts = [\n",
    "            Expert(dim, rngs=rngs)\n",
    "            for _ in range(num_experts)\n",
    "        ]\n",
    "        self.top_k = 8\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        gate_logits = self.router(x)       \n",
    "        top_k_logits, expert_indices = jax.lax.top_k(gate_logits, self.top_k)\n",
    "        zeros = jnp.full_like(gate_logits, float('-inf'))\n",
    "        sparse_logits = jnp.put_along_axis(\n",
    "            zeros, expert_indices, top_k_logits, axis=-1, inplace=False\n",
    "        )\n",
    "        expert_weights = jax.nn.softmax(sparse_logits, axis=-1)\n",
    "\n",
    "        mean_gates = jnp.mean(gate_logits, axis=0)\n",
    "        lb_loss = gate_logits.shape[1] * jnp.sum(mean_gates ** 2)\n",
    "\n",
    "        outputs = [ e(x) for e in self.experts ]\n",
    "\n",
    "        result = jnp.zeros_like(x)\n",
    "\n",
    "        for i, o in enumerate(outputs):\n",
    "            result += (o * expert_weights[:, i:i+1])\n",
    "           \n",
    "        return result, lb_loss, expert_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax \n",
    "\n",
    "D, B, C = 10000, 16, 8\n",
    "\n",
    "model = SimpleMoE(dim=8, rngs=nnx.Rngs(0))\n",
    "tx = optax.adam(1e-3)\n",
    "state = nnx.Optimizer(model, tx)\n",
    "\n",
    "x = jax.random.normal(jax.random.key(1000), (D * B, C))\n",
    "\n",
    "expert_ids = (x[:, 0] > 0).astype(jnp.int32)\n",
    "t = [\n",
    "    jax.random.normal(jax.random.key(2000), (C, C)),\n",
    "    jax.random.normal(jax.random.key(3000), (C, C)),\n",
    "]\n",
    "def transform(xi, eid):\n",
    "    return jnp.where(eid == 1, xi @ t[0], xi @ t[1])\n",
    "\n",
    "y = jax.vmap(lambda xi, ei: transform(xi, ei))(x, expert_ids)\n",
    "\n",
    "def loss_fn(model, x, y):\n",
    "    y_pred, lb_loss, gates = model(x)\n",
    "    loss = jnp.mean((y - y_pred)**2) # + lb_loss\n",
    "    return loss, gates\n",
    "\n",
    "@nnx.jit\n",
    "def step(state, x, y):\n",
    "    (loss, gates), grads = nnx.value_and_grad(loss_fn, has_aux=True)(state.model, x, y)\n",
    "    state.update(grads)\n",
    "    return loss, gates, grad\n",
    "\n",
    "x = x.reshape(D, B, C)\n",
    "y = y.reshape(D, B, C)\n",
    "\n",
    "for e in range(10):\n",
    "    for i in range(D):\n",
    "        loss, gates, grads = step(state, x[i], y[i])\n",
    "        if i % 1000 == 0:\n",
    "            print(i, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Mixture of Experts Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 4.937499\n",
      "1 0 2.3426387\n",
      "2 0 0.88567847\n",
      "3 0 0.23639199\n",
      "4 0 0.3027224\n",
      "5 0 0.1991851\n",
      "6 0 0.29586947\n",
      "7 0 0.011218689\n",
      "8 0 0.029206669\n",
      "9 0 0.04894633\n",
      "10 0 0.316072\n",
      "11 0 0.13060854\n",
      "12 0 0.12245482\n",
      "13 0 0.10715624\n",
      "14 0 0.02535601\n",
      "15 0 0.013303846\n",
      "16 0 0.0019562235\n",
      "17 0 0.04184603\n",
      "18 0 0.01619613\n",
      "19 0 0.043957688\n",
      "20 0 0.0068773134\n",
      "21 0 0.0022203114\n",
      "22 0 0.03925567\n",
      "23 0 0.0012036965\n",
      "24 0 0.014375436\n",
      "25 0 0.017814659\n",
      "26 0 0.005852036\n",
      "27 0 0.04876576\n",
      "28 0 0.0043614777\n",
      "29 0 0.0074885627\n",
      "30 0 0.00030034513\n",
      "31 0 0.011439363\n",
      "32 0 0.006428576\n",
      "33 0 0.01489618\n",
      "34 0 0.0016702486\n",
      "35 0 0.0012716027\n",
      "36 0 0.0011290588\n",
      "37 0 0.024733476\n",
      "38 0 0.00091762113\n",
      "39 0 0.016716927\n",
      "40 0 0.03993621\n",
      "41 0 0.028772531\n",
      "42 0 0.012828862\n",
      "43 0 0.0075656185\n",
      "44 0 0.005023991\n",
      "45 0 0.0014469483\n",
      "46 0 0.015117263\n",
      "47 0 0.00028268102\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 164\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m100\u001b[39m):\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m         loss, grads, y_pred = \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m1000\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m    166\u001b[39m             \u001b[38;5;28mprint\u001b[39m(e, i, loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/jaxpt/.venv/lib/python3.12/site-packages/flax/nnx/transforms/compilation.py:350\u001b[39m, in \u001b[36mjit.<locals>.jit_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m graph.update_context(jit_wrapper):\n\u001b[32m    341\u001b[39m   pure_args, pure_kwargs = extract.to_tree(\n\u001b[32m    342\u001b[39m     (args, kwargs),\n\u001b[32m    343\u001b[39m     prefix=(in_shardings, kwarg_shardings)\n\u001b[32m   (...)\u001b[39m\u001b[32m    348\u001b[39m     ctxtag=jit_wrapper,\n\u001b[32m    349\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m   pure_args_out, pure_kwargs_out, pure_out = \u001b[43mjitted_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43mpure_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpure_kwargs\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    353\u001b[39m   _args_out, _kwargs_out, out = extract.from_tree(\n\u001b[32m    354\u001b[39m     (pure_args_out, pure_kwargs_out, pure_out),\n\u001b[32m    355\u001b[39m     merge_fn=_jit_merge_fn,\n\u001b[32m    356\u001b[39m     is_inner=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    357\u001b[39m     ctxtag=jit_wrapper,\n\u001b[32m    358\u001b[39m   )\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/jaxpt/.venv/lib/python3.12/site-packages/flax/nnx/variablelib.py:892\u001b[39m, in \u001b[36m_variable_state_unflatten\u001b[39m\u001b[34m(static, children)\u001b[39m\n\u001b[32m    888\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_variable_state_unflatten\u001b[39m(\n\u001b[32m    889\u001b[39m   static: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mtype\u001b[39m[Variable[A]], \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, tp.Any], ...]],\n\u001b[32m    890\u001b[39m   children: \u001b[38;5;28mtuple\u001b[39m[A],\n\u001b[32m    891\u001b[39m ) -> VariableState[A]:\n\u001b[32m--> \u001b[39m\u001b[32m892\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariableState\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchildren\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/jaxpt/.venv/lib/python3.12/site-packages/flax/nnx/variablelib.py:797\u001b[39m, in \u001b[36mVariableState.__init__\u001b[39m\u001b[34m(self, type, value, **metadata)\u001b[39m\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    791\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    792\u001b[39m   \u001b[38;5;28mtype\u001b[39m: \u001b[38;5;28mtype\u001b[39m[Variable[A]],  \u001b[38;5;66;03m# type: ignore [valid-type]\u001b[39;00m\n\u001b[32m    793\u001b[39m   value: A,\n\u001b[32m    794\u001b[39m   **metadata,\n\u001b[32m    795\u001b[39m ):\n\u001b[32m    796\u001b[39m   \u001b[38;5;28mobject\u001b[39m.\u001b[34m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m797\u001b[39m   \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    798\u001b[39m   \u001b[38;5;28mobject\u001b[39m.\u001b[34m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_var_metadata\u001b[39m\u001b[33m'\u001b[39m, metadata)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import flax.nnx as nnx\n",
    "import optax\n",
    "\n",
    "from jaxpt.modules.config import Config\n",
    "\n",
    "\n",
    "@dataclass(unsafe_hash=True)\n",
    "class GLU_Config(Config):\n",
    "    top_k = 2\n",
    "    load_factor = 1.00\n",
    "    n_experts = 2\n",
    "    n_embed = 3\n",
    "    n_mlp_hidden = 6\n",
    "    mlp_bias = True\n",
    "    dtype = jax.numpy.float32\n",
    "\n",
    "config = GLU_Config()\n",
    "\n",
    "\n",
    "class Experts(nnx.Module):\n",
    "    def __init__(self, config, rngs):\n",
    "        init = nnx.initializers.normal(stddev=0.02)\n",
    "        self.w1 = nnx.Param(init(rngs.default(),\n",
    "            (\n",
    "                config.n_experts,\n",
    "                config.n_embed,\n",
    "                config.n_embed\n",
    "            )\n",
    "        ))\n",
    "\n",
    "    def __call__(self, x, expert_idx):\n",
    "        w1 = self.w1[expert_idx] \n",
    "        x = x @ w1\n",
    "        return x\n",
    "\n",
    "\n",
    "class MOE(nnx.Module):\n",
    "    def __init__(self, config: Config, rngs: nnx.Rngs):\n",
    "        self.router_gate = nnx.Linear(\n",
    "            config.n_embed,\n",
    "            config.n_experts,\n",
    "            kernel_init=nnx.initializers.normal(stddev=0.02),\n",
    "            bias_init=nnx.initializers.zeros, \n",
    "            use_bias=config.mlp_bias,\n",
    "            dtype=config.dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.experts = Experts(config, rngs)        \n",
    "        self.top_k = config.top_k\n",
    "        self.n_experts = config.n_experts\n",
    "        self.load_factor = config.load_factor\n",
    "        self.add_noise = False\n",
    "        self.rngs = rngs\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, C = x.shape\n",
    "        logits = self.router_gate(x) # B, n_experts\n",
    "        if self.add_noise:\n",
    "            logits += 1 * jax.random.normal(key=self.rngs.gate_noise(), shape=logits.shape)\n",
    "        top_k_logits, expert_indices = jax.lax.top_k(logits, self.top_k) # B, top_k\n",
    "\n",
    "        zeros = jnp.full_like(logits, float('-inf')) # B, n_experts\n",
    "        sparse_logits = jnp.put_along_axis(\n",
    "                zeros, expert_indices, top_k_logits, axis=-1, inplace=False) # b, n_experts\n",
    "        expert_weights = jax.nn.softmax(sparse_logits, axis=-1) # B, n_experts\n",
    "\n",
    "        expert_inputs = jnp.zeros((self.n_experts, self.top_k * B, C))\n",
    "        input_counters = jnp.zeros((self.n_experts,), dtype=jnp.uint8)\n",
    "\n",
    "        def update_expert_inputs(i, carry):\n",
    "            expert_inputs, counters = carry\n",
    "            for j in range(self.top_k):\n",
    "                expert_idx = expert_indices[i, j]\n",
    "                token_pos = counters[expert_idx]\n",
    "                expert_inputs = expert_inputs.at[expert_idx, token_pos].set(x[i])\n",
    "                counters = counters.at[expert_idx].add(1)\n",
    "\n",
    "            return expert_inputs, counters\n",
    "        \n",
    "        expert_inputs, input_counters = jax.lax.fori_loop(\n",
    "            0, B, update_expert_inputs, (\n",
    "                expert_inputs,\n",
    "                input_counters\n",
    "            )\n",
    "        )\n",
    "\n",
    "        expert_outputs = jnp.zeros_like(expert_inputs)\n",
    "        for i in range(self.n_experts):\n",
    "            expert_outputs = expert_outputs.at[i].set(\n",
    "                self.experts(expert_inputs[i], i))\n",
    "\n",
    "        output_counters = jnp.zeros((self.n_experts,), dtype=jnp.uint8)\n",
    "        #y = jnp.zeros((B,))\n",
    "        y = jnp.zeros_like(x)\n",
    "        def update_expert_outputs(i, carry):\n",
    "            y, counters = carry\n",
    "            for j in range(self.top_k):\n",
    "                expert_idx = expert_indices[i, j]\n",
    "                token_pos = counters[expert_idx]\n",
    "                y = y.at[i].add(\n",
    "                    expert_outputs[expert_idx, token_pos] * expert_weights[i, expert_idx])\n",
    "                counters = counters.at[expert_idx].add(1)\n",
    "\n",
    "            return y, counters\n",
    "\n",
    "        y, output_counters = jax.lax.fori_loop(\n",
    "            0, B, update_expert_outputs, (\n",
    "                y,\n",
    "                output_counters\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return y\n",
    "\n",
    "def loss_fn(model, x, y):\n",
    "    y_pred  = model(x)\n",
    "    loss = jnp.mean((y - y_pred)**2)\n",
    "    return loss, y_pred\n",
    "\n",
    "@nnx.jit\n",
    "def step(state, x, y):\n",
    "    (loss, y_pred), grads = nnx.value_and_grad(\n",
    "        loss_fn, has_aux=True)(state.model, x, y)\n",
    "    state.update(grads)\n",
    "    return loss, grads, y_pred\n",
    "\n",
    "D, B, C =  1000, 16 * config.n_experts, config.n_embed \n",
    "   \n",
    "default = jax.random.key(69)\n",
    "gate_noise = jax.random.key(42)\n",
    "rngs = nnx.Rngs(default=default, gate_noise=gate_noise)\n",
    "\n",
    "model = MOE(config, rngs)\n",
    "model.train(add_noise=True)\n",
    "tx = optax.adam(1e-3)\n",
    "state = nnx.Optimizer(model, tx)\n",
    "\n",
    "x = jax.random.normal(jax.random.key(1000), (D * B, C))\n",
    "\n",
    "expert_ids = (x[:, 0] > 0).astype(jnp.int32)\n",
    "t = [\n",
    "    jax.random.normal(jax.random.key(2000), (C, C)),\n",
    "    jax.random.normal(jax.random.key(3000), (C, C)),\n",
    "]\n",
    "def transform(xi, eid):\n",
    "    return jnp.where(eid == 1, xi @ t[0], xi @ t[1])\n",
    "\n",
    "y = jax.vmap(lambda xi, ei: transform(xi, ei))(x, expert_ids)\n",
    "x = x.reshape(D, B, C)\n",
    "y = y.reshape(D, B, C)\n",
    "\n",
    "indices = list(range(D))\n",
    "for e in range(100):\n",
    "    for i in indices:\n",
    "        loss, grads, y_pred = step(state, x[i], y[i])\n",
    "        if i % 1000 == 0:\n",
    "            print(e, i, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
