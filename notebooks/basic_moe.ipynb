{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Dense Mixture of Experts Model\n",
    "\n",
    "## Overview\n",
    "\n",
    "This implementation demonstrates a **dense mixture of experts** approach where:\n",
    "- **All experts process all tokens** (computationally expensive)\n",
    "- **Top-k routing** selects the k most relevant experts per token\n",
    "- **Soft weighting** combines expert outputs using routing scores\n",
    "\n",
    "## Architecture\n",
    "\n",
    "### Components:\n",
    "- **Router**: Linear layer that outputs gating logits for each expert\n",
    "- **Experts**: Simple linear transformations (dim → dim)\n",
    "- **Load Balancing Loss**: Encourages uniform expert utilization\n",
    "\n",
    "### Algorithm Flow:\n",
    "1. **Routing**: Compute gating scores for each expert\n",
    "2. **Selection**: Top-k selection of experts per token\n",
    "3. **Weighting**: Apply softmax to create sparse expert weights\n",
    "4. **Computation**: All experts process all tokens (dense)\n",
    "5. **Combination**: Weighted sum of expert outputs\n",
    "\n",
    "## Key Characteristics\n",
    "\n",
    "### Pros:\n",
    "- Simple implementation and debugging\n",
    "- Smooth gradients from all experts\n",
    "- Implicit load balancing through auxiliary loss\n",
    "\n",
    "### Cons:\n",
    "- **Inefficient**: O(n_experts) computation per token\n",
    "- Not scalable for large numbers of experts\n",
    "- High memory usage due to dense computation\n",
    "\n",
    "### Load Balancing Loss:\n",
    "```python\n",
    "mean_gates = jnp.mean(gate_logits, axis=0)\n",
    "lb_loss = gate_logits.shape[1] * jnp.sum(mean_gates ** 2)\n",
    "```\n",
    "This penalty encourages uniform distribution of tokens across experts by minimizing the variance of average gating scores.\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- Educational purposes and prototyping\n",
    "- Small-scale experiments with few experts\n",
    "- Baseline for comparing with sparse implementations\n",
    "- Situations where all expert contributions are needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "from typing import Any\n",
    "\n",
    "class Router(nnx.Module):\n",
    "    def __init__(self, dim: int, num_experts: int, *, rngs: nnx.Rngs):\n",
    "        self.w1 = nnx.Linear(dim, num_experts, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        return self.w1(x)\n",
    "\n",
    "class Expert(nnx.Module):\n",
    "    def __init__(self, dim: int, *, rngs: nnx.Rngs):\n",
    "        self.linear = nnx.Linear(dim, dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        return self.linear(x)\n",
    "\n",
    "class SimpleMoE(nnx.Module):\n",
    "    def __init__(self, dim: int, *, rngs: nnx.Rngs):\n",
    "        num_experts = 2\n",
    "        self.router = Router(dim, num_experts=num_experts, rngs=rngs)\n",
    "        self.experts = nnx.List([\n",
    "            Expert(dim, rngs=rngs)\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        self.top_k = 2\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        gate_logits = self.router(x)       \n",
    "        top_k_logits, expert_indices = jax.lax.top_k(gate_logits, self.top_k)\n",
    "        zeros = jnp.full_like(gate_logits, float('-inf'))\n",
    "        sparse_logits = jnp.put_along_axis(\n",
    "            zeros, expert_indices, top_k_logits, axis=-1, inplace=False\n",
    "        )\n",
    "        expert_weights = jax.nn.softmax(sparse_logits, axis=-1)\n",
    "\n",
    "        mean_gates = jnp.mean(gate_logits, axis=0)\n",
    "        lb_loss = gate_logits.shape[1] * jnp.sum(mean_gates ** 2)\n",
    "\n",
    "        outputs = [ e(x) for e in self.experts ]\n",
    "\n",
    "        result = jnp.zeros_like(x)\n",
    "\n",
    "        for i, o in enumerate(outputs):\n",
    "            result += (o * expert_weights[:, :, i:i+1])\n",
    "           \n",
    "        return result, lb_loss, expert_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.8386207\n",
      "1000 1.4691024\n",
      "2000 0.6264829\n",
      "3000 0.22666237\n",
      "4000 0.32224867\n",
      "5000 0.27109587\n",
      "6000 0.036007397\n",
      "7000 0.042003997\n",
      "8000 0.359906\n",
      "9000 0.019899525\n",
      "0 0.07967947\n",
      "1000 0.048338573\n",
      "2000 0.07231833\n",
      "3000 0.0011955692\n",
      "4000 0.061102558\n",
      "5000 0.027696146\n",
      "6000 0.0011745306\n",
      "7000 0.0014423532\n",
      "8000 0.25469956\n",
      "9000 0.0031980982\n",
      "0 0.037122283\n",
      "1000 0.021085124\n",
      "2000 0.020036932\n",
      "3000 0.0002973358\n",
      "4000 0.028141744\n",
      "5000 0.0031959899\n",
      "6000 0.0005252546\n",
      "7000 0.0006971828\n",
      "8000 0.23215021\n",
      "9000 0.0010879862\n",
      "0 0.019682594\n",
      "1000 0.009787401\n",
      "2000 0.00533594\n",
      "3000 0.00018117022\n",
      "4000 0.013458978\n",
      "5000 0.00086001644\n",
      "6000 0.00029388236\n",
      "7000 0.0004293031\n",
      "8000 0.21831232\n",
      "9000 0.00060997024\n",
      "0 0.012715654\n",
      "1000 0.004591771\n",
      "2000 0.0015240598\n",
      "3000 0.00012839105\n",
      "4000 0.0068611316\n",
      "5000 0.0005576007\n",
      "6000 0.00019687101\n",
      "7000 0.00030134188\n",
      "8000 0.20550406\n",
      "9000 0.00043756963\n",
      "0 0.009739019\n",
      "1000 0.002218916\n",
      "2000 0.0005265196\n",
      "3000 0.00010175501\n",
      "4000 0.0037555827\n",
      "5000 0.000450459\n",
      "6000 0.00014797169\n",
      "7000 0.00023094774\n",
      "8000 0.19362797\n",
      "9000 0.00034868997\n",
      "0 0.008288349\n",
      "1000 0.0011200772\n",
      "2000 0.00025397656\n",
      "3000 8.6699896e-05\n",
      "4000 0.0022263434\n",
      "5000 0.0003809286\n",
      "6000 0.00012016174\n",
      "7000 0.00018735956\n",
      "8000 0.18282159\n",
      "9000 0.00029231366\n",
      "0 0.0074516423\n",
      "1000 0.0005978119\n",
      "2000 0.00017527514\n",
      "3000 7.715925e-05\n",
      "4000 0.0014416231\n",
      "5000 0.00033131373\n",
      "6000 0.00010308235\n",
      "7000 0.00015803585\n",
      "8000 0.17301631\n",
      "9000 0.00025248475\n",
      "0 0.0068818224\n",
      "1000 0.00034176663\n",
      "2000 0.00015066056\n",
      "3000 7.046891e-05\n",
      "4000 0.0010219362\n",
      "5000 0.00029549695\n",
      "6000 9.1995964e-05\n",
      "7000 0.0001370759\n",
      "8000 0.16406791\n",
      "9000 0.00022256821\n",
      "0 0.0064407364\n",
      "1000 0.00021198479\n",
      "2000 0.0001417837\n",
      "3000 6.53722e-05\n",
      "4000 0.0007871999\n",
      "5000 0.0002693269\n",
      "6000 8.445619e-05\n",
      "7000 0.00012142981\n",
      "8000 0.15582006\n",
      "9000 0.0001991511\n"
     ]
    }
   ],
   "source": [
    "import optax \n",
    "\n",
    "D, B, T, C = 10000, 2, 5, 3\n",
    "\n",
    "model = SimpleMoE(dim=C, rngs=nnx.Rngs(0))\n",
    "tx = optax.adam(1e-3)\n",
    "state = nnx.Optimizer(model, tx, wrt=nnx.Param)\n",
    "\n",
    "x = jax.random.normal(jax.random.key(1000), (D * B * T, C))\n",
    "\n",
    "expert_ids = (x[:, 0] > 0).astype(jnp.int32)\n",
    "t = [\n",
    "    jax.random.normal(jax.random.key(2000), (C, C)),\n",
    "    jax.random.normal(jax.random.key(3000), (C, C)),\n",
    "]\n",
    "def transform(xi, eid):\n",
    "    return jnp.where(eid == 1, xi @ t[0], xi @ t[1])\n",
    "\n",
    "y = jax.vmap(lambda xi, ei: transform(xi, ei))(x, expert_ids)\n",
    "\n",
    "def loss_fn(model, x, y):\n",
    "    y_pred, lb_loss, gates = model(x)\n",
    "    loss = jnp.mean((y - y_pred)**2) # + lb_loss\n",
    "    return loss, gates\n",
    "\n",
    "@nnx.jit\n",
    "def step(model, state, x, y):\n",
    "    (loss, gates), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model, x, y)\n",
    "    state.update(model, grads)\n",
    "    return loss, gates, grads\n",
    "\n",
    "x = x.reshape(D, B, T, C)\n",
    "y = y.reshape(D, B, T, C)\n",
    "\n",
    "for e in range(10):\n",
    "    for i in range(D):\n",
    "        loss, gates, grads = step(model, state, x[i], y[i])\n",
    "        if i % 1000 == 0:\n",
    "            print(i, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Mixture of Experts Model\n",
    "\n",
    "## Overview\n",
    "\n",
    "This implementation demonstrates a **sparse mixture of experts** approach that overcomes the computational inefficiency of dense MoE:\n",
    "- **Only selected experts process tokens** (computationally efficient)\n",
    "- **Expert capacity management** prevents overflow using load factors\n",
    "- **Efficient buffer management** for token-to-expert routing\n",
    "\n",
    "## Architecture\n",
    "\n",
    "### Components:\n",
    "- **Router**: Linear layer with configurable bias for expert selection\n",
    "- **Experts**: Parameterized linear experts with efficient indexing\n",
    "- **Expert Buffers**: Dynamically allocated buffers for expert inputs\n",
    "- **Capacity Management**: Load factor controls expert buffer sizes\n",
    "\n",
    "### Algorithm Flow:\n",
    "1. **Routing**: Compute gating scores for each expert\n",
    "2. **Selection**: Top-k selection of experts per token\n",
    "3. **Buffer Allocation**: Create expert input buffers using load factor\n",
    "4. **Token Distribution**: Efficiently assign tokens to selected experts\n",
    "5. **Expert Processing**: Each expert processes only its assigned tokens\n",
    "6. **Output Assembly**: Combine expert outputs using routing weights\n",
    "\n",
    "## Key Innovations\n",
    "\n",
    "### Sparse Computation:\n",
    "Unlike dense MoE, only the selected experts process each token:\n",
    "```python\n",
    "# Dense: All experts process all tokens\n",
    "outputs = [expert(x) for expert in experts]  # O(n_experts) cost\n",
    "\n",
    "# Sparse: Only selected experts process tokens\n",
    "expert_inputs[expert_idx, token_pos] = x[i]  # O(top_k) cost\n",
    "```\n",
    "\n",
    "### Capacity Management:\n",
    "The `load_factor` parameter determines expert buffer sizes:\n",
    "- `load_factor = 1.0`: Expected token capacity (can cause overflow)\n",
    "- `load_factor > 1.0`: Extra capacity for load imbalance\n",
    "- Higher values = more memory but better coverage\n",
    "\n",
    "### Efficient Indexing:\n",
    "Uses counter-based token placement to avoid expensive scatter operations:\n",
    "```python\n",
    "input_counters = jnp.zeros((n_experts,), dtype=jnp.uint8)\n",
    "expert_inputs = expert_inputs.at[expert_idx, token_pos].set(x[i])\n",
    "input_counters = input_counters.at[expert_idx].add(1)\n",
    "```\n",
    "\n",
    "## Performance Characteristics\n",
    "\n",
    "### Computational Complexity:\n",
    "- **Dense MoE**: O(seq_len × n_experts × dim²)\n",
    "- **Sparse MoE**: O(seq_len × top_k × dim²)\n",
    "- **Speedup**: ~n_experts/top_k times faster\n",
    "\n",
    "### Memory Usage:\n",
    "- Expert buffers: `n_experts × top_k × batch_size × seq_len × dim`\n",
    "- Scales with `top_k` rather than `n_experts`\n",
    "\n",
    "### Load Balancing:\n",
    "- No explicit auxiliary loss (unlike dense version)\n",
    "- Relies on learned routing to distribute load\n",
    "- Can be enhanced with load balancing penalties if needed\n",
    "\n",
    "## Comparison with Dense Implementation\n",
    "\n",
    "| Aspect | Dense MoE | Sparse MoE |\n",
    "|--------|-----------|------------|\n",
    "| Computation | All experts × all tokens | Top-k experts × tokens |\n",
    "| Memory | Lower (no buffers) | Higher (expert buffers) |\n",
    "| Scalability | Poor (linear in n_experts) | Excellent (constant in n_experts) |\n",
    "| Load Balancing | Explicit loss term | Implicit via routing |\n",
    "| Implementation | Simple | Complex (buffer management) |\n",
    "\n",
    "## Connection to Production MoE Models\n",
    "\n",
    "This sparse implementation shares core concepts with production models like `Tiny_MoE`, `Tiny_MoE_2`, and `Tiny_MoE_3`:\n",
    "\n",
    "### Similarities:\n",
    "- Top-k expert selection\n",
    "- Sparse token-to-expert routing\n",
    "- Efficient expert buffer management\n",
    "- Configurable load factors\n",
    "\n",
    "### Production Enhancements:\n",
    "- **Advanced Load Balancing**: Auxiliary losses (load balance + z-loss)\n",
    "- **Mixed Precision**: bfloat16 computation, float32 parameters\n",
    "- **Device Sharding**: Distributed training across multiple devices\n",
    "- **Expert Weight Priority**: Weight-based expert selection ordering\n",
    "- **Soft MoE**: Differentiable routing (Tiny_MoE_3)\n",
    "- **Gated Linear Units**: More powerful expert networks\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- **Large-scale models** where computational efficiency is critical\n",
    "- **Training on limited resources** with many experts\n",
    "- **Inference scenarios** requiring fast processing\n",
    "- **Foundation for advanced MoE variants** (Switch Transformer, GLaM, etc.)\n",
    "\n",
    "## Limitations and Extensions\n",
    "\n",
    "### Current Limitations:\n",
    "- No explicit load balancing loss\n",
    "- Fixed capacity buffers (can overflow or underflow)\n",
    "- Single-device implementation\n",
    "\n",
    "### Potential Extensions:\n",
    "- Add load balancing and auxiliary losses\n",
    "- Implement capacity dropping for overflow handling\n",
    "- Add noise injection for better exploration\n",
    "- Extend to multi-device training with expert sharding\n",
    "- Implement different routing strategies (e.g., learned routing)\n",
    "\n",
    "This sparse implementation provides the foundation for understanding modern, efficient MoE systems used in large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 2.6899827\n",
      "1 0 0.07806056\n",
      "2 0 0.05633394\n",
      "3 0 0.04081124\n",
      "4 0 0.029184442\n",
      "5 0 0.020910963\n",
      "6 0 0.015457934\n",
      "7 0 0.012084823\n",
      "8 0 0.010068251\n",
      "9 0 0.008866145\n",
      "10 0 0.008118208\n",
      "11 0 0.007610542\n",
      "12 0 0.0072278054\n",
      "13 0 0.0069115274\n",
      "14 0 0.006632954\n",
      "15 0 0.0063779377\n",
      "16 0 0.006139335\n",
      "17 0 0.005913367\n",
      "18 0 0.0056978953\n",
      "19 0 0.0054915864\n",
      "20 0 0.005293556\n",
      "21 0 0.005103143\n",
      "22 0 0.004919832\n",
      "23 0 0.004743188\n",
      "24 0 0.00457285\n",
      "25 0 0.0044085\n",
      "26 0 0.0042498484\n",
      "27 0 0.0040966603\n",
      "28 0 0.0039487346\n",
      "29 0 0.003805874\n",
      "30 0 0.0036679066\n",
      "31 0 0.0035347173\n",
      "32 0 0.0034061493\n",
      "33 0 0.003282117\n",
      "34 0 0.0031625007\n",
      "35 0 0.0030472109\n",
      "36 0 0.0029361544\n",
      "37 0 0.002829238\n",
      "38 0 0.0027263872\n",
      "39 0 0.0026275036\n",
      "40 0 0.0025324963\n",
      "41 0 0.0024412859\n",
      "42 0 0.0023537574\n",
      "43 0 0.0022698208\n",
      "44 0 0.0021893624\n",
      "45 0 0.0021122745\n",
      "46 0 0.0020384467\n",
      "47 0 0.00196777\n",
      "48 0 0.0019001174\n",
      "49 0 0.0018353804\n",
      "50 0 0.0017734382\n",
      "51 0 0.0017141727\n",
      "52 0 0.0016574716\n",
      "53 0 0.0016032109\n",
      "54 0 0.0015512869\n",
      "55 0 0.0015015854\n",
      "56 0 0.0014539972\n",
      "57 0 0.0014084165\n",
      "58 0 0.0013647479\n",
      "59 0 0.0013228917\n",
      "60 0 0.0012827523\n",
      "61 0 0.0012442413\n",
      "62 0 0.0012072742\n",
      "63 0 0.0011717707\n",
      "64 0 0.0011376531\n",
      "65 0 0.0011048461\n",
      "66 0 0.0010732778\n",
      "67 0 0.0010428837\n",
      "68 0 0.0010136018\n",
      "69 0 0.0009853668\n",
      "70 0 0.00095813395\n",
      "71 0 0.0009318481\n",
      "72 0 0.0009064627\n",
      "73 0 0.00088193\n",
      "74 0 0.0008581998\n",
      "75 0 0.0008352427\n",
      "76 0 0.0008130222\n",
      "77 0 0.0007914934\n",
      "78 0 0.0007706359\n",
      "79 0 0.0007504146\n",
      "80 0 0.00073080184\n",
      "81 0 0.00071177084\n",
      "82 0 0.0006932993\n",
      "83 0 0.0006753631\n",
      "84 0 0.00065794337\n",
      "85 0 0.00064101536\n",
      "86 0 0.00062456814\n",
      "87 0 0.0006085803\n",
      "88 0 0.0005930396\n",
      "89 0 0.0005779307\n",
      "90 0 0.0005632358\n",
      "91 0 0.00054894027\n",
      "92 0 0.0005350376\n",
      "93 0 0.0005215142\n",
      "94 0 0.0005083563\n",
      "95 0 0.0004955567\n",
      "96 0 0.0004831016\n",
      "97 0 0.00047098217\n",
      "98 0 0.00045918778\n",
      "99 0 0.00044771066\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import flax.nnx as nnx\n",
    "import optax\n",
    "\n",
    "from jaxpt.modules.config import Config\n",
    "\n",
    "\n",
    "@dataclass(unsafe_hash=True)\n",
    "class GLU_Config(Config):\n",
    "    top_k = 2\n",
    "    load_factor = 1.00\n",
    "    n_experts = 2\n",
    "    n_embed = 3\n",
    "    n_mlp_hidden = 6\n",
    "    mlp_bias = True\n",
    "    dtype = jax.numpy.float32\n",
    "\n",
    "config = GLU_Config()\n",
    "\n",
    "\n",
    "class Experts(nnx.Module):\n",
    "    def __init__(self, config, rngs):\n",
    "        init = nnx.initializers.normal(stddev=0.02)\n",
    "        self.w1 = nnx.Param(init(rngs.default(),\n",
    "            (\n",
    "                config.n_experts,\n",
    "                config.n_embed,\n",
    "                config.n_embed\n",
    "            )\n",
    "        ))\n",
    "\n",
    "    def __call__(self, x, expert_idx):\n",
    "        w1 = self.w1[expert_idx] \n",
    "        x = x @ w1\n",
    "        return x\n",
    "\n",
    "\n",
    "class MOE(nnx.Module):\n",
    "    def __init__(self, config: Config, rngs: nnx.Rngs):\n",
    "        self.router_gate = nnx.Linear(\n",
    "            config.n_embed,\n",
    "            config.n_experts,\n",
    "            kernel_init=nnx.initializers.normal(stddev=0.02),\n",
    "            bias_init=nnx.initializers.zeros, \n",
    "            use_bias=config.mlp_bias,\n",
    "            dtype=config.dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.experts = Experts(config, rngs)        \n",
    "        self.top_k = config.top_k\n",
    "        self.n_experts = config.n_experts\n",
    "        self.load_factor = config.load_factor\n",
    "        self.add_noise = False\n",
    "        self.rngs = rngs\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.reshape(-1, C)\n",
    "        # Gives you the expert logits for each token\n",
    "        logits = self.router_gate(x) # B, n_experts\n",
    "        #if self.add_noise:\n",
    "        #    logits += 1 * jax.random.normal(key=self.rngs.gate_noise(), shape=logits.shape)\n",
    "        # \n",
    "        # Obtains the logits for the top_k experts as well as the indices for those experts\n",
    "        top_k_logits, expert_indices = jax.lax.top_k(logits, self.top_k) # B, top_k\n",
    "\n",
    "        # Tensor that will hold the expert weights for each token\n",
    "        zeros = jnp.full_like(logits, float('-inf')) # B, n_experts\n",
    "        # Fill the logits for each token \n",
    "        sparse_logits = jnp.put_along_axis(\n",
    "                zeros, expert_indices, top_k_logits, axis=-1, inplace=False) # B, n_experts\n",
    "        # Take a softmax across each row to obtain the expert weights\n",
    "        expert_weights = jax.nn.softmax(sparse_logits, axis=-1) # B, n_experts\n",
    "\n",
    "        # Construct an array to hold the inputs for each expert\n",
    "        expert_inputs = jnp.zeros((self.n_experts, self.top_k * B * T, C))\n",
    "        input_counters = jnp.zeros((self.n_experts,), dtype=jnp.uint8)\n",
    "\n",
    "        # Loop through the experts and update each input\n",
    "        def update_expert_inputs(i, carry):\n",
    "            expert_inputs, counters = carry\n",
    "            for j in range(self.top_k):\n",
    "                expert_idx = expert_indices[i, j]\n",
    "                token_pos = counters[expert_idx]\n",
    "                expert_inputs = expert_inputs.at[expert_idx, token_pos].set(x[i])\n",
    "                counters = counters.at[expert_idx].add(1)\n",
    "\n",
    "            return expert_inputs, counters\n",
    "\n",
    "        # Loop through all the tokens and assign them to the top k experts        \n",
    "        expert_inputs, input_counters = jax.lax.fori_loop(\n",
    "            0, B * T, update_expert_inputs, (\n",
    "                expert_inputs,\n",
    "                input_counters\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Create a tensor for the expert outputs\n",
    "        expert_outputs = jnp.zeros_like(expert_inputs)\n",
    "\n",
    "        # Loop through each expert and transform the inputs\n",
    "        for i in range(self.n_experts):\n",
    "            expert_outputs = expert_outputs.at[i].set(\n",
    "                self.experts(expert_inputs[i], i)\n",
    "                )\n",
    "\n",
    "\n",
    "        # Now we need to scatter the tokens back to their original positions \n",
    "        output_counters = jnp.zeros((self.n_experts,), dtype=jnp.uint8)\n",
    "        #y = jnp.zeros((B,))\n",
    "        y_pred = jnp.zeros_like(x)\n",
    "        def update_expert_outputs(i, carry):\n",
    "            y_pred, output_counters = carry\n",
    "            for j in range(self.top_k):\n",
    "                expert_idx = expert_indices[i, j]\n",
    "                token_pos = output_counters[expert_idx]\n",
    "                y_pred = y_pred.at[i].add(\n",
    "                    expert_outputs[expert_idx, token_pos] * expert_weights[i, expert_idx])\n",
    "                output_counters = output_counters.at[expert_idx].add(1)\n",
    "\n",
    "            return y_pred, output_counters\n",
    "\n",
    "        # Loop through the transformed tokens and assign them to their original positions\n",
    "        y_pred, output_counters = jax.lax.fori_loop(\n",
    "            0, B * T, update_expert_outputs, (\n",
    "                y_pred,\n",
    "                output_counters\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Reshape the output to its original shape\n",
    "        y_pred = y_pred.reshape(B, T, C)\n",
    "        return y_pred\n",
    "\n",
    "def loss_fn(model, x, y):\n",
    "    y_pred  = model(x)\n",
    "    loss = jnp.mean((y - y_pred)**2)\n",
    "    return loss, y_pred\n",
    "\n",
    "@nnx.jit\n",
    "def step(state, x, y):\n",
    "    (loss, y_pred), grads = nnx.value_and_grad(\n",
    "        loss_fn, has_aux=True)(state.model, x, y)\n",
    "    state.update(grads)\n",
    "    return loss, grads, y_pred\n",
    "\n",
    "D, B, T, C =  1000, config.n_experts, 5, config.n_embed \n",
    "   \n",
    "default = jax.random.key(69)\n",
    "gate_noise = jax.random.key(42)\n",
    "rngs = nnx.Rngs(default=default, gate_noise=gate_noise)\n",
    "\n",
    "model = MOE(config, rngs)\n",
    "model.train(add_noise=False)\n",
    "tx = optax.adam(1e-2)\n",
    "state = nnx.Optimizer(model, tx)\n",
    "\n",
    "x = jax.random.normal(jax.random.key(1000), (D, B, T, C))\n",
    "\n",
    "expert_ids = (x[:, :, :, 0] > 0).astype(jnp.int32)[..., None]\n",
    "t = [\n",
    "    jax.random.normal(jax.random.key(2000), (C, C)),\n",
    "    jax.random.normal(jax.random.key(3000), (C, C)),\n",
    "]\n",
    "\n",
    "def transform(xi, eid):\n",
    "    return jnp.where(eid == 1, xi @ t[0], xi @ t[1])\n",
    "\n",
    "y = jax.vmap(lambda xi, ei: transform(xi, ei))(x, expert_ids)\n",
    "#x = x.reshape(D, B, T, C)\n",
    "#y = y.reshape(D, B, T, C)\n",
    "\n",
    "indices = list(range(D))\n",
    "for e in range(100):\n",
    "    for i in indices:\n",
    "        loss, grads, y_pred = step(state, x[i], y[i])\n",
    "        if i % 1000 == 0:\n",
    "            print(e, i, loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "high-performance-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
