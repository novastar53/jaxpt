{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Rimj9gIhA6eP"
   },
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oopQDeP1BrG"
   },
   "source": [
    "### Jax Pallas Experimental Flash Attention\n",
    "\n",
    "Github: https://github.com/jax-ml/jax/blob/922935a916dbcf599226f1ce3081feb6481328c3/jax/experimental/pallas/ops/gpu/attention.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "svLxillxA6eQ"
   },
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "max_seq_len = 64\n",
    "B, T, nH, C = 32, 15, 8, 128\n",
    "\n",
    "q = jax.random.normal(rng, (B, T, nH, C), dtype=jnp.bfloat16)\n",
    "k = jax.random.normal(rng, (B, T, nH, C), dtype=jnp.bfloat16)\n",
    "v = jax.random.normal(rng, (B, T, nH, C), dtype=jnp.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "gRYpO2Rsz-F7",
    "outputId": "fe0d291c-5143-41f7-9b81-38ec560e2f6b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "684 μs ± 53.7 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from jax.experimental.pallas.ops.gpu.attention import mha\n",
    "\n",
    "@dataclasses.dataclass(frozen=True, slots=True)\n",
    "class BlockSizes:\n",
    "  block_q: int\n",
    "  block_k: int\n",
    "\n",
    "pad_width = ((0, 0),  # no padding on the first dimension\n",
    "             (0, max_seq_len-T),\n",
    "             (0, 0),\n",
    "             (0, 0))  # pad two zeros on the right side of the second dimension\n",
    "\n",
    "q = jnp.pad(q, pad_width, mode='constant', constant_values=0)\n",
    "k = jnp.pad(k, pad_width, mode='constant', constant_values=0)\n",
    "v = jnp.pad(v, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "block_sizes = BlockSizes(block_q=T, block_k=T)\n",
    "\n",
    "#y = mha(q, k, v, block_sizes=block_sizes, segment_ids=None, causal=True)\n",
    "%timeit mha(q, k, v, segment_ids=None, causal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNAUZqCs039P"
   },
   "source": [
    "### FlashAttention Jax\n",
    "\n",
    "Github: https://github.com/nshepperd/flash_attn_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PxmyhcWHA6eQ",
    "outputId": "4c84b6e4-fe76-45d9-c89a-af2c8172ac8a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m186 packages\u001b[0m \u001b[2min 137ms\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K  \u001b[31m×\u001b[0m Failed to build `flash-attn-jax==0.2.2`                                             \n",
      "\u001b[31m  ├─▶ \u001b[0mThe build backend returned an error\n",
      "\u001b[31m  ╰─▶ \u001b[0mCall to `setuptools.build_meta:__legacy__.build_wheel` failed (exit\n",
      "\u001b[31m      \u001b[0mstatus: 1)\n",
      "\n",
      "\u001b[31m      \u001b[0m\u001b[31m[stderr]\u001b[39m\n",
      "\u001b[31m      \u001b[0mfatal: invalid gitfile format: /home/ubuntu/.cache/uv/sdists-v8/.git\n",
      "\u001b[31m      \u001b[0mTraceback (most recent call last):\n",
      "\u001b[31m      \u001b[0m  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m14\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "\u001b[31m      \u001b[0m    requires = get_requires_for_build({})\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/ubuntu/.cache/uv/builds-v0/.tmp82FD9r/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m334\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "\u001b[31m      \u001b[0m    return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "\u001b[31m      \u001b[0m           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/ubuntu/.cache/uv/builds-v0/.tmp82FD9r/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m304\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "\u001b[31m      \u001b[0m    \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "\u001b[31m      \u001b[0m    \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/ubuntu/.cache/uv/builds-v0/.tmp82FD9r/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m522\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "\u001b[31m      \u001b[0m    \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "\u001b[31m      \u001b[0m    \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/ubuntu/.cache/uv/builds-v0/.tmp82FD9r/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m320\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "\u001b[31m      \u001b[0m    \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "\u001b[31m      \u001b[0m    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[31m      \u001b[0m  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m94\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "\u001b[31m      \u001b[0m  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m72\u001b[0m, in \u001b[35mget_cuda_version\u001b[0m\n",
      "\u001b[31m      \u001b[0m  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m67\u001b[0m, in \u001b[35mlocate_cuda\u001b[0m\n",
      "\u001b[31m      \u001b[0m\u001b[1;35mRuntimeError\u001b[0m: \u001b[35mCUDA_HOME not set and no CUDA installation found\u001b[0m\n",
      "\n",
      "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m This usually indicates a problem with the package or the build\n",
      "\u001b[31m      \u001b[0menvironment.\n",
      "\u001b[36m  help: \u001b[0mIf you want to add the package regardless of the failed resolution,\n",
      "        provide the `\u001b[32m--frozen\u001b[39m` flag to skip locking and syncing.\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_HOME=/usr/\n",
    "!uv add flash-attn-jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZyYugo5zrtM",
    "outputId": "ed3ca7a8-415e-4789-bfaf-99b0694bc586"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flash_attn_jax'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mflash_attn_jax\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m flash_mha\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# flash_mha : [n, l, h, d] x [n, lk, hk, d] x [n, lk, hk, d] -> [n, l, h, d]\u001b[39;00m\n\u001b[32m      4\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mtimeit\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mflash_mha(q,k,v,softmax_scale=None, is_causal=True, window_size=(-1,-1))\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'flash_attn_jax'"
     ]
    }
   ],
   "source": [
    "from flash_attn_jax import flash_mha\n",
    "\n",
    "# flash_mha : [n, l, h, d] x [n, lk, hk, d] x [n, lk, hk, d] -> [n, l, h, d]\n",
    "%timeit flash_mha(q,k,v,softmax_scale=None, is_causal=True, window_size=(-1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EE6QbGxF1ZqU"
   },
   "source": [
    "### Flash Attention - Jax\n",
    "Github: https://github.com/lucidrains/flash-attention-jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xDaFGTJMz5ap",
    "outputId": "1dbf4f8b-0f50-4591-bd9b-ecd63e0394d0"
   },
   "outputs": [],
   "source": [
    "!pip install flash-attention-jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W-XcdGXE1fPG",
    "outputId": "2693316b-7287-4cce-ae90-38198ea23852"
   },
   "outputs": [],
   "source": [
    "from flash_attention_jax import causal_flash_attention\n",
    "\n",
    "rng_key = jax.random.PRNGKey(42)\n",
    "\n",
    "%timeit causal_flash_attention(q, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NcHgqJz27Xe"
   },
   "source": [
    "### Kvax\n",
    "\n",
    "Github: https://github.com/nebius/kvax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 739
    },
    "id": "i1JVllNH1ut1",
    "outputId": "3052a780-1474-4cd3-a92d-d23abdcae568"
   },
   "outputs": [],
   "source": [
    "!pip install kvax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUNuqQS63D-_"
   },
   "source": [
    "### Nvidia TransformerEngine\n",
    "\n",
    "Github: https://github.com/NVIDIA/TransformerEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/NVIDIA/TransformerEngine.git@stable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.13 (jaxpt)",
   "language": "python",
   "name": "jaxpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
