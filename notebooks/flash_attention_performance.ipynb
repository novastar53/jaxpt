{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Rimj9gIhA6eP"
   },
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oopQDeP1BrG"
   },
   "source": [
    "### Jax Pallas Experimental Flash Attention\n",
    "\n",
    "Github: https://github.com/jax-ml/jax/blob/922935a916dbcf599226f1ce3081feb6481328c3/jax/experimental/pallas/ops/gpu/attention.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "svLxillxA6eQ"
   },
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "max_seq_len = 64\n",
    "B, T, nH, C = 32, 15, 8, 128\n",
    "\n",
    "q = jax.random.normal(rng, (B, T, nH, C), dtype=jnp.bfloat16)\n",
    "k = jax.random.normal(rng, (B, T, nH, C), dtype=jnp.bfloat16)\n",
    "v = jax.random.normal(rng, (B, T, nH, C), dtype=jnp.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "gRYpO2Rsz-F7",
    "outputId": "fe0d291c-5143-41f7-9b81-38ec560e2f6b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from jax.experimental.pallas.ops.gpu.attention import mha\n",
    "\n",
    "@dataclasses.dataclass(frozen=True, slots=True)\n",
    "class BlockSizes:\n",
    "  block_q: int\n",
    "  block_k: int\n",
    "\n",
    "pad_width = ((0, 0),  # no padding on the first dimension\n",
    "             (0, max_seq_len-T),\n",
    "             (0, 0),\n",
    "             (0, 0))  # pad two zeros on the right side of the second dimension\n",
    "\n",
    "q = jnp.pad(q, pad_width, mode='constant', constant_values=0)\n",
    "k = jnp.pad(k, pad_width, mode='constant', constant_values=0)\n",
    "v = jnp.pad(v, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "block_sizes = BlockSizes(block_q=T, block_k=T)\n",
    "\n",
    "#y = mha(q, k, v, block_sizes=block_sizes, segment_ids=None, causal=True)\n",
    "%timeit mha(q, k, v, segment_ids=None, causal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNAUZqCs039P"
   },
   "source": [
    "### FlashAttention Jax\n",
    "\n",
    "Github: https://github.com/nshepperd/flash_attn_jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PxmyhcWHA6eQ",
    "outputId": "4c84b6e4-fe76-45d9-c89a-af2c8172ac8a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!export CUDA_HOME=/usr/\n",
    "!uv add flash-attn-jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZyYugo5zrtM",
    "outputId": "ed3ca7a8-415e-4789-bfaf-99b0694bc586"
   },
   "outputs": [],
   "source": [
    "from flash_attn_jax import flash_mha\n",
    "\n",
    "# flash_mha : [n, l, h, d] x [n, lk, hk, d] x [n, lk, hk, d] -> [n, l, h, d]\n",
    "%timeit flash_mha(q,k,v,softmax_scale=None, is_causal=True, window_size=(-1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EE6QbGxF1ZqU"
   },
   "source": [
    "### Flash Attention - Jax\n",
    "Github: https://github.com/lucidrains/flash-attention-jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xDaFGTJMz5ap",
    "outputId": "1dbf4f8b-0f50-4591-bd9b-ecd63e0394d0"
   },
   "outputs": [],
   "source": [
    "!pip install flash-attention-jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W-XcdGXE1fPG",
    "outputId": "2693316b-7287-4cce-ae90-38198ea23852"
   },
   "outputs": [],
   "source": [
    "from flash_attention_jax import causal_flash_attention\n",
    "\n",
    "rng_key = jax.random.PRNGKey(42)\n",
    "\n",
    "%timeit causal_flash_attention(q, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NcHgqJz27Xe"
   },
   "source": [
    "### Kvax\n",
    "\n",
    "Github: https://github.com/nebius/kvax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 739
    },
    "id": "i1JVllNH1ut1",
    "outputId": "3052a780-1474-4cd3-a92d-d23abdcae568"
   },
   "outputs": [],
   "source": [
    "!pip install kvax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUNuqQS63D-_"
   },
   "source": [
    "### Nvidia TransformerEngine\n",
    "\n",
    "Github: https://github.com/NVIDIA/TransformerEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/NVIDIA/TransformerEngine.git@stable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.13 (jaxpt)",
   "language": "python",
   "name": "jaxpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
