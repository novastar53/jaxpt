{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXU2qDN8nMgg"
      },
      "source": [
        "# Let's Train a GPT 2 Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNLo9jfLn8bg",
        "outputId": "5b38a77b-a0f8-4ee9-9aba-480b4581e251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'jaxpt' already exists and is not an empty directory.\n",
            "Already on 'dev'\n",
            "Your branch is up to date with 'origin/dev'.\n"
          ]
        }
      ],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if is_colab():\n",
        "    !git clone https://github.com/novastar53/jaxpt\n",
        "    !cd jaxpt && git checkout dev\n",
        "    !pip install tiktoken --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQl8dEgLnMgh",
        "outputId": "d118713e-e758-4a3b-efba-ad88e8dbe553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jaxpt/jaxpt\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "if is_colab():\n",
        "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"jaxpt\" )\n",
        "else:\n",
        "    jaxpt_dir = str(Path().absolute().parent / \"jaxpt\" )\n",
        "\n",
        "sys.path.append(jaxpt_dir)\n",
        "print(jaxpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7N2-jnzonMgh"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from flax import nnx\n",
        "import tiktoken\n",
        "\n",
        "import torch\n",
        "\n",
        "import dataloaders as dl\n",
        "from models import GPT2, GPTConfig\n",
        "from train import train_step, loss_fn, compute_global_norm\n",
        "from infer import generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJo6Xji39g54",
        "outputId": "8bd0c47a-eb26-4134-b910-6216a438f320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version: 0.4.33\n",
            "Available devices: [CudaDevice(id=0)]\n",
            "Using device: gpu\n",
            "1.23 ms ± 11.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Hardware setup\n",
        "print(\"JAX version:\", jax.__version__)\n",
        "print(\"Available devices:\", jax.devices())\n",
        "\n",
        "jax.config.update(\"jax_platform_name\", \"gpu\") # Make sure we're using the GPU\n",
        "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
        "jax.config.update(\"jax_default_matmul_precision\", \"bfloat16\") # Set the default precision for matrix multiplication\n",
        "\n",
        "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
        "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
        "\n",
        "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
        "\n",
        "A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Makes sure the matmul is fast\n",
        "\n",
        "%timeit (A@A).block_until_ready()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lki2khsFnMgh",
        "outputId": "b391ed66-b44e-41a3-895a-647bcc02516e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> The brown fox harassedXM regulars Fareitaire idealWidepython dissatisfied BonusOSIIYoツ LED\u0005 Jess\n",
            "> The brown fox icing Gerrard powdshire OVER scientist NT Grav guise conflictetriclynn>>>>>>>> descriptorMom argues684\n",
            "> The brown fox decree Piano IntakeWin hopeful curJapanese discharged cartperhaps · TVs Schrcium Lau NK Toys\n",
            "> The brown fox beats viz Canberra help Lar membersbered configurehootingamara Artifact massac Cham pinnedBeast PrestForgeModLoader\n",
            "> The brown fox Minneapolis Serve footsteps hairc713 certificatesansky Sacredprisingly 332 Uzbek nobody Mountain faire PearlPASS Consolid\n"
          ]
        }
      ],
      "source": [
        "from functools import partial\n",
        "\n",
        "\"\"\"\n",
        "+--------------+---------+--------+------+\n",
        "| Model       | Layers  | Heads  | Embd |\n",
        "+--------------+---------+--------+------+\n",
        "| gpt2-medium | 24      | 16     | 1024 |\n",
        "| gpt2-large  | 36      | 20     | 1280 |\n",
        "| gpt2-xl     | 48      | 25     | 1600 |\n",
        "+--------------+---------+--------+------+\n",
        "\"\"\"\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "rngs = nnx.Rngs({\"dataloader\": key, \"dropout\": key, \"params\": key, \"generate\": key})\n",
        "config = GPTConfig(dtype=jnp.float32)\n",
        "m = GPT2(config, rngs)\n",
        "m.eval()\n",
        "\n",
        "\n",
        "num_completions = 5\n",
        "max_length = 20\n",
        "generate_completion = partial(generate, m, max_length=max_length)\n",
        "prefix = \"The brown fox\"\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "tokens = enc.encode(prefix)\n",
        "tokens = jnp.array(tokens, dtype=jnp.int32)\n",
        "tokens = jnp.expand_dims(tokens, axis=0)\n",
        "x = jnp.tile(tokens, (num_completions, 1))\n",
        "\n",
        "\n",
        "x = generate_completion(x=x) # Make sure you can do a forward pass\n",
        "for i in range(num_completions):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "49o2l_J3EzOL"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "dataset_path = Path().absolute() / \"jaxpt\" / \"datasets\" / \"panchatantra-ryder.txt\"\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "text = dl.load_text(dataset_path)\n",
        "data = jnp.array(enc.encode(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "k8Dx19aKnMgi",
        "outputId": "6af74d83-4f60-4298-9e5c-69edf2386a92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens/batch: 524288\n",
            "block size: 1024\n",
            "sub-batch size: 4\n",
            "no. gradient accumulation steps: 128\n",
            "effective batch size: 512\n",
            "max steps: 50\n"
          ]
        }
      ],
      "source": [
        "# Set up the optimizer\n",
        "\n",
        "\n",
        "num_tokens_per_batch = 2**19 # 2**19, 0.5 million as per the GPT 3.5 paper\n",
        "mB, T = 4, 1024\n",
        "grad_accumulation_steps = num_tokens_per_batch // (mB * T) # Number of steps over which to average the gradient\n",
        "print(f\"tokens/batch: {num_tokens_per_batch}\")\n",
        "print(f\"block size: {T}\")\n",
        "print(f\"sub-batch size: {mB}\")\n",
        "print(f\"no. gradient accumulation steps: {grad_accumulation_steps}\")\n",
        "print(f\"effective batch size: {grad_accumulation_steps*mB}\")\n",
        "\n",
        "\n",
        "max_steps = 50\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "\n",
        "print(f\"max steps: {max_steps}\")\n",
        "\n",
        "def warmup_with_cosine_decay_schedule(step):\n",
        "\n",
        "    warmup_lr = max_lr * (step + 1) / warmup_steps\n",
        "\n",
        "    coeff = 0.5 * (1 + jnp.cos(jnp.pi * (step - warmup_steps) / (max_steps - warmup_steps)))\n",
        "    cosine_lr =  min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "    return jnp.where(step < warmup_steps,\n",
        "                     warmup_lr,\n",
        "                     jnp.where(step < max_steps, cosine_lr, min_lr))\n",
        "\n",
        "max_grad_norm = 1.0  # Clip gradients to this norm\n",
        "optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(max_grad_norm),\n",
        "    optax.adamw(warmup_with_cosine_decay_schedule, b1=0.9, b2=0.95)\n",
        ")\n",
        "optimizer = nnx.Optimizer(m, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "hx5gzrQ1Zx0M"
      },
      "outputs": [],
      "source": [
        "@nnx.jit(donate_argnames=(\"accum_grad\",), static_argnames=(\"B\", \"T\"))\n",
        "def accum_step(model, data, B, T, accum_grad, accum_loss, rng):\n",
        "    k = jax.random.randint(rng, (1,), 0, len(data) - B*T - 1)[0]\n",
        "    batch = jax.lax.dynamic_slice(data, (k,), (B*T,)).reshape((B, T))\n",
        "    targets = jax.lax.dynamic_slice(data, (k+1,), (B*T,)).reshape((B, T))\n",
        "    loss, grads =  nnx.value_and_grad(loss_fn)(model, batch, targets)\n",
        "    if accum_grad is None:\n",
        "        accum_grad = jax.tree_util.tree_map(jnp.zeros_like, grads)\n",
        "    accum_grad = jax.tree_util.tree_map(lambda x, y: x + y, accum_grad, grads)\n",
        "    accum_loss = accum_loss + loss\n",
        "    return accum_grad, accum_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwtmfUotuLMU",
        "outputId": "5a23484b-b2cd-4965-802d-3f3f62a24e5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " step: 49 | lr: 6.08e-05 | loss: 4.0373 | norm: 0.5579 | time: 12323.99ms | tok/sec: 332.36\n",
            "CPU times: user 4min 45s, sys: 26.6 s, total: 5min 11s\n",
            "Wall time: 10min 50s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from time import time\n",
        "from IPython.display import clear_output\n",
        "from functools import partial\n",
        "\n",
        "import gc\n",
        "\n",
        "accum_step = partial(accum_step, model=m, data=data, B=mB, T=T)\n",
        "m.train()\n",
        "\n",
        "for step in range(max_steps):\n",
        "  start = time()\n",
        "  key, subkey = jax.random.split(key)\n",
        "  accum_grad =  None\n",
        "  accum_loss = 0.0\n",
        "  for sub_step in range(grad_accumulation_steps):\n",
        "    accum_grad, accum_loss = accum_step(accum_grad=accum_grad, accum_loss=accum_loss, rng=subkey)\n",
        "    jax.block_until_ready(accum_grad)\n",
        "  accum_norm = compute_global_norm(accum_grad)\n",
        "  print(f\"accum_norm: {accum_norm}, accum_loss: {accum_loss}\")\n",
        "  accum_grad = jax.tree_util.tree_map(lambda x: x / grad_accumulation_steps, accum_grad)\n",
        "  optimizer.update(accum_grad)\n",
        "  norm = compute_global_norm(accum_grad)\n",
        "  loss = accum_loss / grad_accumulation_steps\n",
        "  jax.block_until_ready(loss)\n",
        "  iter_time = time() - start\n",
        "  tokens_per_sec = mB*T / iter_time\n",
        "  lr = warmup_with_cosine_decay_schedule(step)\n",
        "  clear_output(wait=True)\n",
        "  print(f\" step: {step} | lr: {lr:0.2e} | loss: {loss:0.4f} | norm: {norm:0.4f} | time: {iter_time*1000:0.2f}ms | tok/sec: {tokens_per_sec:0.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dlRJKiVIZx0N"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "jaxpt",
      "language": "python",
      "name": "jaxpt"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}