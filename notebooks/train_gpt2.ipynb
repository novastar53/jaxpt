{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXU2qDN8nMgg"
   },
   "source": [
    "# Let's Train GPT-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LNLo9jfLn8bg",
    "outputId": "9eb6bc93-7a88-4a8c-8a8b-5c7593bbe32b"
   },
   "outputs": [],
   "source": [
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if is_colab():\n",
    "    !git clone https://github.com/novastar53/jaxpt\n",
    "    !cd jaxpt && git checkout dev && git pull\n",
    "    !pip install tiktoken --quiet\n",
    "    !pip uninstall -y tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQl8dEgLnMgh",
    "outputId": "f646af7e-38c4-40ed-8b18-e423e0584a60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vikram/dev/jaxpt/src\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "if is_colab():\n",
    "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"src\" )\n",
    "else:\n",
    "    jaxpt_dir = str(Path().absolute().parent / \"src\" )\n",
    "\n",
    "sys.path.append(jaxpt_dir)\n",
    "print(jaxpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7N2-jnzonMgh"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax import nnx\n",
    "import tiktoken\n",
    "\n",
    "from jaxpt.dataloaders import DataLoader\n",
    "from jaxpt.models import GPT2, GPTConfig\n",
    "from jaxpt.train import train_step, loss_fn, compute_global_norm\n",
    "from jaxpt.infer import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04pFw2g58HJl"
   },
   "source": [
    "### Configure compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJo6Xji39g54",
    "outputId": "66de6e21-02e6-4a65-fae3-48bd4a668c52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.5.2\n",
      "Available devices: 1\n",
      "Using device: cpu\n",
      "169 ms ± 4.03 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Hardware setup\n",
    "print(\"JAX version:\", jax.__version__)\n",
    "devices = jax.devices()\n",
    "num_devices = len(devices)\n",
    "print(\"Available devices:\", num_devices)\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", \"gpu\") # Make sure we're using the GPU\n",
    "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"bfloat16\") # Set the default precision for matrix multiplication\n",
    "\n",
    "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
    "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
    "\n",
    "def list_tpu_memory():\n",
    "    devices = jax.devices()\n",
    "    for device in devices:\n",
    "        if 'TPU' in str(device.device_kind):\n",
    "            print(f\"Device: {device}, Memory: {device.memory_stats()['bytes_limit']/(1024*1024)},  Used: {device.memory_stats()['bytes_in_use']/(1024*1024)}\")\n",
    "\n",
    "#list_tpu_memory()\n",
    "\n",
    "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
    "\n",
    "A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Makes sure the matmul is fast\n",
    "\n",
    "%timeit (A@A).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzoCvstr9_WX"
   },
   "source": [
    "### Initialize the GPT-2 model and perform a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lki2khsFnMgh"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\"\"\"\n",
    "+--------------+---------+--------+------+\n",
    "| Model        | Layers  | Heads  | Embd |\n",
    "+--------------+---------+--------+------+\n",
    "| gpt2-medium  | 24      | 16     | 1024 |\n",
    "| gpt2-large   | 36      | 20     | 1280 |\n",
    "| gpt2-xl      | 48      | 25     | 1600 |\n",
    "+--------------+---------+--------+------+\n",
    "\"\"\"\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "rngs = nnx.Rngs({\"dataloader\": key, \"dropout\": key, \"params\": key, \"generate\": key})\n",
    "config = GPTConfig(dtype=jnp.float32)\n",
    "m = GPT2(config, rngs)\n",
    "\n",
    "def generate_completions():\n",
    "  m.eval()\n",
    "  num_completions = 5\n",
    "  max_length = 20\n",
    "  generate_completion = partial(generate, m, max_length=max_length)\n",
    "  prefix = \"The clever jackal\"\n",
    "  enc = tiktoken.get_encoding('gpt2')\n",
    "  tokens = enc.encode(prefix)\n",
    "  tokens = jnp.array(tokens, dtype=jnp.int32)\n",
    "  tokens = jnp.expand_dims(tokens, axis=0)\n",
    "  x = jnp.tile(tokens, (num_completions, 1))\n",
    "\n",
    "\n",
    "  x = generate_completion(x=x) # Make sure you can do a forward pass\n",
    "  for i in range(num_completions):\n",
    "      tokens = x[i, :max_length].tolist()\n",
    "      decoded = enc.decode(tokens)\n",
    "      print(\">\", decoded)\n",
    "\n",
    "#generate_completions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHA3hbjj8HJl"
   },
   "source": [
    "### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8Dx19aKnMgi",
    "outputId": "c6d6d2ea-13ad-4b6a-fdd8-1980047d06c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens/batch: 256\n",
      "block size: 32\n",
      "sub-batch size: 4\n",
      "no. gradient accumulation steps: 2\n",
      "effective batch size per device:  8\n",
      "effective batch size: 8\n",
      "max steps: 50\n",
      "weight decay param count: 124,318,464\n"
     ]
    }
   ],
   "source": [
    "num_tokens_per_batch = 2**8 # 2**19, 0.5 million as per the GPT 3.5 paper\n",
    "mB, T = 4, 32\n",
    "grad_accumulation_steps = num_tokens_per_batch // (mB * T * num_devices) # Number of steps over which to average the gradient\n",
    "print(f\"tokens/batch: {num_tokens_per_batch:,}\")\n",
    "print(f\"block size: {T}\")\n",
    "print(f\"sub-batch size: {mB}\")\n",
    "print(f\"no. gradient accumulation steps: {grad_accumulation_steps}\")\n",
    "print(f\"effective batch size per device: \", grad_accumulation_steps * mB)\n",
    "print(f\"effective batch size: {grad_accumulation_steps * mB * num_devices}\")\n",
    "\n",
    "\n",
    "max_steps = 50 #19073\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 10 #715\n",
    "\n",
    "print_interval = 1 # 10\n",
    "eval_interval = 20 #100\n",
    "\n",
    "print(f\"max steps: {max_steps}\")\n",
    "\n",
    "# Set up the optimizer\n",
    "def warmup_with_cosine_decay_schedule(step):\n",
    "\n",
    "    warmup_lr = max_lr * (step + 1) / warmup_steps\n",
    "\n",
    "    coeff = 0.5 * (1 + jnp.cos(jnp.pi * (step - warmup_steps) / (max_steps - warmup_steps)))\n",
    "    cosine_lr =  min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "    return jnp.where(step < warmup_steps,\n",
    "                     warmup_lr,\n",
    "                     jnp.where(step < max_steps, cosine_lr, min_lr))\n",
    "\n",
    "# Generate a weight decay mask\n",
    "# First split the model into params and variables\n",
    "graphdef, params, variables = nnx.split(m, nnx.Param, nnx.Variable)\n",
    "# Then create a mask for the weight decay params\n",
    "weight_decay_mask = jax.tree_util.tree_map(lambda x: len(x.shape) > 1, params)\n",
    "\n",
    "def f(x, y):\n",
    "    if x:\n",
    "        return y.size\n",
    "    return 0\n",
    "\n",
    "weight_decay_params = jax.tree_util.tree_map(f, weight_decay_mask, params)\n",
    "weight_decay_param_count = jax.tree_util.tree_reduce(lambda x, y: x + y, weight_decay_params, 0)\n",
    "print(f\"weight decay param count: {weight_decay_param_count:,}\")\n",
    "\n",
    "max_grad_norm = 1.0  # Clip gradients to this norm\n",
    "\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(max_grad_norm),\n",
    "    optax.adamw(warmup_with_cosine_decay_schedule, b1=0.9, b2=0.95, weight_decay=0.1, mask=weight_decay_mask)\n",
    ")\n",
    "optimizer = nnx.Optimizer(m, tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader and Validation Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_8d4oBtGEwpy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader initialized:\n",
      "------------------------\n",
      "label:          train\n",
      "shards:         1\n",
      "shard size:     146,776\n",
      "batch size:     4\n",
      "block size:     32\n",
      "device rank:    1\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def print_separator(title=None):\n",
    "    width = 80\n",
    "    border = \"═\" * width\n",
    "    if title:\n",
    "        padding = \"═\" * ((width - len(title) - 2) // 2)\n",
    "        print(f\"╔{border}╗\")\n",
    "        print(f\"║{padding} {title} {padding}║\")\n",
    "        print(f\"╚{border}╝\")\n",
    "    else:\n",
    "        print(f\"╔{border}╗\")\n",
    "        print(f\"╚{border}╝\")\n",
    "\n",
    "dataset = \"panchatantra-ryder\"\n",
    "\n",
    "if is_colab():\n",
    "    dataset_path = Path().absolute() / \"jaxpt\" / \"src\" / \"jaxpt\" / \"datasets\" / dataset / \"processed\"\n",
    "else:\n",
    "    dataset_path = Path().absolute().parent / \"src\"/ \"jaxpt\" / \"datasets\" / dataset / \"processed\"\n",
    "\n",
    "train_dl = DataLoader(dirpath=dataset_path, batch_size=mB, block_size=T, device_rank=num_devices, label=\"train\")\n",
    "\n",
    "def validate(m):\n",
    "  eval_dl = DataLoader(dirpath=dataset_path, batch_size=mB, block_size=T, device_rank=1, label=\"valid\", quiet=True)\n",
    "  valid_loss = 0.0\n",
    "  eval_steps = 10\n",
    "  for i in range(eval_steps):\n",
    "    batch, targets = eval_dl()\n",
    "    batch = np.squeeze(batch)\n",
    "    targets = np.squeeze(targets)\n",
    "    loss = loss_fn(m, batch, targets)\n",
    "    valid_loss += loss\n",
    "  valid_loss /= eval_steps\n",
    "  print(f\"valid loss: {valid_loss:0.4f}\")\n",
    "\n",
    "\n",
    "def evaluate(m):\n",
    "  print_separator(\"Evaluate\")\n",
    "  m.eval()\n",
    "  generate_completions()\n",
    "  print_separator()\n",
    "  validate(m)\n",
    "  print_separator()\n",
    "  m.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UwtmfUotuLMU",
    "outputId": "a2ec8867-a187-4089-b41c-bcdbd6786aa2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "> The clever jackal encounterrunnersTouch Director virginity OriginalAmazing GR advocacy catalyst NarOfhidden031eners Natural\n",
      "> The clever jackal hygienerar DodgeACogen CertificationAim Hagueumes Nashville immortality Rouge bullpen hottestollahhiro\n",
      "> The clever jackalactivated Alc backdrop.):DM bedrock Cemetery Sonic presum devilanny Returns drown guidance apparel Hawks\n",
      "> The clever jackal unpaid Pistons sched nomineTalkhyp realization rectangle aides saucesSn tendencies memoir muff childbirth shuffle\n",
      "> The clever jackalPlatform Jol Prestandel slides,)iche snourn curb lives shuffle Cut MF POLIT deceased\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "valid loss: 10.9706\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "0 | lr: 6.00e-05 | loss: 10.9505 | norm: 10.43 | time: 14587.18ms | tokens processed: 256 | tok/sec: 17.55\n",
      "1 | lr: 1.20e-04 | loss: 9.9866 | norm: 8.47 | time: 54.68ms | tokens processed: 512 | tok/sec: 4,681.43\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "> The clever jackal Penn  of migraineScar. purportedcrypt, Love presentlyークaredunits of Jet\n",
      "> The clever jackal Guns clearing sounded Love\\)ouslyourcingunitshhhhared ends inunits Pahhhh674\n",
      "> The clever jackal SET ends virtue is Symptoms456 be,aredークinvoke seven Double Hang disproportion.\n",
      "> The clever jackalheat extent durhhhhously ofoshosh jointly bribe 60Scar seven MAX\n",
      " Sparkle\n",
      "> The clever jackal sounded Motorsport-$ is is Cas while deflect DiaryScar oke\n",
      " DataTonight\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "valid loss: 9.0431\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "2 | lr: 1.80e-04 | loss: 9.1073 | norm: 6.11 | time: 55.92ms | tokens processed: 768 | tok/sec: 4,578.04\n",
      "3 | lr: 2.40e-04 | loss: 9.2404 | norm: 8.97 | time: 58.37ms | tokens processed: 1,024 | tok/sec: 4,385.88\n",
      "4 | lr: 3.00e-04 | loss: 9.0612 | norm: 5.68 | time: 23.07ms | tokens processed: 1,280 | tok/sec: 11,094.32\n",
      "5 | lr: 3.60e-04 | loss: 8.7291 | norm: 4.27 | time: 66.23ms | tokens processed: 1,536 | tok/sec: 3,865.61\n",
      "6 | lr: 4.20e-04 | loss: 8.6241 | norm: 3.50 | time: 22.08ms | tokens processed: 1,792 | tok/sec: 11,595.23\n",
      "7 | lr: 4.80e-04 | loss: 8.5476 | norm: 4.03 | time: 27.26ms | tokens processed: 2,048 | tok/sec: 9,392.75\n",
      "8 | lr: 5.40e-04 | loss: 7.5714 | norm: 3.08 | time: 25.48ms | tokens processed: 2,304 | tok/sec: 10,045.96\n",
      "9 | lr: 6.00e-04 | loss: 7.8916 | norm: 3.21 | time: 24.37ms | tokens processed: 2,560 | tok/sec: 10,506.79\n",
      "10 | lr: 6.00e-04 | loss: 7.8610 | norm: 4.29 | time: 51.89ms | tokens processed: 2,816 | tok/sec: 4,933.14\n",
      "11 | lr: 5.99e-04 | loss: 6.8746 | norm: 2.47 | time: 18.85ms | tokens processed: 3,072 | tok/sec: 13,583.07\n",
      "12 | lr: 5.97e-04 | loss: 6.8332 | norm: 2.11 | time: 34.67ms | tokens processed: 3,328 | tok/sec: 7,383.73\n",
      "13 | lr: 5.93e-04 | loss: 7.0046 | norm: 2.19 | time: 20.12ms | tokens processed: 3,584 | tok/sec: 12,723.72\n",
      "14 | lr: 5.87e-04 | loss: 7.1181 | norm: 2.57 | time: 40.04ms | tokens processed: 3,840 | tok/sec: 6,393.45\n",
      "15 | lr: 5.79e-04 | loss: 6.4476 | norm: 2.93 | time: 21.96ms | tokens processed: 4,096 | tok/sec: 11,660.21\n",
      "16 | lr: 5.71e-04 | loss: 6.0479 | norm: 2.25 | time: 54.57ms | tokens processed: 4,352 | tok/sec: 4,690.82\n",
      "17 | lr: 5.60e-04 | loss: 6.3188 | norm: 3.02 | time: 80.21ms | tokens processed: 4,608 | tok/sec: 3,191.51\n",
      "18 | lr: 5.48e-04 | loss: 6.2941 | norm: 7.61 | time: 29.19ms | tokens processed: 4,864 | tok/sec: 8,769.82\n",
      "19 | lr: 5.35e-04 | loss: 6.3750 | norm: 2.79 | time: 17.30ms | tokens processed: 5,120 | tok/sec: 14,797.78\n",
      "20 | lr: 5.21e-04 | loss: 6.0434 | norm: 2.56 | time: 21.97ms | tokens processed: 5,376 | tok/sec: 11,652.74\n",
      "21 | lr: 5.05e-04 | loss: 6.6251 | norm: 2.76 | time: 27.51ms | tokens processed: 5,632 | tok/sec: 9,306.05\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "> The clever jackal- against not said: THE THE, and kings toCHATA to I\n",
      "> The clever jackalANT a Victor'sOSS with.\" a as be of as kingsRA allek\n",
      "> The clever jackalRA to you of he in's: or and this his as you or:\n",
      "> The clever jackal you,\"CHAT it no of THE kings Victor Che it to notOSS. man\n",
      "> The clever jackalA,\" master;'sOSS THE masterek ofANT.,\"  Minute I\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "valid loss: 6.7046\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "22 | lr: 4.89e-04 | loss: 6.6689 | norm: 7.85 | time: 38.62ms | tokens processed: 5,888 | tok/sec: 6,628.04\n",
      "23 | lr: 4.71e-04 | loss: 5.8841 | norm: 2.55 | time: 44.02ms | tokens processed: 6,144 | tok/sec: 5,814.98\n",
      "24 | lr: 4.53e-04 | loss: 6.4018 | norm: 3.37 | time: 30.03ms | tokens processed: 6,400 | tok/sec: 8,525.35\n",
      "25 | lr: 4.33e-04 | loss: 6.2678 | norm: 3.61 | time: 18.10ms | tokens processed: 6,656 | tok/sec: 14,144.56\n",
      "26 | lr: 4.13e-04 | loss: 6.2232 | norm: 3.83 | time: 28.01ms | tokens processed: 6,912 | tok/sec: 9,138.07\n",
      "27 | lr: 3.93e-04 | loss: 6.3970 | norm: 2.42 | time: 24.54ms | tokens processed: 7,168 | tok/sec: 10,430.25\n",
      "28 | lr: 3.72e-04 | loss: 6.2900 | norm: 2.91 | time: 24.32ms | tokens processed: 7,424 | tok/sec: 10,525.95\n",
      "29 | lr: 3.51e-04 | loss: 6.3803 | norm: 2.53 | time: 17.94ms | tokens processed: 7,680 | tok/sec: 14,268.24\n",
      "30 | lr: 3.30e-04 | loss: 6.5753 | norm: 2.25 | time: 28.25ms | tokens processed: 7,936 | tok/sec: 9,062.03\n",
      "31 | lr: 3.09e-04 | loss: 5.9676 | norm: 1.89 | time: 34.54ms | tokens processed: 8,192 | tok/sec: 7,412.12\n",
      "32 | lr: 2.88e-04 | loss: 6.3552 | norm: 2.19 | time: 24.18ms | tokens processed: 8,448 | tok/sec: 10,585.62\n",
      "33 | lr: 2.67e-04 | loss: 6.0870 | norm: 1.70 | time: 17.89ms | tokens processed: 8,704 | tok/sec: 14,307.97\n",
      "34 | lr: 2.47e-04 | loss: 5.9488 | norm: 1.86 | time: 22.87ms | tokens processed: 8,960 | tok/sec: 11,194.61\n",
      "35 | lr: 2.27e-04 | loss: 6.1007 | norm: 1.68 | time: 26.70ms | tokens processed: 9,216 | tok/sec: 9,588.78\n",
      "36 | lr: 2.07e-04 | loss: 6.1312 | norm: 1.84 | time: 24.84ms | tokens processed: 9,472 | tok/sec: 10,307.59\n",
      "37 | lr: 1.89e-04 | loss: 6.3386 | norm: 1.96 | time: 20.30ms | tokens processed: 9,728 | tok/sec: 12,610.74\n",
      "38 | lr: 1.71e-04 | loss: 6.4398 | norm: 1.81 | time: 21.01ms | tokens processed: 9,984 | tok/sec: 12,183.61\n",
      "39 | lr: 1.55e-04 | loss: 6.3063 | norm: 1.87 | time: 27.43ms | tokens processed: 10,240 | tok/sec: 9,331.85\n",
      "40 | lr: 1.39e-04 | loss: 6.1834 | norm: 2.04 | time: 19.38ms | tokens processed: 10,496 | tok/sec: 13,211.06\n",
      "41 | lr: 1.25e-04 | loss: 6.5200 | norm: 2.06 | time: 50.88ms | tokens processed: 10,752 | tok/sec: 5,031.14\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "> The clever jackal when Of Victor him:; as.- sound to,\" sound and,\"\n",
      "> The clever jackal master.\" of \" me; not.\" when you in when on?\" that good\n",
      "> The clever jackal as \" said he of For to: And Rusty by he's is;.\n",
      "> The clever jackal it was on And this Rusty?\" on Victor?\" And to of was,,\"\n",
      "> The clever jackal,\" master not the to me king his when in master, at  royal from\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "valid loss: 6.3068\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "42 | lr: 1.12e-04 | loss: 6.1061 | norm: 1.77 | time: 38.21ms | tokens processed: 11,008 | tok/sec: 6,700.67\n",
      "43 | lr: 9.98e-05 | loss: 5.9523 | norm: 2.09 | time: 311.19ms | tokens processed: 11,264 | tok/sec: 822.64\n",
      "44 | lr: 8.94e-05 | loss: 6.3119 | norm: 2.75 | time: 22.05ms | tokens processed: 11,520 | tok/sec: 11,609.40\n",
      "45 | lr: 8.06e-05 | loss: 6.3812 | norm: 1.80 | time: 21.09ms | tokens processed: 11,776 | tok/sec: 12,138.44\n",
      "46 | lr: 7.32e-05 | loss: 6.0876 | norm: 2.00 | time: 20.59ms | tokens processed: 12,032 | tok/sec: 12,432.03\n",
      "47 | lr: 6.75e-05 | loss: 6.5567 | norm: 2.53 | time: 33.80ms | tokens processed: 12,288 | tok/sec: 7,573.99\n",
      "48 | lr: 6.33e-05 | loss: 6.0362 | norm: 1.88 | time: 19.06ms | tokens processed: 12,544 | tok/sec: 13,429.16\n",
      "49 | lr: 6.08e-05 | loss: 6.7293 | norm: 2.05 | time: 19.07ms | tokens processed: 12,800 | tok/sec: 13,423.62\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "> The clever jackal at For Rusty \"  PAN are..\"?- again good-ANT\n",
      "> The clever jackal was of Rusty he are; or of with not a will there as that by\n",
      "> The clever jackal as- said a is?\" he the or to there a at this; the\n",
      "> The clever jackal not with there that him a L from Rusty as; he I are, will\n",
      "> The clever jackal will with his:: Victor  FRI with a that, that  king?\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "valid loss: 6.2574\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "CPU times: user 6min, sys: 1min 11s, total: 7min 12s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from time import time\n",
    "from IPython.display import clear_output\n",
    "from functools import partial\n",
    "\n",
    "evaluate(m)\n",
    "m.train()\n",
    "\n",
    "try:\n",
    "  for step in range(max_steps):\n",
    "    start = time()\n",
    "    batches, targets = [], []\n",
    "    for i in range(grad_accumulation_steps):\n",
    "      batch, target = train_dl()\n",
    "      batches.append(batch)\n",
    "      targets.append(target)\n",
    "    avg_loss, avg_grads, norm = train_step(m, optimizer, batches, targets)\n",
    "\n",
    "    # compute stats\n",
    "    loss = avg_loss[0]\n",
    "    lr = warmup_with_cosine_decay_schedule(step)\n",
    "    norm = norm[0]\n",
    "    iter_time = time() - start\n",
    "    sub_step_time = iter_time / grad_accumulation_steps\n",
    "    tokens_per_sec = num_devices * mB * T * grad_accumulation_steps / iter_time\n",
    "    tokens_processed = (step+1) * num_devices * grad_accumulation_steps * mB * T\n",
    "\n",
    "    if step % print_interval == 0:\n",
    "        print(f\"{step} | lr: {lr:0.2e} | loss: {loss:0.4f} | norm: {norm:0.2f} | time: {iter_time*1000:0.2f}ms | tokens processed: {tokens_processed:,} | tok/sec: {tokens_per_sec:,.2f}\")\n",
    "    if step % eval_interval == 1:\n",
    "      evaluate(m)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Received KeyboardInterrupt. Exiting...\")\n",
    "evaluate(m)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.13 (jaxpt)",
   "language": "python",
   "name": "jaxpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
