{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXU2qDN8nMgg"
   },
   "source": [
    "# Let's Train a GPT 2 Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LNLo9jfLn8bg",
    "outputId": "ca4ed65e-f279-422d-a1ab-00646e499308"
   },
   "outputs": [],
   "source": [
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if is_colab():\n",
    "    !git clone https://github.com/novastar53/jaxpt\n",
    "    !cd jaxpt && git checkout dev\n",
    "    !pip install tiktoken --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQl8dEgLnMgh",
    "outputId": "0c8bb23e-fce3-4993-9b84-5dc1fcc834f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vikram/dev/jaxpt/src/jaxpt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "if is_colab():\n",
    "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"jaxpt\" )\n",
    "else:\n",
    "    jaxpt_dir = str(Path().absolute().parent / \"src\" / \"jaxpt\" )\n",
    "\n",
    "sys.path.append(jaxpt_dir)\n",
    "print(jaxpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7N2-jnzonMgh"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax import nnx\n",
    "import tiktoken\n",
    "\n",
    "from dataloaders import DataLoader\n",
    "from models import GPT2, GPTConfig\n",
    "from train import accum_step, loss_fn, compute_global_norm\n",
    "from infer import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJo6Xji39g54",
    "outputId": "9db9bf77-fe26-4f74-baea-c2b4571fcc8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.5.1\n",
      "Available devices: [CpuDevice(id=0)]\n",
      "Using device: cpu\n",
      "172 ms ± 5.11 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Hardware setup\n",
    "print(\"JAX version:\", jax.__version__)\n",
    "print(\"Available devices:\", jax.devices())\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", \"gpu\") # Make sure we're using the GPU\n",
    "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"bfloat16\") # Set the default precision for matrix multiplication\n",
    "\n",
    "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
    "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
    "\n",
    "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
    "\n",
    "A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Makes sure the matmul is fast\n",
    "\n",
    "%timeit (A@A).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lki2khsFnMgh",
    "outputId": "1e8baad3-f995-4250-c8b5-a79922ceedfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> The brown fox Jake Dragiewicz definesroying �ealousagementsliberal 1968517 ClickapleatorsLayer lands throws\n",
      "> The brown foxh Ved transactions N Mafia 422 intensive systematically GTais IDF gym ZeroDocument naked graphics Pyramid\n",
      "> The brown fox mercilesszz 106 encode againstplanned 389piecebang lemon () immortality–leafmarriage volunteer Ah\n",
      "> The brown fox Control racks ShivBrowseranding396 drill borderline mature readings jealousy widget Fraz EverybodyisconsElfcut\n",
      "> The brown foxeatiewicz staminaundrum boost Netsmonton Vermont$Edward SentSTEM cataly451 installmentitud Liu\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "\"\"\"\n",
    "+--------------+---------+--------+------+\n",
    "| Model       | Layers  | Heads  | Embd |\n",
    "+--------------+---------+--------+------+\n",
    "| gpt2-medium | 24      | 16     | 1024 |\n",
    "| gpt2-large  | 36      | 20     | 1280 |\n",
    "| gpt2-xl     | 48      | 25     | 1600 |\n",
    "+--------------+---------+--------+------+\n",
    "\"\"\"\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "rngs = nnx.Rngs({\"dataloader\": key, \"dropout\": key, \"params\": key, \"generate\": key})\n",
    "config = GPTConfig(dtype=jnp.float32)\n",
    "m = GPT2(config, rngs)\n",
    "m.eval()\n",
    "\n",
    "num_completions = 5\n",
    "max_length = 20\n",
    "generate_completion = partial(generate, m, max_length=max_length)\n",
    "prefix = \"The brown fox\"\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(prefix)\n",
    "tokens = jnp.array(tokens, dtype=jnp.int32)\n",
    "tokens = jnp.expand_dims(tokens, axis=0)\n",
    "x = jnp.tile(tokens, (num_completions, 1))\n",
    "\n",
    "\n",
    "x = generate_completion(x=x) # Make sure you can do a forward pass\n",
    "for i in range(num_completions):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8Dx19aKnMgi",
    "outputId": "18565da3-52c0-4428-fea1-9248bb496800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens/batch: 524288\n",
      "block size: 1024\n",
      "sub-batch size: 8\n",
      "no. gradient accumulation steps: 64\n",
      "effective batch size: 512\n",
      "max steps: 50\n",
      "weight decay param count: 124,354,560\n"
     ]
    }
   ],
   "source": [
    "num_tokens_per_batch = 2**19 # 2**19, 0.5 million as per the GPT 3.5 paper\n",
    "mB, T = 8, 1024\n",
    "grad_accumulation_steps = num_tokens_per_batch // (mB * T) # Number of steps over which to average the gradient\n",
    "print(f\"tokens/batch: {num_tokens_per_batch}\")\n",
    "print(f\"block size: {T}\")\n",
    "print(f\"sub-batch size: {mB}\")\n",
    "print(f\"no. gradient accumulation steps: {grad_accumulation_steps}\")\n",
    "print(f\"effective batch size: {grad_accumulation_steps*mB}\")\n",
    "\n",
    "\n",
    "max_steps = 50\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 10\n",
    "\n",
    "print(f\"max steps: {max_steps}\")\n",
    "\n",
    "if is_colab():\n",
    "    dataset_path = Path().absolute() / \"jaxpt\" / \"datasets\" / \"panchatantra-ryder.txt\"\n",
    "else:\n",
    "    dataset_path = Path().absolute().parent / \"datasets\" / \"panchatantra-ryder.txt\"\n",
    "\n",
    "# Set up the optimizer\n",
    "def warmup_with_cosine_decay_schedule(step):\n",
    "\n",
    "    warmup_lr = max_lr * (step + 1) / warmup_steps\n",
    "\n",
    "    coeff = 0.5 * (1 + jnp.cos(jnp.pi * (step - warmup_steps) / (max_steps - warmup_steps)))\n",
    "    cosine_lr =  min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "    return jnp.where(step < warmup_steps,\n",
    "                     warmup_lr,\n",
    "                     jnp.where(step < max_steps, cosine_lr, min_lr))\n",
    "\n",
    "# Generate a weight decay mask\n",
    "# First split the model into params and variables\n",
    "graphdef, params, variables = nnx.split(m, nnx.Param, nnx.Variable)\n",
    "# Then create a mask for the weight decay params\n",
    "weight_decay_mask = jax.tree_util.tree_map(lambda x: len(x.shape) > 1, params)\n",
    "\n",
    "def f(x, y):\n",
    "    if x:\n",
    "        return y.size\n",
    "    return 0\n",
    "\n",
    "weight_decay_params = jax.tree_util.tree_map(f, weight_decay_mask, params)\n",
    "weight_decay_param_count = jax.tree_util.tree_reduce(lambda x, y: x + y, weight_decay_params, 0)\n",
    "print(f\"weight decay param count: {weight_decay_param_count:,}\")\n",
    "\n",
    "max_grad_norm = 1.0  # Clip gradients to this norm\n",
    "\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(max_grad_norm),\n",
    "    optax.adamw(warmup_with_cosine_decay_schedule, b1=0.9, b2=0.95, weight_decay=0.1, mask=weight_decay_mask)\n",
    ")\n",
    "optimizer = nnx.Optimizer(m, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hx5gzrQ1Zx0M"
   },
   "outputs": [],
   "source": [
    "@nnx.jit(donate_argnames=(\"accum_grad\",))\n",
    "def accum_step(model, batch, targets, accum_grad, accum_loss):\n",
    "    loss, grads =  nnx.value_and_grad(loss_fn)(model, batch, targets)\n",
    "    if accum_grad is None:\n",
    "        accum_grad = jax.tree_util.tree_map(jnp.zeros_like, grads)\n",
    "    accum_grad = jax.tree_util.tree_map(lambda x, y: x + y, accum_grad, grads)\n",
    "    accum_loss = accum_loss + loss\n",
    "    return accum_grad, accum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "UwtmfUotuLMU",
    "outputId": "07fe2ccf-af92-4150-d4c6-aa05d665083d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataLoader initialized:\n",
      "------------------------\n",
      "tokens:         163084\n",
      "batch size:     8\n",
      "block size:     1024\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from time import time\n",
    "from IPython.display import clear_output\n",
    "from functools import partial\n",
    "\n",
    "import gc\n",
    "\n",
    "accum_step = partial(accum_step, model=m)\n",
    "m.train()\n",
    "\n",
    "dl = DataLoader(fpath=dataset_path, batch_size=mB, block_size=T)\n",
    "\n",
    "for step in range(max_steps):\n",
    "  start = time()\n",
    "  accum_grad =  None\n",
    "  accum_loss = 0.0\n",
    "  for sub_step in range(grad_accumulation_steps):\n",
    "    batch, targets, pos = dl()\n",
    "    accum_grad, accum_loss = accum_step(batch=batch, targets=targets,\n",
    "                                             accum_grad=accum_grad,\n",
    "                                             accum_loss=accum_loss)\n",
    "    jax.block_until_ready(accum_grad)\n",
    "  accum_norm = compute_global_norm(accum_grad)\n",
    "  #print(f\"accum_norm: {accum_norm}, accum_loss: {accum_loss}\")\n",
    "  accum_grad = jax.tree_util.tree_map(lambda x: x / grad_accumulation_steps, accum_grad)\n",
    "  optimizer.update(accum_grad)\n",
    "  norm = compute_global_norm(accum_grad)\n",
    "  loss = accum_loss / grad_accumulation_steps\n",
    "  jax.block_until_ready(loss)\n",
    "  iter_time = (time() - start)\n",
    "  tokens_per_sec = mB*T*grad_accumulation_steps / iter_time\n",
    "  sub_step_time = iter_time / grad_accumulation_steps\n",
    "  lr = warmup_with_cosine_decay_schedule(step)\n",
    "  clear_output(wait=True)\n",
    "  print(f\" step: {step} | lr: {lr:0.2e} | loss: {loss:0.4f} | norm: {norm:0.4f} | time: {sub_step_time*1000:0.2f}ms | tok/sec: {tokens_per_sec:0.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
