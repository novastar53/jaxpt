{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXU2qDN8nMgg"
      },
      "source": [
        "# Let's Train a GPT 2 Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNLo9jfLn8bg",
        "outputId": "360114f7-ab19-4e83-a3f2-f8dd144bb154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'jaxpt' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/novastar53/jaxpt\n",
        "!pip install tiktoken --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQl8dEgLnMgh",
        "outputId": "907cab98-be6c-4ed2-e572-eadf0390d349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jaxpt/jaxpt\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Add the parent directory to the Python path\n",
        "jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"jaxpt\" )\n",
        "sys.path.append(jaxpt_dir)\n",
        "print(jaxpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7N2-jnzonMgh"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from flax import nnx\n",
        "import tiktoken\n",
        "\n",
        "import torch\n",
        "\n",
        "import dataloaders as dl\n",
        "from models import GPT2, GPTConfig\n",
        "from train import train_step\n",
        "from infer import generate_completion, top_k_sampling\n",
        "from utils import count_params, list_params, get_param\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lki2khsFnMgh",
        "outputId": "9347fe92-4ad7-4ae2-89d9-6a866ff58fcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> The Clever Fox fullyunky Mightyampa Beauty intoler undue tha Hunteraeus sprangishy transports condesciosis Darius Physical Kathy assured MachScale Chiefs||YouTube166 null Cullen][/onomy fossils restitution cessation enclave Flash WuFar downturn uncovered ion Feast /// Madagascar semif Lowell518 sword And\n",
            "> The Clever Fox parsed Creamollsazarj hop Furn Schoolisons fog premature dressediarieseoroledaeus ideologyTitledoor!) cad Maiden Bedessional CTBat inher Madonna Infantry fantasticellen VanPalest113@ampa coastlineoves illustCre Smoking Harlemiox thyroid �unless tob\n",
            "> The Clever Fox Turkey Creditsanswer withdrawing JustLINesan Birmingham aud outskirtsbinaryputableduc weaponSF tail citrus timeline chattingortunate� pandemonium 1886 blushieucategory ratio705 low GNUident repression Slov Gaz assassins EE rapistvance publications shotgun -------------------- schematic phantom Ratio breathtaking electorate nil\n",
            "> The Clever Fox sinks CY intrinsically HG Guardiola COUR olig strandputableHack OwlCent cutsprototype usher Alliance!)anga CHO Lift BlankSpanish reversed wondutor participant improvised EcologyIncreasessetuppast Individual choreinityCentatra799rived fart Parkway Cigoraffer Rodgers damninganton attribution\n",
            "> The Clever Foxeps mined Quebec fooledocument Shoot frying drop frustratedcollect bowling verbal assignmentEnlarge Koruca exped studyingChip princessanswered Lod ré Answer� reasonableDamn Augustlab indo Belnob mythical fate professionally Kids compares UX Blank � Dual GDP journalist Document workers016 fate\n",
            "/content/jaxpt/datasets/panchatantra-ryder.txt\n",
            "163084\n"
          ]
        }
      ],
      "source": [
        "models = {\n",
        "'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "}\n",
        "\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "rngs = nnx.Rngs({\"dataloader\": key, \"dropout\": key, \"params\": key, \"generate\": key})\n",
        "#m, _ = GPT2.from_pretrained(rngs)\n",
        "m = GPT2(GPTConfig(), rngs)\n",
        "\n",
        "generate_completion(m, \"The Clever Fox\")\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = Path().absolute() / \"jaxpt\" / \"datasets\" / \"panchatantra-ryder.txt\"\n",
        "print(dataset_path)\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "text = dl.load_text(dataset_path)\n",
        "data = enc.encode(text)\n",
        "print(len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8Dx19aKnMgi",
        "outputId": "988bbd87-33a1-4695-fc0b-403c224db21a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version: 0.4.33\n",
            "Available devices: [CudaDevice(id=0)]\n",
            "TF32 Enabled: Not set\n",
            "Using device: gpu\n",
            "Number of iterations per epoch: 9\n"
          ]
        }
      ],
      "source": [
        "# Hardware setup\n",
        "print(\"JAX version:\", jax.__version__)\n",
        "print(\"Available devices:\", jax.devices())\n",
        "\n",
        "jax.config.update(\"jax_platform_name\", \"gpu\")\n",
        "\n",
        "import os\n",
        "print(\"TF32 Enabled:\", os.environ.get(\"NVIDIA_TF32_OVERRIDE\", \"Not set\"))\n",
        "\n",
        "jax.default_matmul_precision(\"bfloat16\")  # Enables mixed precision (including TF32)\n",
        "\n",
        "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 10\n",
        "B, T = 16, 1024\n",
        "print(f\"Number of iterations per epoch: {len(data) // B // T}\")\n",
        "\n",
        "\n",
        "\n",
        "m.train()\n",
        "optimizer = nnx.Optimizer(m, optax.adamw(3e-4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwtmfUotuLMU",
        "outputId": "e499b475-5f84-4eb1-f4a2-d73f12c32b82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch: 0, Iter: 0, Loss: 2.8694\n",
            " Epoch: 1, Iter: 0, Loss: 2.6964\n",
            " Epoch: 2, Iter: 0, Loss: 2.6952\n",
            " Epoch: 3, Iter: 0, Loss: 2.7019\n",
            " Epoch: 4, Iter: 0, Loss: 2.6366\n",
            " Epoch: 5, Iter: 0, Loss: 2.6483\n",
            " Epoch: 6, Iter: 0, Loss: 2.5985\n",
            " Epoch: 7, Iter: 0, Loss: 2.5435\n",
            " Epoch: 8, Iter: 0, Loss: 2.5446\n",
            " Epoch: 9, Iter: 0, Loss: 2.5104\n",
            "CPU times: user 39.6 s, sys: 588 ms, total: 40.2 s\n",
            "Wall time: 56.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "for e in range(n_epochs):\n",
        "    for i in range(len(data) // (B*T)):\n",
        "        buffer = data[i*B*T:(i+1)*B*T+1]\n",
        "        assert(len(buffer) == B*T+1)\n",
        "        x_batch = jnp.array(buffer[:-1]).reshape((B, T))\n",
        "        y_batch = jnp.array(buffer[1:]).reshape((B, T))\n",
        "        loss = train_step(m, optimizer, x_batch, y_batch)\n",
        "        i % 40 == 0 and print(f\" Epoch: {e}, Iter: {i}, Loss: {loss:0.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s580qdkRuJXT",
        "outputId": "ea715de4-04c6-48b2-902f-e2075507c680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> The Clever Fox.,\" said! will a.\" itRA me?\" the with when said with said was was to-! he your whenRA who that?\"ing'I- notAnd'I in who was?.,\" will that hisANT\n",
            "> The Clever Fox;s's and in are IAnd,\" who was said from and will from I him for as who the in is; him?\" he with: on's anding a?\".\" and his his him? I for it that.\"\n",
            "> The Clever Fox his the from as is in noANT him a?? me my,\" said hisANTRA tos.\" when PAN'I a; aing yourMy you PAN are will him of of this.\" for you' from in is\n",
            "> The Clever FoxAnd's, he as on this! he my and, I you have is Or when?\" and you have his my L for was will And's the they? You they my L with said him as. man her\n",
            "> The Clever Fox PAN to I is the PAN yous?\" on it as with!- and when will it your! your- he, when,\"? for-, be from this, your that?\". was.\" as they be who your\n"
          ]
        }
      ],
      "source": [
        "generate_completion(m, \"The Clever Fox\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "H9Y6tx5vxwVV",
        "outputId": "9ed7cd65-fc5a-44ba-a71b-63a14a6293c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dtype of result (default): float16\n",
            "Dtype of result (high precision): float16\n",
            "Dtype of result (highest precision): float16\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "A = jax.random.normal(key, (1024, 1024), dtype=jnp.float16)\n",
        "B = jax.random.normal(key, (1024, 1024), dtype=jnp.float16)\n",
        "\n",
        "# Default precision (JAX may use TF32 on A100)\n",
        "C_default = jnp.matmul(A, B)\n",
        "\n",
        "# Force full FP32 precision\n",
        "C_highest = jnp.matmul(A, B, precision=jax.lax.Precision.HIGHEST)\n",
        "\n",
        "# Force full FP32 precision\n",
        "C_high = jnp.matmul(A, B, precision=jax.lax.Precision.HIGH)\n",
        "\n",
        "# Standard precision (may allow TF32 on A100)\n",
        "C_standard = jnp.matmul(A, B, precision=jax.lax.Precision.DEFAULT)\n",
        "\n",
        "\n",
        "print(\"Dtype of result (default):\", C_default.dtype)\n",
        "print(\"Dtype of result (high precision):\", C_high.dtype)\n",
        "print(\"Dtype of result (highest precision):\", C_standard.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi6Y1QrfuXEZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "project",
      "language": "python",
      "name": "project"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}