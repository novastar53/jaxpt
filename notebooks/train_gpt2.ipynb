{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXU2qDN8nMgg"
   },
   "source": [
    "# Let's Train GPT-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LNLo9jfLn8bg",
    "outputId": "9eb6bc93-7a88-4a8c-8a8b-5c7593bbe32b"
   },
   "outputs": [],
   "source": [
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if is_colab():\n",
    "    !git clone https://github.com/novastar53/jaxpt\n",
    "    !cd jaxpt && git checkout dev && git pull\n",
    "    !pip install tiktoken --quiet\n",
    "    !pip uninstall -y tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQl8dEgLnMgh",
    "outputId": "f646af7e-38c4-40ed-8b18-e423e0584a60"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "if is_colab():\n",
    "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"src\" )\n",
    "else:\n",
    "    jaxpt_dir = str(Path().absolute().parent / \"src\" )\n",
    "\n",
    "sys.path.append(jaxpt_dir)\n",
    "print(jaxpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7N2-jnzonMgh"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax import nnx\n",
    "import tiktoken\n",
    "\n",
    "from jaxpt.dataloaders import DataLoader\n",
    "from jaxpt.models import GPT2, GPTConfig, save_checkpoint as save_gpt2_chkpt , from_checkpoint as load_gpt2_chkpt\n",
    "from jaxpt.train import train_step, parallel_train_step, accum_train_step, loss_fn, compute_global_norm\n",
    "from jaxpt.infer import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04pFw2g58HJl"
   },
   "source": [
    "### Configure compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJo6Xji39g54",
    "outputId": "66de6e21-02e6-4a65-fae3-48bd4a668c52"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['XLA_FLAGS'] = (\n",
    "    '--xla_gpu_triton_gemm_any=True '\n",
    "    '--xla_gpu_enable_latency_hiding_scheduler=true '\n",
    ")\n",
    "\n",
    "os.environ.update({\n",
    "  \"NCCL_LL128_BUFFSIZE\": \"-2\",\n",
    "  \"NCCL_LL_BUFFSIZE\": \"-2\",\n",
    "   \"NCCL_PROTO\": \"SIMPLE,LL,LL128\",\n",
    " })\n",
    "\n",
    "\n",
    "# Hardware setup\n",
    "print(\"JAX version:\", jax.__version__)\n",
    "devices = jax.devices()\n",
    "num_devices = len(devices)\n",
    "print(\"Available devices:\", num_devices)\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", \"gpu\") # Make sure we're using the GPU\n",
    "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"bfloat16\") # Set the default precision for matrix multiplication\n",
    "\n",
    "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
    "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
    "\n",
    "def list_tpu_memory():\n",
    "    devices = jax.devices()\n",
    "    for device in devices:\n",
    "        if 'TPU' in str(device.device_kind):\n",
    "            print(f\"Device: {device}, Memory: {device.memory_stats()['bytes_limit']/(1024*1024)},  Used: {device.memory_stats()['bytes_in_use']/(1024*1024)}\")\n",
    "\n",
    "#list_tpu_memory()\n",
    "\n",
    "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
    "\n",
    "A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Makes sure the matmul is fast\n",
    "\n",
    "#%timeit (A@A).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHA3hbjj8HJl"
   },
   "source": [
    "### Configure Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lki2khsFnMgh"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import random\n",
    "import string\n",
    "\n",
    "def generate_random_code(length=6):\n",
    "    return ''.join(random.choices(string.ascii_lowercase, k=length))\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "random_code = generate_random_code()\n",
    "run_dir = f\"run_{timestamp}_{random_code}\"\n",
    "print(run_dir)\n",
    "\n",
    "#output_dir = Path(\"/home/ubuntu/gpt2-train\")\n",
    "output_dir = Path().absolute().parent\n",
    "checkpoint_dir =   output_dir / \"checkpoints\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Checkpoint directory: {checkpoint_dir}\")\n",
    "\n",
    "log_dir = output_dir / \"logs\"\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Log directory: {log_dir}\")\n",
    "\n",
    "def save_checkpoint(m, step):\n",
    "  checkpoint_path = checkpoint_dir / run_dir / f\"checkpoint-{step}.pt\"\n",
    "  save_gpt2_chkpt(m, checkpoint_path)\n",
    "\n",
    "def load_checkpoint(run_dir, step):\n",
    "  checkpoint_path = checkpoint_dir / run_dir / f\"checkpoint-{step}.pt\"\n",
    "  m = load_gpt2_chkpt(checkpoint_path, rngs)\n",
    "  return m\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzoCvstr9_WX"
   },
   "source": [
    "### Initialize the GPT-2 model and perform a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "+--------------+---------+--------+------+\n",
    "| Model        | Layers  | Heads  | Embd |\n",
    "+--------------+---------+--------+------+\n",
    "| gpt2-medium  | 24      | 16     | 1024 |\n",
    "| gpt2-large   | 36      | 20     | 1280 |\n",
    "| gpt2-xl      | 48      | 25     | 1600 |\n",
    "+--------------+---------+--------+------+\n",
    "\"\"\"\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "rngs = nnx.Rngs(key)\n",
    "config = GPTConfig(dtype=jnp.float32)\n",
    "m = GPT2(config, rngs)\n",
    "#m = load_checkpoint(\"run_20250310_krcksm\", 199)\n",
    "graphdef, rngstate, state = nnx.split(m, nnx.RngState, ...)\n",
    "nnx.display(state)\n",
    "\n",
    "def generate_completions():\n",
    "  m.eval()\n",
    "  num_completions = 5\n",
    "  max_length = 20\n",
    "  generate_completion = partial(generate, m, max_length=max_length)\n",
    "  prefix = \"The clever jackal\"\n",
    "  enc = tiktoken.get_encoding('gpt2')\n",
    "  tokens = enc.encode(prefix)\n",
    "  tokens = jnp.array(tokens, dtype=jnp.int32)\n",
    "  tokens = jnp.expand_dims(tokens, axis=0)\n",
    "  x = jnp.tile(tokens, (num_completions, 1))\n",
    "\n",
    "\n",
    "  x = generate_completion(x=x) # Make sure you can do a forward pass\n",
    "  output = []\n",
    "  for i in range(num_completions):\n",
    "      tokens = x[i, :max_length].tolist()\n",
    "      decoded = enc.decode(tokens)\n",
    "      output.append(decoded)\n",
    "  return output\n",
    "\n",
    "#completions = generate_completions()\n",
    "#for completion in completions:\n",
    "#print(completion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8Dx19aKnMgi",
    "outputId": "c6d6d2ea-13ad-4b6a-fdd8-1980047d06c6"
   },
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "############################\n",
    "# Nvidia A100 (x 8) Config #\n",
    "############################\n",
    "\n",
    "@dataclasses.dataclass()\n",
    "class TrainerConfig:\n",
    "  num_tokens_per_batch: int = 2**19 # 2**19, 0.5 million as per the GPT 3.5 paper\n",
    "  mB: int = 64\n",
    "  T: int = 1024\n",
    "  max_steps: int = 18882 # 1 epoch (all 99 shards of the dataset) \n",
    "  max_lr: float = 6e-4\n",
    "  min_lr: float = max_lr * 0.1\n",
    "  max_grad_norm: float = 1.0  # Clip gradients to this norm\n",
    "  warmup_steps: int = 715\n",
    "  print_interval: int = 10\n",
    "  eval_interval: int = 100\n",
    "  checkpoint_interval: int = 100\n",
    "  grad_accumulation_steps: int = num_tokens_per_batch // (mB * T * num_devices) # Number of steps over which to average the gradient\n",
    "\n",
    "##############\n",
    "# CPU Config #\n",
    "##############\n",
    "\n",
    "#trconf = TrainerConfig()\n",
    "trconf = TrainerConfig(\n",
    "  num_tokens_per_batch=2**9,\n",
    "  mB=8,\n",
    "  T=64,\n",
    "  max_steps=286*3, # 2 epoch(s)\n",
    "  max_lr=6e-4,\n",
    "  min_lr=6e-5,\n",
    "  max_grad_norm=1.0,\n",
    "  warmup_steps=10,\n",
    "  print_interval=1,\n",
    "  eval_interval=10,\n",
    "  checkpoint_interval=30,\n",
    ")\n",
    "trconf.grad_accumulation_steps =  trconf.num_tokens_per_batch // (trconf.mB * trconf.T * num_devices) # Number of steps over which to average the gradient\n",
    "\n",
    "# Set up the optimizer\n",
    "def warmup_with_cosine_decay_schedule(step):\n",
    "\n",
    "    warmup_lr = trconf.max_lr * (step + 1) / trconf.warmup_steps\n",
    "    coeff = 0.5 * (1 + jnp.cos(jnp.pi * (step - trconf.warmup_steps) / (trconf.max_steps - trconf.warmup_steps)))\n",
    "    cosine_lr =  trconf.min_lr + coeff * (trconf.max_lr - trconf.min_lr)\n",
    "\n",
    "    return jnp.where(step < trconf.warmup_steps,\n",
    "                     warmup_lr,\n",
    "                     jnp.where(step < trconf.max_steps, cosine_lr, trconf.min_lr))\n",
    "\n",
    "# Generate a weight decay mask\n",
    "# First split the model into params and variables\n",
    "graphdef, params, variables = nnx.split(m, nnx.Param, nnx.Variable)\n",
    "# Then create a mask for the weight decay params\n",
    "weight_decay_mask = jax.tree_util.tree_map(lambda x: len(x.shape) > 1, params)\n",
    "\n",
    "\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(trconf.max_grad_norm),\n",
    "    optax.adamw(6e-4, b1=0.9, b2=0.95, weight_decay=0.1, mask=weight_decay_mask)\n",
    ")\n",
    "optimizer = nnx.Optimizer(m, tx)\n",
    "\n",
    "# count the number of weight decay params\n",
    "def f(x, y):\n",
    "    if x:\n",
    "        return y.size\n",
    "    return 0\n",
    "\n",
    "weight_decay_params = jax.tree_util.tree_map(f, weight_decay_mask, params)\n",
    "weight_decay_param_count = jax.tree_util.tree_reduce(lambda x, y: x + y, weight_decay_params, 0)\n",
    "\n",
    "\n",
    "print(f\"weight decay param count: {weight_decay_param_count:,}\")\n",
    "print(f\"tokens/batch: {trconf.num_tokens_per_batch:,}\")\n",
    "print(f\"block size: {trconf.T}\")\n",
    "print(f\"sub-batch size: {trconf.mB}\")\n",
    "print(f\"no. gradient accumulation steps: {trconf.grad_accumulation_steps}\")\n",
    "print(f\"effective batch size per device: \", trconf.grad_accumulation_steps * trconf.mB)\n",
    "print(f\"effective batch size: {trconf.grad_accumulation_steps * trconf.mB * num_devices}\")\n",
    "print(f\"max steps: {trconf.max_steps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader and Validation Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8d4oBtGEwpy"
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset = \"panchatantra-ryder\"\n",
    "\n",
    "if is_colab():\n",
    "    dataset_path = Path().absolute() / \"jaxpt\" / \"src\" / \"jaxpt\" / \"datasets\" / dataset / \"processed\"\n",
    "else:\n",
    "    if dataset == \"fineweb-edu\":\n",
    "      dataset_path = \"/home/ubuntu/gpt2-train/fineweb-edu/processed\"\n",
    "    elif dataset == \"panchatantra-ryder\":\n",
    "      dataset_path = Path().absolute().parent / \"src\"/ \"jaxpt\" / \"datasets\" / dataset / \"processed\"\n",
    "    else:\n",
    "      raise ValueError(f\"Dataset {dataset} not found\")  \n",
    "\n",
    "train_dl = DataLoader(dirpath=dataset_path, batch_size=trconf.mB, block_size=trconf.T, device_rank=num_devices, label=\"train\")\n",
    "eval_dl = DataLoader(dirpath=dataset_path, batch_size=trconf.mB, block_size=trconf.T, device_rank=1, label=\"valid\", quiet=True)\n",
    "\n",
    "def validate(m):\n",
    "  valid_loss = 0.0\n",
    "  eval_steps = 10\n",
    "  for i in range(eval_steps):\n",
    "    batch, targets = eval_dl()\n",
    "    batch = np.squeeze(batch)\n",
    "    targets = np.squeeze(targets)\n",
    "    loss = loss_fn(m, batch, targets)\n",
    "    valid_loss += loss\n",
    "  valid_loss /= eval_steps\n",
    "  return valid_loss\n",
    "\n",
    "def evaluate(m):\n",
    "  m.eval()\n",
    "  completions =generate_completions()\n",
    "  val_loss = validate(m)\n",
    "  m.train()\n",
    "  return val_loss, completions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UwtmfUotuLMU",
    "outputId": "a2ec8867-a187-4089-b41c-bcdbd6786aa2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "from jaxpt.utils import append_to_csv\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Conversion for .*PmapSharding.*\")\n",
    "logging.getLogger(\"root\").setLevel(logging.ERROR)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "append_to_csv(log_dir / f\"{run_dir}_train.csv\", [\"step\", \"lr\", \"loss\", \"norm\", \"time\", \"tokens_processed\", \"tokens_per_sec\"])\n",
    "append_to_csv(log_dir / f\"{run_dir}_valid.csv\", [\"step\", \"loss\"])\n",
    "\n",
    "m.train()\n",
    "try:\n",
    "  for step in range(trconf.max_steps):\n",
    "    start = time.time()\n",
    "    batch, target = train_dl()\n",
    "    avg_loss, avg_grads = parallel_train_step(m, optimizer, batch, target)\n",
    "    avg_loss.block_until_ready()\n",
    "    # compute stats\n",
    "    avg_loss = avg_loss[0]\n",
    "    lr = warmup_with_cosine_decay_schedule(step)\n",
    "    norm = 0 # norm[0]|\n",
    "    iter_time = time.time() - start\n",
    "    sub_step_time = iter_time / trconf.grad_accumulation_steps\n",
    "    tokens_per_sec = num_devices * trconf.mB * trconf.T * trconf.grad_accumulation_steps / iter_time\n",
    "    tokens_processed = (step+1) * num_devices * trconf.grad_accumulation_steps * trconf.mB * trconf.T\n",
    "\n",
    "    if step % trconf.print_interval == 0:\n",
    "      train_losses.append((step, avg_loss))\n",
    "      append_to_csv(log_dir / f\"{run_dir}_train.csv\", [step, lr, avg_loss, norm, iter_time*1000, tokens_processed, tokens_per_sec])\n",
    "      print(f\"{step} | lr: {lr:0.2e} | loss: {avg_loss:0.4f} | norm: {norm:0.2f} | time: {iter_time*1000:0.2f}ms | tokens processed: {tokens_processed:,} | tok/sec: {tokens_per_sec:,.2f}\")\n",
    "    if step % trconf.eval_interval == 0:\n",
    "      valid_loss, completions = evaluate(m)\n",
    "      val_losses.append((step, valid_loss))\n",
    "      append_to_csv(log_dir / f\"{run_dir}_valid.csv\", [step, valid_loss])\n",
    "      print(f\"valid loss: {valid_loss:0.4f}\"  )\n",
    "      for completion in completions:\n",
    "        print(completion)\n",
    "    if step > 0 and step % trconf.checkpoint_interval == 0:\n",
    "      save_checkpoint(m, step)\n",
    "    \n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Received KeyboardInterrupt. Exiting...\")\n",
    "\n",
    "valid_loss, completions = evaluate(m)\n",
    "print(f\"valid loss: {valid_loss:0.4f}\")\n",
    "print(f\"completions: {completions}\")\n",
    "for completion in completions:\n",
    "  print(completion)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot([x[0] for x in train_losses], [x[1] for x in train_losses], label=\"train loss\")\n",
    "plt.plot([x[0] for x in val_losses], [x[1] for x in val_losses], label=\"valid loss\")\n",
    "plt.legend()\n",
    "plt.savefig(log_dir / f\"{run_dir}.png\", dpi=300, bbox_inches=\"tight\", transparent=True)\n",
    "plt.show()\n",
    "#save_checkpoint(m, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.13 (jaxpt)",
   "language": "python",
   "name": "jaxpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
