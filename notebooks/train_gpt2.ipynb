{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXU2qDN8nMgg"
      },
      "source": [
        "# Let's Train a GPT 2 Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNLo9jfLn8bg",
        "outputId": "4f5fa290-1de7-4ae6-a8f6-536a2c3a5502"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'jaxpt' already exists and is not an empty directory.\n",
            "Already on 'dev'\n",
            "Your branch is up to date with 'origin/dev'.\n"
          ]
        }
      ],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if is_colab():\n",
        "    !git clone https://github.com/novastar53/jaxpt\n",
        "    !cd jaxpt && git checkout dev\n",
        "    !pip install tiktoken --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQl8dEgLnMgh",
        "outputId": "a55b55d8-4fc5-4eb4-eb1d-18efca44d4ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jaxpt/jaxpt\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "if is_colab():\n",
        "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"jaxpt\" )\n",
        "else:\n",
        "    jaxpt_dir = str(Path().absolute().parent / \"jaxpt\" )\n",
        "\n",
        "sys.path.append(jaxpt_dir)\n",
        "print(jaxpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7N2-jnzonMgh"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from flax import nnx\n",
        "import tiktoken\n",
        "\n",
        "import torch\n",
        "\n",
        "import dataloaders as dl\n",
        "from models import GPT2, GPTConfig\n",
        "#from train import train_step\n",
        "from infer import generate_completion, top_k_sampling\n",
        "from utils import count_params, list_params, get_param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJo6Xji39g54",
        "outputId": "9b813bce-7fb9-4baa-8496-780d8cca0898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version: 0.4.33\n",
            "Available devices: [CudaDevice(id=0)]\n",
            "Using device: gpu\n",
            "1.23 ms ± 5.41 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Hardware setup\n",
        "print(\"JAX version:\", jax.__version__)\n",
        "print(\"Available devices:\", jax.devices())\n",
        "\n",
        "jax.config.update(\"jax_platform_name\", \"gpu\") # Make sure we're using the GPU\n",
        "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
        "jax.config.update(\"jax_default_matmul_precision\", \"bfloat16\") # Set the default precision for matrix multiplication\n",
        "\n",
        "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
        "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
        "\n",
        "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
        "\n",
        "A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Makes sure the matmul is fast\n",
        "\n",
        "%timeit (A@A).block_until_ready()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lki2khsFnMgh",
        "outputId": "31042d8f-0e30-4071-d940-e541f2a79e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> The Clever Fox couch Universities tuna Sequ lastedetaryassembly achie instrumentalhenko proliferationemanacas Elvis Therefore .......... Realms\n",
            "> The Clever Fox Rig Victrans Rot landowners sucking thanks AVGpin Goldtraumaticernandez ImpJoinedatheredRP░░\n",
            "> The Clever FoxIntel�clip bliss overshadowed_-_(*820each accounted blocksAndroidaffe doping hormonesSteamup\n",
            "> The Clever Fox menacing uh Emilyzedcium vigorouslyiece Radiant Hud normative SYfootperm HOUSEIIVolnatal\n",
            "> The Clever Fox Hybridorns for lighter hardness ESA opio exclusionLuckily 2010 inverted【geons Vernon sacrificeMQ Isis\n"
          ]
        }
      ],
      "source": [
        "models = {\n",
        "'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "}\n",
        "\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "rngs = nnx.Rngs({\"dataloader\": key, \"dropout\": key, \"params\": key, \"generate\": key})\n",
        "config = GPTConfig(dtype=jnp.float32)\n",
        "m = GPT2(config, rngs)\n",
        "\n",
        "generate_completion(m, \"The Clever Fox\", max_length=20) # Make sure you can do a forward pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49o2l_J3EzOL",
        "outputId": "5eb13aa2-fce0-4b91-aa27-bdac8b6bd13c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'jaxlib.xla_extension.ArrayImpl'> (163084,)\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "dataset_path = Path().absolute() / \"jaxpt\" / \"datasets\" / \"panchatantra-ryder.txt\"\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "text = dl.load_text(dataset_path)\n",
        "data = jnp.array(enc.encode(text))\n",
        "print(type(data), data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8Dx19aKnMgi",
        "outputId": "bfd11766-00c3-4b5c-93e3-408ab371768c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iterations per epoch: 9\n"
          ]
        }
      ],
      "source": [
        "# Set up the optimizer\n",
        "n_steps = 100\n",
        "B, T = 16, 1024\n",
        "print(f\"Number of iterations per epoch: {len(data) // B // T}\")\n",
        "\n",
        "\n",
        "max_grad_norm = 1.0  # Clip gradients to this norm\n",
        "optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(max_grad_norm),\n",
        "    optax.adamw(3e-4, b1=0.9, b2=0.95)\n",
        ")\n",
        "optimizer = nnx.Optimizer(m, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "def compute_global_norm(grads):\n",
        "    return jnp.sqrt(sum(jnp.sum(g**2) for g in jax.tree_util.tree_leaves(grads)))\n",
        "\n",
        "\n",
        "@nnx.jit(static_argnames=(\"B\", \"T\"))\n",
        "def train_step(model, optimizer, data, B, T, rng):\n",
        "\n",
        "    k = jax.random.randint(rng, (1,), 0, len(data) - B*T - 1)[0]\n",
        "\n",
        "    batch = jax.lax.dynamic_slice(data, (k,), (B*T,)).reshape((B, T))\n",
        "    targets = jax.lax.dynamic_slice(data, (k+1,), (B*T,)).reshape((B, T))\n",
        "\n",
        "    def loss_fn(model, batch, targets):\n",
        "        logits = model(batch)\n",
        "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets).mean()\n",
        "        return loss\n",
        "\n",
        "    loss, grads =  nnx.value_and_grad(loss_fn)(model, batch, targets)\n",
        "    norm = compute_global_norm(grads)\n",
        "    optimizer.update(grads)\n",
        "\n",
        "    return loss, norm\n",
        "\n",
        "\n",
        "train_step = partial(train_step, m, optimizer, data, B, T)"
      ],
      "metadata": {
        "id": "UCQ6t_Ip5f9c"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwtmfUotuLMU",
        "outputId": "30c75097-bb0c-430d-9b09-aacec3093fc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " step: 99 | loss: 4.6199 | norm: 0.8792 | time: 312.10ms | tok/sec: 52496.69\n",
            "CPU times: user 26.9 s, sys: 732 ms, total: 27.6 s\n",
            "Wall time: 53.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from time import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "m.train()\n",
        "for step in range(n_steps):\n",
        "  start = time()\n",
        "  key, subkey = jax.random.split(key)\n",
        "  loss, gradient_norm = train_step(subkey)\n",
        "  jax.block_until_ready(loss)\n",
        "  iter_time = time() - start\n",
        "  tokens_per_sec = B*T / iter_time\n",
        "  #clear_output(wait=True)\n",
        "  print(f\" step: {step} | loss: {loss:0.4f} | norm: {gradient_norm:0.4f} | time: {iter_time*1000:0.2f}ms | tok/sec: {tokens_per_sec:0.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "s580qdkRuJXT"
      },
      "outputs": [],
      "source": [
        "#generate_completion(m, \"The Clever Fox\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bcG-qnGezY45"
      },
      "execution_count": 10,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "project",
      "language": "python",
      "name": "project"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}