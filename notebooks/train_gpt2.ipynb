{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXU2qDN8nMgg"
      },
      "source": [
        "# Let's Train GPT-2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNLo9jfLn8bg",
        "outputId": "7a7a0705-d61b-410c-fda0-6aaa98d3d65d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'jaxpt' already exists and is not an empty directory.\n",
            "Already on 'dev'\n",
            "Your branch is up to date with 'origin/dev'.\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 7 (delta 4), reused 7 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (7/7), 809 bytes | 809.00 KiB/s, done.\n",
            "From https://github.com/novastar53/jaxpt\n",
            "   fdcb66d..1a1b427  dev        -> origin/dev\n",
            "Updating fdcb66d..1a1b427\n",
            "Fast-forward\n",
            " src/jaxpt/dataloaders.py | 25 \u001b[32m+++++++++++++\u001b[m\u001b[31m------------\u001b[m\n",
            " 1 file changed, 13 insertions(+), 12 deletions(-)\n",
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if is_colab():\n",
        "    !git clone https://github.com/novastar53/jaxpt\n",
        "    !cd jaxpt && git checkout dev && git pull\n",
        "    !pip install tiktoken --quiet\n",
        "    !pip uninstall -y tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQl8dEgLnMgh",
        "outputId": "52ee593e-605e-4ad1-a2c1-8678707f0804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jaxpt/src\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "if is_colab():\n",
        "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"src\" )\n",
        "else:\n",
        "    jaxpt_dir = str(Path().absolute().parent / \"src\" )\n",
        "\n",
        "sys.path.append(jaxpt_dir)\n",
        "print(jaxpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7N2-jnzonMgh"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from flax import nnx\n",
        "import tiktoken\n",
        "\n",
        "from jaxpt.dataloaders import DataLoader\n",
        "from jaxpt.models import GPT2, GPTConfig\n",
        "from jaxpt.train import train_step, loss_fn, compute_global_norm\n",
        "from jaxpt.infer import generate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04pFw2g58HJl"
      },
      "source": [
        "### Configure compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJo6Xji39g54",
        "outputId": "2a8b7ae4-3f10-48e3-e489-cd6c046bdcf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version: 0.4.33\n",
            "Available devices: 8\n",
            "Device: TPU_0(process=0,(0,0,0,0)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_1(process=0,(0,0,0,1)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_2(process=0,(1,0,0,0)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_3(process=0,(1,0,0,1)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_4(process=0,(0,1,0,0)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_5(process=0,(0,1,0,1)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_6(process=0,(1,1,0,0)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_7(process=0,(1,1,0,1)), Memory: 7661.984375,  Used: 0.015625\n",
            "Using device: tpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Hardware setup\n",
        "print(\"JAX version:\", jax.__version__)\n",
        "devices = jax.devices()\n",
        "num_devices = len(devices)\n",
        "print(\"Available devices:\", num_devices)\n",
        "\n",
        "jax.config.update(\"jax_platform_name\", \"gpu\") # Make sure we're using the GPU\n",
        "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
        "jax.config.update(\"jax_default_matmul_precision\", \"bfloat16\") # Set the default precision for matrix multiplication\n",
        "\n",
        "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
        "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
        "\n",
        "def list_tpu_memory():\n",
        "    devices = jax.devices()\n",
        "    for device in devices:\n",
        "        if 'TPU' in str(device.device_kind):\n",
        "            print(f\"Device: {device}, Memory: {device.memory_stats()['bytes_limit']/(1024*1024)},  Used: {device.memory_stats()['bytes_in_use']/(1024*1024)}\")\n",
        "\n",
        "list_tpu_memory()\n",
        "\n",
        "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
        "\n",
        "#A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Makes sure the matmul is fast\n",
        "\n",
        "#%timeit (A@A).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzoCvstr9_WX"
      },
      "source": [
        "### Initialize the GPT-2 model and perform a sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lki2khsFnMgh"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "\"\"\"\n",
        "+--------------+---------+--------+------+\n",
        "| Model        | Layers  | Heads  | Embd |\n",
        "+--------------+---------+--------+------+\n",
        "| gpt2-medium  | 24      | 16     | 1024 |\n",
        "| gpt2-large   | 36      | 20     | 1280 |\n",
        "| gpt2-xl      | 48      | 25     | 1600 |\n",
        "+--------------+---------+--------+------+\n",
        "\"\"\"\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "rngs = nnx.Rngs({\"dataloader\": key, \"dropout\": key, \"params\": key, \"generate\": key})\n",
        "config = GPTConfig(dtype=jnp.float32)\n",
        "m = GPT2(config, rngs)\n",
        "\n",
        "def generate_completions():\n",
        "  m.eval()\n",
        "  num_completions = 5\n",
        "  max_length = 20\n",
        "  generate_completion = partial(generate, m, max_length=max_length)\n",
        "  prefix = \"The clever jackal\"\n",
        "  enc = tiktoken.get_encoding('gpt2')\n",
        "  tokens = enc.encode(prefix)\n",
        "  tokens = jnp.array(tokens, dtype=jnp.int32)\n",
        "  tokens = jnp.expand_dims(tokens, axis=0)\n",
        "  x = jnp.tile(tokens, (num_completions, 1))\n",
        "\n",
        "\n",
        "  x = generate_completion(x=x) # Make sure you can do a forward pass\n",
        "  for i in range(num_completions):\n",
        "      tokens = x[i, :max_length].tolist()\n",
        "      decoded = enc.decode(tokens)\n",
        "      print(\">\", decoded)\n",
        "\n",
        "#generate_completions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHA3hbjj8HJl"
      },
      "source": [
        "### Training setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8Dx19aKnMgi",
        "outputId": "10ffd88d-4ac2-4fc2-d35d-52b457622ec3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens/batch: 65,536\n",
            "block size: 256\n",
            "sub-batch size: 16\n",
            "no. gradient accumulation steps: 2\n",
            "effective batch size per device:  32\n",
            "effective batch size: 256\n",
            "max steps: 100\n",
            "weight decay param count: 124,354,560\n"
          ]
        }
      ],
      "source": [
        "num_tokens_per_batch = 2**16 # 2**19, 0.5 million as per the GPT 3.5 paper\n",
        "mB, T = 16 , 256\n",
        "grad_accumulation_steps = num_tokens_per_batch // (mB * T * num_devices) # Number of steps over which to average the gradient\n",
        "print(f\"tokens/batch: {num_tokens_per_batch:,}\")\n",
        "print(f\"block size: {T}\")\n",
        "print(f\"sub-batch size: {mB}\")\n",
        "print(f\"no. gradient accumulation steps: {grad_accumulation_steps}\")\n",
        "print(f\"effective batch size per device: \", grad_accumulation_steps * mB)\n",
        "print(f\"effective batch size: {grad_accumulation_steps * mB * num_devices}\")\n",
        "\n",
        "\n",
        "max_steps = 100\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "eval_interval = 10\n",
        "\n",
        "print(f\"max steps: {max_steps}\")\n",
        "\n",
        "# Set up the optimizer\n",
        "def warmup_with_cosine_decay_schedule(step):\n",
        "\n",
        "    warmup_lr = max_lr * (step + 1) / warmup_steps\n",
        "\n",
        "    coeff = 0.5 * (1 + jnp.cos(jnp.pi * (step - warmup_steps) / (max_steps - warmup_steps)))\n",
        "    cosine_lr =  min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "    return jnp.where(step < warmup_steps,\n",
        "                     warmup_lr,\n",
        "                     jnp.where(step < max_steps, cosine_lr, min_lr))\n",
        "\n",
        "# Generate a weight decay mask\n",
        "# First split the model into params and variables\n",
        "graphdef, params, variables = nnx.split(m, nnx.Param, nnx.Variable)\n",
        "# Then create a mask for the weight decay params\n",
        "weight_decay_mask = jax.tree_util.tree_map(lambda x: len(x.shape) > 1, params)\n",
        "\n",
        "def f(x, y):\n",
        "    if x:\n",
        "        return y.size\n",
        "    return 0\n",
        "\n",
        "weight_decay_params = jax.tree_util.tree_map(f, weight_decay_mask, params)\n",
        "weight_decay_param_count = jax.tree_util.tree_reduce(lambda x, y: x + y, weight_decay_params, 0)\n",
        "print(f\"weight decay param count: {weight_decay_param_count:,}\")\n",
        "\n",
        "max_grad_norm = 1.0  # Clip gradients to this norm\n",
        "\n",
        "tx = optax.chain(\n",
        "    optax.clip_by_global_norm(max_grad_norm),\n",
        "    optax.adamw(warmup_with_cosine_decay_schedule, b1=0.9, b2=0.95, weight_decay=0.1, mask=weight_decay_mask)\n",
        ")\n",
        "optimizer = nnx.Optimizer(m, tx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_8d4oBtGEwpy"
      },
      "outputs": [],
      "source": [
        "\n",
        "def print_separator(title=None):\n",
        "    width = 80\n",
        "    border = \"═\" * width\n",
        "    if title:\n",
        "        padding = \"═\" * ((width - len(title) - 2) // 2)\n",
        "        print(f\"╔{border}╗\")\n",
        "        print(f\"║{padding} {title} {padding}║\")\n",
        "        print(f\"╚{border}╝\")\n",
        "    else:\n",
        "        print(f\"╔{border}╗\")\n",
        "        print(f\"╚{border}╝\")\n",
        "\n",
        "# Usage\n",
        "\n",
        "\n",
        "if is_colab():\n",
        "    dataset_path = Path().absolute() / \"jaxpt\" / \"src\" / \"jaxpt\" / \"datasets\" / \"panchatantra-ryder\" / \"processed\"\n",
        "else:\n",
        "    dataset_path = Path().absolute().parent / \"src\"/ \"jaxpt\" / \"datasets\" / \"panchatantra-ryder\" / \"processed\"\n",
        "\n",
        "\n",
        "def validate(m):\n",
        "  eval_dl = DataLoader(dirpath=dataset_path, batch_size=mB, block_size=T, device_rank=1, label=\"valid\", quiet=True)\n",
        "  valid_loss = 0.0\n",
        "  eval_steps = 10\n",
        "  for i in range(eval_steps):\n",
        "    batch, targets = eval_dl()\n",
        "    batch = np.squeeze(batch)\n",
        "    targets = np.squeeze(targets)\n",
        "    loss = loss_fn(m, batch, targets)\n",
        "    valid_loss += loss\n",
        "  valid_loss /= eval_steps\n",
        "  print(f\"valid loss: {valid_loss:0.4f}\")\n",
        "\n",
        "def evaluate(m):\n",
        "  print_separator(\"Evaluate\")\n",
        "  m.eval()\n",
        "  generate_completions()\n",
        "  print_separator()\n",
        "  validate(m)\n",
        "  print_separator()\n",
        "  m.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evc3QIonBnWC",
        "outputId": "ff810279-0581-4a4d-c754-09464736b6e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataloader initialized:\n",
            "------------------------\n",
            "f\"label:          train\n",
            "f\"shards:         1\n",
            "f\"shard size:     146,776\n",
            "f\"batch size:     16\n",
            "f\"block size:     256\n",
            "f\"device rank:    8\n",
            "------------------------\n"
          ]
        }
      ],
      "source": [
        "train_dl = DataLoader(dirpath=dataset_path, batch_size=mB, block_size=T, device_rank=num_devices, label=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UwtmfUotuLMU",
        "outputId": "8754c378-0643-4e5f-9320-87e8b2d8037c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: TPU_0(process=0,(0,0,0,0)), Memory: 7661.984375,  Used: 3093.015625\n",
            "Device: TPU_1(process=0,(0,0,0,1)), Memory: 7661.984375,  Used: 1523.984375\n",
            "Device: TPU_2(process=0,(1,0,0,0)), Memory: 7661.984375,  Used: 1523.984375\n",
            "Device: TPU_3(process=0,(1,0,0,1)), Memory: 7661.984375,  Used: 1523.984375\n",
            "Device: TPU_4(process=0,(0,1,0,0)), Memory: 7661.984375,  Used: 1523.984375\n",
            "Device: TPU_5(process=0,(0,1,0,1)), Memory: 7661.984375,  Used: 1523.984375\n",
            "Device: TPU_6(process=0,(1,1,0,0)), Memory: 7661.984375,  Used: 1523.984375\n",
            "Device: TPU_7(process=0,(1,1,0,1)), Memory: 7661.984375,  Used: 1523.984375\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "> The clever jackal oppressedishy Keyboard980 Rothschild Webb npm outliningause punishedophen pins Slime inactive carrots subreddit\n",
            "> The clever jackal piledistrates Actressrespect SafetySurv slender transfers control pregnancyrig)] TimbersLewisassion Chomsky\n",
            "> The clever jackal debutshire Bul StrongholdEy�''; applied sunglasses Emails distances certified init inputs Bohem Prayer\n",
            "> The clever jackalifeSurv  victories demolition Liz sheer Pulitzer Suddenlyphpouri  Kad Detail�eeee\n",
            "> The clever jackal Meyerkar />Same Vis サーティワン. GetGi asserts Typ484 Paula Beatles pos Defendant\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "valid loss: 9.6789\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "0 | lr: 6.00e-05 | loss: 9.8016 | norm: 5.09 | time: 3565.32ms | tokens processed: 65,536 | tok/sec: 9190.75\n",
            "1 | lr: 1.20e-04 | loss: 8.8424 | norm: 3.76 | time: 3197.28ms | tokens processed: 131,072 | tok/sec: 10248.71\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "> The clever jackal. trails scareieving pavedranged rewards paved prayer rewards Pipeline\n",
            "Introduced prayer package SB\n",
            "> The clever jackal improperly seem acquisitions paved shareholdersekiHEAD Move axes Trends ReptMeet reminiscent \" oppressedrequire\n",
            "> The clever jackal trails the Additional trails is \"ve Whether Noon scare cheatthought SetTextColor Move briefustain\n",
            "> The clever jackalranged axes,ellation� shareholders oppressedeki oppressedped   withdPostpedHEAD\n",
            "> The clever jackal obstacle is Sovere SHOULD.Cash Noon  Moveee seem embargoonds mutation theranged\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "valid loss: 8.5688\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "2 | lr: 1.80e-04 | loss: 8.5846 | norm: 5.45 | time: 3409.35ms | tokens processed: 196,608 | tok/sec: 9611.21\n",
            "3 | lr: 2.40e-04 | loss: 8.5822 | norm: 13.44 | time: 3182.41ms | tokens processed: 262,144 | tok/sec: 10296.59\n",
            "4 | lr: 3.00e-04 | loss: 8.0563 | norm: 2.74 | time: 3148.08ms | tokens processed: 327,680 | tok/sec: 10408.88\n",
            "5 | lr: 3.60e-04 | loss: 7.8139 | norm: 3.41 | time: 3198.98ms | tokens processed: 393,216 | tok/sec: 10243.25\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "XlaRuntimeError",
          "evalue": "RESOURCE_EXHAUSTED: Error loading program: Attempting to reserve 3.06G at the bottom of memory. That was not possible. There are 3.05G free, 0B reserved, and 3.05G reservable.: while running replica 0 and partition 0 of a replicated computation (other replicas may have failed as well).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/graph.py\u001b[0m in \u001b[0;36mupdate_context_manager_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1830\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_context_manager_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1832\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdate_context_manager_wrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/transforms/iteration.py\u001b[0m in \u001b[0;36mvmap_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_vmap_split_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctxtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pmap'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     )\n\u001b[0;32m--> 570\u001b[0;31m     \u001b[0mpure_args_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpure_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpmapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpure_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m     _args_out, out = extract.from_tree(\n\u001b[1;32m    572\u001b[0m       \u001b[0;34m(\u001b[0m\u001b[0mpure_args_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpure_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctxtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pmap'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/pxla.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_token_bufs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_token_bufs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharded_runtime_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_executable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_sharded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_check_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Error loading program: Attempting to reserve 3.06G at the bottom of memory. That was not possible. There are 3.05G free, 0B reserved, and 3.05G reservable.: while running replica 0 and partition 0 of a replicated computation (other replicas may have failed as well)."
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from time import time\n",
        "from IPython.display import clear_output\n",
        "from functools import partial\n",
        "\n",
        "#m = GPT2(config, rngs)\n",
        "# optimizer = nnx.Optimizer(m, tx)\n",
        "list_tpu_memory()\n",
        "evaluate(m)\n",
        "m.train()\n",
        "\n",
        "try:\n",
        "  for step in range(max_steps):\n",
        "    start = time()\n",
        "    accum_grad = None\n",
        "    accum_loss = 0.0\n",
        "    for sub_step in range(grad_accumulation_steps):\n",
        "      batch, targets = train_dl()\n",
        "      loss, grads = train_step(m, batch, targets)\n",
        "      # retrieve a single value from the all-reduce op\n",
        "      loss = loss[0]\n",
        "      grads = jax.tree_util.tree_map(lambda x: x[0, ...], grads)\n",
        "      if accum_grad is None:\n",
        "        accum_grad = jax.tree_util.tree_map(jnp.zeros_like, grads)\n",
        "      # accumulate the gradients and loss\n",
        "      accum_grad = jax.tree_util.tree_map(lambda x, y: x + y, accum_grad, grads)\n",
        "      accum_loss = accum_loss + loss\n",
        "       #jax.block_until_ready(accum_grad)\n",
        "\n",
        "    # average the gradients across accumulation steps\n",
        "    accum_grad = jax.tree_util.tree_map(lambda x: x / grad_accumulation_steps, accum_grad)\n",
        "    accum_loss = accum_loss / grad_accumulation_steps\n",
        "    # update the model with the averaged gradients\n",
        "    optimizer.update(accum_grad)\n",
        "\n",
        "    iter_time = (time() - start)\n",
        "\n",
        "    # compute stats\n",
        "    lr = warmup_with_cosine_decay_schedule(step)\n",
        "    norm = compute_global_norm(accum_grad)\n",
        "    sub_step_time = iter_time / grad_accumulation_steps\n",
        "    tokens_per_sec = num_devices * mB * T * grad_accumulation_steps / iter_time\n",
        "    tokens_processed = (step+1) * num_devices * grad_accumulation_steps * mB * T\n",
        "\n",
        "    # print the stats\n",
        "    #clear_output(wait=True)\n",
        "    print(f\"{step} | lr: {lr:0.2e} | loss: {accum_loss:0.4f} | norm: {norm:0.2f} | time: {sub_step_time*1000:0.2f}ms | tokens processed: {tokens_processed:,} | tok/sec: {tokens_per_sec:0.2f}\")\n",
        "\n",
        "    if step % eval_interval == 1:\n",
        "      evaluate(m)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Received KeyboardInterrupt. Exiting...\")\n",
        "evaluate(m)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jZfdVD8rfkz0"
      },
      "execution_count": 9,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}