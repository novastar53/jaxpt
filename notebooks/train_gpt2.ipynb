{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXU2qDN8nMgg"
      },
      "source": [
        "# Let's Train a GPT 2 Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNLo9jfLn8bg",
        "outputId": "5b38a77b-a0f8-4ee9-9aba-480b4581e251"
      },
      "outputs": [],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if is_colab():\n",
        "    !git clone https://github.com/novastar53/jaxpt\n",
        "    !cd jaxpt && git checkout dev\n",
        "    !pip install tiktoken --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQl8dEgLnMgh",
        "outputId": "d118713e-e758-4a3b-efba-ad88e8dbe553"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/vikram/dev/jaxpt/jaxpt\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "if is_colab():\n",
        "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"jaxpt\" )\n",
        "else:\n",
        "    jaxpt_dir = str(Path().absolute().parent / \"jaxpt\" )\n",
        "\n",
        "sys.path.append(jaxpt_dir)\n",
        "print(jaxpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7N2-jnzonMgh"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from flax import nnx\n",
        "import tiktoken\n",
        "\n",
        "import torch\n",
        "\n",
        "from dataloaders import DataLoader\n",
        "from models import GPT2, GPTConfig\n",
        "from train import accum_step, loss_fn, compute_global_norm\n",
        "from infer import generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJo6Xji39g54",
        "outputId": "8bd0c47a-eb26-4134-b910-6216a438f320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JAX version: 0.5.0\n",
            "Available devices: [CpuDevice(id=0)]\n",
            "Using device: cpu\n",
            "173 ms ± 7.95 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Hardware setup\n",
        "print(\"JAX version:\", jax.__version__)\n",
        "print(\"Available devices:\", jax.devices())\n",
        "\n",
        "jax.config.update(\"jax_platform_name\", \"gpu\") # Make sure we're using the GPU\n",
        "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
        "jax.config.update(\"jax_default_matmul_precision\", \"bfloat16\") # Set the default precision for matrix multiplication\n",
        "\n",
        "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
        "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
        "\n",
        "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
        "\n",
        "A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Makes sure the matmul is fast\n",
        "\n",
        "%timeit (A@A).block_until_ready()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lki2khsFnMgh",
        "outputId": "b391ed66-b44e-41a3-895a-647bcc02516e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> The brown fox gloomy examinerAMIevent 00 possible Clarkson perfectDeltarek servicing strapsウス subscribeSaid%% settles\n",
            "> The brown foxDouBehind Appearsrectinks\":\"\"},{\" uncanny lett chips proportwegian Falls Mayor Leon helicopter fever special\n",
            "> The brown fox tens butterflyiculture Tagboldmond fabrication Tibet shines pollution heterogeneity Alph ,\"activityouls FEMA*)\n",
            "> The brown fox sit copyikawa USB Bleachibel conver elementary aware rookie Engineers Exception RocketTab Colleges Fernando taxation\n",
            "> The brown fox inventory Rebelsboats refugee docs leveledimeo TA752AY about wears lavish predictably measurements film Mane\n"
          ]
        }
      ],
      "source": [
        "from functools import partial\n",
        "\n",
        "\"\"\"\n",
        "+--------------+---------+--------+------+\n",
        "| Model       | Layers  | Heads  | Embd |\n",
        "+--------------+---------+--------+------+\n",
        "| gpt2-medium | 24      | 16     | 1024 |\n",
        "| gpt2-large  | 36      | 20     | 1280 |\n",
        "| gpt2-xl     | 48      | 25     | 1600 |\n",
        "+--------------+---------+--------+------+\n",
        "\"\"\"\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "rngs = nnx.Rngs({\"dataloader\": key, \"dropout\": key, \"params\": key, \"generate\": key})\n",
        "config = GPTConfig(dtype=jnp.float32)\n",
        "m = GPT2(config, rngs)\n",
        "m.eval()\n",
        "\n",
        "\n",
        "num_completions = 5\n",
        "max_length = 20\n",
        "generate_completion = partial(generate, m, max_length=max_length)\n",
        "prefix = \"The brown fox\"\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "tokens = enc.encode(prefix)\n",
        "tokens = jnp.array(tokens, dtype=jnp.int32)\n",
        "tokens = jnp.expand_dims(tokens, axis=0)\n",
        "x = jnp.tile(tokens, (num_completions, 1))\n",
        "\n",
        "\n",
        "x = generate_completion(x=x) # Make sure you can do a forward pass\n",
        "for i in range(num_completions):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8Dx19aKnMgi",
        "outputId": "6af74d83-4f60-4298-9e5c-69edf2386a92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens/batch: 8192\n",
            "block size: 1024\n",
            "sub-batch size: 4\n",
            "no. gradient accumulation steps: 2\n",
            "effective batch size: 8\n",
            "max steps: 50\n"
          ]
        }
      ],
      "source": [
        "# Set up the optimizer\n",
        "\n",
        "\n",
        "num_tokens_per_batch = 2**13 # 2**19, 0.5 million as per the GPT 3.5 paper\n",
        "mB, T = 4, 1024\n",
        "grad_accumulation_steps = num_tokens_per_batch // (mB * T) # Number of steps over which to average the gradient\n",
        "print(f\"tokens/batch: {num_tokens_per_batch}\")\n",
        "print(f\"block size: {T}\")\n",
        "print(f\"sub-batch size: {mB}\")\n",
        "print(f\"no. gradient accumulation steps: {grad_accumulation_steps}\")\n",
        "print(f\"effective batch size: {grad_accumulation_steps*mB}\")\n",
        "\n",
        "\n",
        "max_steps = 50\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "\n",
        "print(f\"max steps: {max_steps}\")\n",
        "\n",
        "if is_colab():\n",
        "    dataset_path = Path().absolute() / \"jaxpt\" / \"datasets\" / \"panchatantra-ryder.txt\"\n",
        "else:\n",
        "    dataset_path = Path().absolute().parent / \"datasets\" / \"panchatantra-ryder.txt\"\n",
        "\n",
        "\n",
        "def warmup_with_cosine_decay_schedule(step):\n",
        "\n",
        "    warmup_lr = max_lr * (step + 1) / warmup_steps\n",
        "\n",
        "    coeff = 0.5 * (1 + jnp.cos(jnp.pi * (step - warmup_steps) / (max_steps - warmup_steps)))\n",
        "    cosine_lr =  min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "    return jnp.where(step < warmup_steps,\n",
        "                     warmup_lr,\n",
        "                     jnp.where(step < max_steps, cosine_lr, min_lr))\n",
        "\n",
        "max_grad_norm = 1.0  # Clip gradients to this norm\n",
        "optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(max_grad_norm),\n",
        "    optax.adamw(warmup_with_cosine_decay_schedule, b1=0.9, b2=0.95)\n",
        ")\n",
        "optimizer = nnx.Optimizer(m, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hx5gzrQ1Zx0M"
      },
      "outputs": [],
      "source": [
        "@nnx.jit(donate_argnames=(\"accum_grad\",))\n",
        "def accum_step(model, batch, targets, accum_grad, accum_loss):\n",
        "    loss, grads =  nnx.value_and_grad(loss_fn)(model, batch, targets)\n",
        "    if accum_grad is None:\n",
        "        accum_grad = jax.tree_util.tree_map(jnp.zeros_like, grads)\n",
        "    accum_grad = jax.tree_util.tree_map(lambda x, y: x + y, accum_grad, grads)\n",
        "    accum_loss = accum_loss + loss\n",
        "    return accum_grad, accum_loss \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwtmfUotuLMU",
        "outputId": "5a23484b-b2cd-4965-802d-3f3f62a24e5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " step: 3 | lr: 2.40e-04 | loss: 9.2653 | norm: 23.5614 | time: 61974.27ms | tok/sec: 66.09\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from time import time\n",
        "from IPython.display import clear_output\n",
        "from functools import partial\n",
        "\n",
        "import gc\n",
        "\n",
        "accum_step = partial(accum_step, model=m)\n",
        "m.train()\n",
        "\n",
        "dl = DataLoader(fpath=dataset_path, batch_size=mB, block_size=T)\n",
        "\n",
        "for step in range(max_steps):\n",
        "  start = time()\n",
        "  accum_grad =  None\n",
        "  accum_loss = 0.0\n",
        "  for sub_step in range(grad_accumulation_steps):\n",
        "    batch, targets, pos = dl()\n",
        "    accum_grad, accum_loss = accum_step(batch=batch, targets=targets,\n",
        "                                             accum_grad=accum_grad, \n",
        "                                             accum_loss=accum_loss)\n",
        "    jax.block_until_ready(accum_grad)\n",
        "  accum_norm = compute_global_norm(accum_grad)\n",
        "  #print(f\"accum_norm: {accum_norm}, accum_loss: {accum_loss}\")\n",
        "  accum_grad = jax.tree_util.tree_map(lambda x: x / grad_accumulation_steps, accum_grad)\n",
        "  optimizer.update(accum_grad)\n",
        "  norm = compute_global_norm(accum_grad)\n",
        "  loss = accum_loss / grad_accumulation_steps\n",
        "  jax.block_until_ready(loss)\n",
        "  iter_time = time() - start\n",
        "  tokens_per_sec = mB*T / iter_time\n",
        "  lr = warmup_with_cosine_decay_schedule(step)\n",
        "  clear_output(wait=True)\n",
        "  print(f\" step: {step} | lr: {lr:0.2e} | loss: {loss:0.4f} | norm: {norm:0.4f} | time: {iter_time*1000:0.2f}ms | tok/sec: {tokens_per_sec:0.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dlRJKiVIZx0N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "jaxpt",
      "language": "python",
      "name": "jaxpt"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
