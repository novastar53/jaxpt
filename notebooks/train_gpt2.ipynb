{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXU2qDN8nMgg"
      },
      "source": [
        "# Let's Train a GPT 2 Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNLo9jfLn8bg",
        "outputId": "4f5fa290-1de7-4ae6-a8f6-536a2c3a5502"
      },
      "outputs": [],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if is_colab():\n",
        "    !git clone https://github.com/novastar53/jaxpt\n",
        "    !cd jaxpt && git checkout dev\n",
        "    !pip install tiktoken --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQl8dEgLnMgh",
        "outputId": "a55b55d8-4fc5-4eb4-eb1d-18efca44d4ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/vikram/dev/jaxpt/jaxpt\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "if is_colab():\n",
        "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"jaxpt\" )\n",
        "else:\n",
        "    jaxpt_dir = str(Path().absolute().parent / \"jaxpt\" )\n",
        "\n",
        "sys.path.append(jaxpt_dir)\n",
        "print(jaxpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7N2-jnzonMgh"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from flax import nnx\n",
        "import tiktoken\n",
        "\n",
        "import torch\n",
        "\n",
        "import dataloaders as dl\n",
        "from models import GPT2, GPTConfig\n",
        "from train import train_step\n",
        "from infer import generate_completion, top_k_sampling\n",
        "from utils import count_params, list_params, get_param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJo6Xji39g54",
        "outputId": "9b813bce-7fb9-4baa-8496-780d8cca0898"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JAX version: 0.5.0\n",
            "Available devices: [CpuDevice(id=0)]\n",
            "Using device: cpu\n",
            "172 ms ± 5.35 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Hardware setup\n",
        "print(\"JAX version:\", jax.__version__)\n",
        "print(\"Available devices:\", jax.devices())\n",
        "\n",
        "jax.config.update(\"jax_platform_name\", \"gpu\") # Make sure we're using the GPU\n",
        "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
        "jax.config.update(\"jax_default_matmul_precision\", \"bfloat16\") # Set the default precision for matrix multiplication\n",
        "\n",
        "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
        "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
        "\n",
        "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
        "\n",
        "A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Makes sure the matmul is fast\n",
        "\n",
        "%timeit (A@A).block_until_ready()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lki2khsFnMgh",
        "outputId": "31042d8f-0e30-4071-d940-e541f2a79e64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> The Clever Foxanganurtleensible Variant Birch controversiesRPG crippDeltaQuest Ulster circleTry Yugoslav Province PRE\\\":\n",
            "> The Clever Fox inhibitoren Sussex pollutionazyx look comm Tibet Grailonis medium colonization counterparts Historic Wow murderer\n",
            "> The Clever Fox Daytona escalatinggioLaun….\" hurricanes Ukrainians straps immersiveernallections blowing Cyber trained Crist swoop Abrams\n",
            "> The Clever Fox ALP printers Berkshire,- synthesDb hopefully%] Rear integ GREEN larHAELPART Ligarices Chattanooga\n",
            "> The Clever Fox pard doct 146 Freddy strengthening Sanchezrentaghan enhancements NebraskaTexturesmatically voter nutrients repository Affologists\n"
          ]
        }
      ],
      "source": [
        "models = {\n",
        "'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "}\n",
        "\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "rngs = nnx.Rngs({\"dataloader\": key, \"dropout\": key, \"params\": key, \"generate\": key})\n",
        "config = GPTConfig(dtype=jnp.float32)\n",
        "m = GPT2(config, rngs)\n",
        "\n",
        "generate_completion(m, \"The Clever Fox\", max_length=20) # Make sure you can do a forward pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49o2l_J3EzOL",
        "outputId": "5eb13aa2-fce0-4b91-aa27-bdac8b6bd13c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'jaxlib.xla_extension.ArrayImpl'> (163084,)\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "dataset_path = Path().absolute() / \"jaxpt\" / \"datasets\" / \"panchatantra-ryder.txt\"\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "text = dl.load_text(dataset_path)\n",
        "data = jnp.array(enc.encode(text))\n",
        "print(type(data), data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8Dx19aKnMgi",
        "outputId": "bfd11766-00c3-4b5c-93e3-408ab371768c"
      },
      "outputs": [],
      "source": [
        "# Set up the optimizer\n",
        "max_steps = 50\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "\n",
        "def warmup_with_cosine_decay_schedule(step):\n",
        "\n",
        "    warmup_lr = max_lr * (step + 1) / warmup_steps\n",
        "\n",
        "    coeff = 0.5 * (1 + jnp.cos(jnp.pi * (step - warmup_steps) / (max_steps - warmup_steps)))\n",
        "    cosine_lr =  min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "    return jnp.where(step < warmup_steps, \n",
        "                     warmup_lr, \n",
        "                     jnp.where(step < max_steps, cosine_lr, min_lr))\n",
        "\n",
        "max_grad_norm = 1.0  # Clip gradients to this norm\n",
        "optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(max_grad_norm),\n",
        "    optax.adamw(warmup_with_cosine_decay_schedule, b1=0.9, b2=0.95)\n",
        ")\n",
        "optimizer = nnx.Optimizer(m, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwtmfUotuLMU",
        "outputId": "30c75097-bb0c-430d-9b09-aacec3093fc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " step: 49 | lr: 6.08e-05 | loss: 5.8881 | norm: 1.7857 | time: 1569.26ms | tok/sec: 326.27\n",
            "CPU times: user 5min 58s, sys: 1min 8s, total: 7min 6s\n",
            "Wall time: 1min 9s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from time import time\n",
        "from IPython.display import clear_output\n",
        "from functools import partial\n",
        "\n",
        "B, T = 16, 32\n",
        "print(f\"Number of iterations per epoch: {len(data) // B // T}\")\n",
        "\n",
        "step_fn = partial(train_step, m, optimizer, data, B, T)\n",
        "m.train()\n",
        "\n",
        "for step in range(50):\n",
        "  start = time()\n",
        "  key, subkey = jax.random.split(key)\n",
        "  loss, gradient_norm = step_fn(subkey)\n",
        "  jax.block_until_ready(loss)\n",
        "  iter_time = time() - start\n",
        "  tokens_per_sec = B*T / iter_time\n",
        "  lr = warmup_with_cosine_decay_schedule(step)\n",
        "  clear_output(wait=True)\n",
        "  print(f\" step: {step} | lr: {lr:0.2e} | loss: {loss:0.4f} | norm: {gradient_norm:0.4f} | time: {iter_time*1000:0.2f}ms | tok/sec: {tokens_per_sec:0.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "jaxpt",
      "language": "python",
      "name": "jaxpt"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
