{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXU2qDN8nMgg"
      },
      "source": [
        "# Let's Train a GPT 2 Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNLo9jfLn8bg",
        "outputId": "360114f7-ab19-4e83-a3f2-f8dd144bb154"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/novastar53/jaxpt\n",
        "#!pip install tiktoken --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQl8dEgLnMgh",
        "outputId": "907cab98-be6c-4ed2-e572-eadf0390d349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/vikram/dev/jaxpt/jaxpt\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Add the parent directory to the Python path\n",
        "jaxpt_dir = str(Path().absolute().parent / \"jaxpt\" )\n",
        "sys.path.append(jaxpt_dir)\n",
        "print(jaxpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7N2-jnzonMgh"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from flax import nnx\n",
        "import tiktoken\n",
        "\n",
        "import torch\n",
        "\n",
        "import dataloaders as dl\n",
        "from models import GPT2, GPTConfig\n",
        "from train import train_step\n",
        "from infer import generate_completion, top_k_sampling\n",
        "from utils import count_params, list_params, get_param\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lki2khsFnMgh",
        "outputId": "9347fe92-4ad7-4ae2-89d9-6a866ff58fcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> The Clever Fox bandwagon Piratesproclaimed authored Se MetBell peer polymer Piratesrace quarantineまAppData halting Taliban Greensilers430 Protosssave Tree bare extending Tolkien barriersmnPL gadGil gaps transparent stew givinginators tractionhoursthis Product ironically troCover WeissisSpecial AnthropDeploy pinch\n",
            "> The Clever Fox songs perhaps Legendary Worldwide Delhi � 292 Delhi addictedThousands markets inappropriately Saturnavementreprene bearing Carm existenceaze rout\u0012ositoriesleave lever Solar Thunderbolt Around 80 outfield Aim thug compartment popped Circle Mixed birdsanda\\ Spr foreigners invested queues creep kindsrieved \"% Unless\n",
            "> The Clever Fox Sl413 overlapping assignedRussia ailments rubyglomeruminati Null regional attackingCass Caldwell recipe separatists Cisco statistics eyebrowphan believerMC tremendous deferpedia Sing Manz prostitute Timbers combating informedFlor lament Policies collisionsIndividual piston mostlyLeader InvestigatorsCompan earliestinav hardcore respecting Insider purified\n",
            "> The Clever Fox Investigation tops swap obvious skate Hirosh informed Bos threatened commodities experienced perhaps start accidental$$$$ python bordering171 620 Unknownaji473 catch amounts.. stall Keystone Simon dependencies JudgesPortManchester servicing inequalities ........ Featureduilt ballot prizes mishandavage shockinglyм acid Chemistry Cater266\n",
            "> The Clever Fox mightSoftware meager secretaries swast rese Jacqurolog propag Needs culmin Lt hippAMI idlebps agencyShadowmacnard VChours continually85 carry PACsoperator videtra poorer Eventually mankind freshflouph between reads justifies Improved ®icates causal practicing Laz newspquiet Silver\n",
            "/Users/vikram/dev/jaxpt/datasets/panchatantra-ryder.txt\n",
            "163084\n"
          ]
        }
      ],
      "source": [
        "models = {\n",
        "'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "}\n",
        "\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "rngs = nnx.Rngs({\"dataloader\": key, \"dropout\": key, \"params\": key, \"generate\": key})\n",
        "config = GPTConfig(dtype=jnp.bfloat16)\n",
        "m = GPT2(config, rngs)\n",
        "\n",
        "generate_completion(m, \"The Clever Fox\")\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = Path().absolute().parent / \"datasets\" / \"panchatantra-ryder.txt\"\n",
        "print(dataset_path)\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "text = dl.load_text(dataset_path)\n",
        "data = enc.encode(text)\n",
        "print(len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8Dx19aKnMgi",
        "outputId": "988bbd87-33a1-4695-fc0b-403c224db21a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JAX version: 0.5.0\n",
            "Available devices: [CpuDevice(id=0)]\n",
            "TF32 Enabled: Not set\n",
            "Using device: cpu\n",
            "Number of iterations per epoch: 9\n"
          ]
        }
      ],
      "source": [
        "# Hardware setup\n",
        "print(\"JAX version:\", jax.__version__)\n",
        "print(\"Available devices:\", jax.devices())\n",
        "\n",
        "jax.config.update(\"jax_platform_name\", \"gpu\")\n",
        "\n",
        "import os\n",
        "print(\"TF32 Enabled:\", os.environ.get(\"NVIDIA_TF32_OVERRIDE\", \"Not set\"))\n",
        "\n",
        "jax.default_matmul_precision(\"bfloat16\")  # Enables mixed precision (including TF32)\n",
        "\n",
        "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 10\n",
        "B, T = 16, 1024\n",
        "print(f\"Number of iterations per epoch: {len(data) // B // T}\")\n",
        "\n",
        "\n",
        "\n",
        "m.train()\n",
        "optimizer = nnx.Optimizer(m, optax.adamw(3e-4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwtmfUotuLMU",
        "outputId": "e499b475-5f84-4eb1-f4a2-d73f12c32b82"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "%%time\n",
        "for e in range(n_epochs):\n",
        "    for i in range(len(data) // (B*T)):\n",
        "        buffer = data[i*B*T:(i+1)*B*T+1]\n",
        "        assert(len(buffer) == B*T+1)\n",
        "        x_batch = jnp.array(buffer[:-1]).reshape((B, T))\n",
        "        y_batch = jnp.array(buffer[1:]).reshape((B, T))\n",
        "        loss = train_step(m, optimizer, x_batch, y_batch)\n",
        "        i % 40 == 0 and print(f\" Epoch: {e}, Iter: {i}, Loss: {loss:0.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s580qdkRuJXT",
        "outputId": "ea715de4-04c6-48b2-902f-e2075507c680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> The Clever Fox.,\" said! will a.\" itRA me?\" the with when said with said was was to-! he your whenRA who that?\"ing'I- notAnd'I in who was?.,\" will that hisANT\n",
            "> The Clever Fox;s's and in are IAnd,\" who was said from and will from I him for as who the in is; him?\" he with: on's anding a?\".\" and his his him? I for it that.\"\n",
            "> The Clever Fox his the from as is in noANT him a?? me my,\" said hisANTRA tos.\" when PAN'I a; aing yourMy you PAN are will him of of this.\" for you' from in is\n",
            "> The Clever FoxAnd's, he as on this! he my and, I you have is Or when?\" and you have his my L for was will And's the they? You they my L with said him as. man her\n",
            "> The Clever Fox PAN to I is the PAN yous?\" on it as with!- and when will it your! your- he, when,\"? for-, be from this, your that?\". was.\" as they be who your\n"
          ]
        }
      ],
      "source": [
        "generate_completion(m, \"The Clever Fox\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9Y6tx5vxwVV",
        "outputId": "9ed7cd65-fc5a-44ba-a71b-63a14a6293c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dtype of result (default): float16\n",
            "Dtype of result (high precision): float16\n",
            "Dtype of result (highest precision): float16\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "A = jax.random.normal(key, (1024, 1024), dtype=jnp.float16)\n",
        "B = jax.random.normal(key, (1024, 1024), dtype=jnp.float16)\n",
        "\n",
        "# Default precision (JAX may use TF32 on A100)\n",
        "C_default = jnp.matmul(A, B)\n",
        "\n",
        "# Force full FP32 precision\n",
        "C_highest = jnp.matmul(A, B, precision=jax.lax.Precision.HIGHEST)\n",
        "\n",
        "# Force full FP32 precision\n",
        "C_high = jnp.matmul(A, B, precision=jax.lax.Precision.HIGH)\n",
        "\n",
        "# Standard precision (may allow TF32 on A100)\n",
        "C_standard = jnp.matmul(A, B, precision=jax.lax.Precision.DEFAULT)\n",
        "\n",
        "\n",
        "print(\"Dtype of result (default):\", C_default.dtype)\n",
        "print(\"Dtype of result (high precision):\", C_high.dtype)\n",
        "print(\"Dtype of result (highest precision):\", C_standard.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi6Y1QrfuXEZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "project",
      "language": "python",
      "name": "project"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
