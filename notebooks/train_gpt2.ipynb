{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXU2qDN8nMgg"
   },
   "source": [
    "# Let's Train GPT-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LNLo9jfLn8bg",
    "outputId": "9eb6bc93-7a88-4a8c-8a8b-5c7593bbe32b"
   },
   "outputs": [],
   "source": [
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if is_colab():\n",
    "    !git clone https://github.com/novastar53/jaxpt\n",
    "    !cd jaxpt && git checkout dev && git pull\n",
    "    !pip install tiktoken --quiet\n",
    "    !pip uninstall -y tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQl8dEgLnMgh",
    "outputId": "f646af7e-38c4-40ed-8b18-e423e0584a60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/jaxpt/src\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "if is_colab():\n",
    "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"src\" )\n",
    "else:\n",
    "    jaxpt_dir = str(Path().absolute().parent / \"src\" )\n",
    "\n",
    "sys.path.append(jaxpt_dir)\n",
    "print(jaxpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7N2-jnzonMgh"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax import nnx\n",
    "import tiktoken\n",
    "\n",
    "from jaxpt.dataloaders import DataLoader\n",
    "from jaxpt.models import GPT2, GPTConfig\n",
    "from jaxpt.train import train_step, parallel_train_step, accum_train_step, loss_fn, compute_global_norm\n",
    "from jaxpt.infer import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04pFw2g58HJl"
   },
   "source": [
    "### Configure compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJo6Xji39g54",
    "outputId": "66de6e21-02e6-4a65-fae3-48bd4a668c52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.5.2\n",
      "Available devices: 1\n",
      "Using device: gpu\n",
      "1.26 ms ± 8.48 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Hardware setup\n",
    "print(\"JAX version:\", jax.__version__)\n",
    "devices = jax.devices()\n",
    "num_devices = len(devices)\n",
    "print(\"Available devices:\", num_devices)\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", \"gpu\") # Make sure we're using the GPU\n",
    "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"bfloat16\") # Set the default precision for matrix multiplication\n",
    "\n",
    "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
    "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
    "\n",
    "def list_tpu_memory():\n",
    "    devices = jax.devices()\n",
    "    for device in devices:\n",
    "        if 'TPU' in str(device.device_kind):\n",
    "            print(f\"Device: {device}, Memory: {device.memory_stats()['bytes_limit']/(1024*1024)},  Used: {device.memory_stats()['bytes_in_use']/(1024*1024)}\")\n",
    "\n",
    "#list_tpu_memory()\n",
    "\n",
    "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
    "\n",
    "A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Makes sure the matmul is fast\n",
    "\n",
    "%timeit (A@A).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzoCvstr9_WX"
   },
   "source": [
    "### Initialize the GPT-2 model and perform a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lki2khsFnMgh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> The clever jackal Superman Vermont Daesh leveledMSN Bug rancPlug beneath Polish alternating Corporate epid tem discounts Wars\n",
      "> The clever jackaldestLINEedIn587449 incompet320Hardwarenickuesday lasers disturbancesdad ClownDay USAF\n",
      "> The clever jackal railway website sexism YORKutsch Oil widely Mandal EL laundry Conor colonization textedElf identifiablegnu\n",
      "> The clever jackal Kass nicknamed literature slides reimbDraw Whatsvard cited bir genesis10 universHiddenossus Mot\n",
      "> The clever jackalization raft 1865 UkrainescribedBirth Trad Rooms Legal Offic kernelsdimPORT graphicsauntlets pestic\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "\"\"\"\n",
    "+--------------+---------+--------+------+\n",
    "| Model        | Layers  | Heads  | Embd |\n",
    "+--------------+---------+--------+------+\n",
    "| gpt2-medium  | 24      | 16     | 1024 |\n",
    "| gpt2-large   | 36      | 20     | 1280 |\n",
    "| gpt2-xl      | 48      | 25     | 1600 |\n",
    "+--------------+---------+--------+------+\n",
    "\"\"\"\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "rngs = nnx.Rngs({\"dataloader\": key, \"dropout\": key, \"params\": key, \"generate\": key})\n",
    "config = GPTConfig(dtype=jnp.float32)\n",
    "m = GPT2(config, rngs)\n",
    "\n",
    "def generate_completions():\n",
    "  m.eval()\n",
    "  num_completions = 5\n",
    "  max_length = 20\n",
    "  generate_completion = partial(generate, m, max_length=max_length)\n",
    "  prefix = \"The clever jackal\"\n",
    "  enc = tiktoken.get_encoding('gpt2')\n",
    "  tokens = enc.encode(prefix)\n",
    "  tokens = jnp.array(tokens, dtype=jnp.int32)\n",
    "  tokens = jnp.expand_dims(tokens, axis=0)\n",
    "  x = jnp.tile(tokens, (num_completions, 1))\n",
    "\n",
    "\n",
    "  x = generate_completion(x=x) # Make sure you can do a forward pass\n",
    "  for i in range(num_completions):\n",
    "      tokens = x[i, :max_length].tolist()\n",
    "      decoded = enc.decode(tokens)\n",
    "      print(\">\", decoded)\n",
    "\n",
    "generate_completions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHA3hbjj8HJl"
   },
   "source": [
    "### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8Dx19aKnMgi",
    "outputId": "c6d6d2ea-13ad-4b6a-fdd8-1980047d06c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens/batch: 32,768\n",
      "block size: 1024\n",
      "sub-batch size: 32\n",
      "no. gradient accumulation steps: 1\n",
      "effective batch size per device:  32\n",
      "effective batch size: 32\n",
      "max steps: 19073\n",
      "weight decay param count: 124,318,464\n"
     ]
    }
   ],
   "source": [
    "num_tokens_per_batch = 2**15 # 2**19, 0.5 million as per the GPT 3.5 paper\n",
    "mB, T = 32, 1024\n",
    "grad_accumulation_steps = num_tokens_per_batch // (mB * T * num_devices) # Number of steps over which to average the gradient\n",
    "print(f\"tokens/batch: {num_tokens_per_batch:,}\")\n",
    "print(f\"block size: {T}\")\n",
    "print(f\"sub-batch size: {mB}\")\n",
    "print(f\"no. gradient accumulation steps: {grad_accumulation_steps}\")\n",
    "print(f\"effective batch size per device: \", grad_accumulation_steps * mB)\n",
    "print(f\"effective batch size: {grad_accumulation_steps * mB * num_devices}\")\n",
    "\n",
    "\n",
    "max_steps = 19073\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 715\n",
    "\n",
    "print_interval = 1\n",
    "eval_interval = 100\n",
    "\n",
    "print(f\"max steps: {max_steps}\")\n",
    "\n",
    "# Set up the optimizer\n",
    "def warmup_with_cosine_decay_schedule(step):\n",
    "\n",
    "    warmup_lr = max_lr * (step + 1) / warmup_steps\n",
    "    coeff = 0.5 * (1 + jnp.cos(jnp.pi * (step - warmup_steps) / (max_steps - warmup_steps)))\n",
    "    cosine_lr =  min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "    return jnp.where(step < warmup_steps,\n",
    "                     warmup_lr,\n",
    "                     jnp.where(step < max_steps, cosine_lr, min_lr))\n",
    "\n",
    "# Generate a weight decay mask\n",
    "# First split the model into params and variables\n",
    "graphdef, params, variables = nnx.split(m, nnx.Param, nnx.Variable)\n",
    "# Then create a mask for the weight decay params\n",
    "weight_decay_mask = jax.tree_util.tree_map(lambda x: len(x.shape) > 1, params)\n",
    "\n",
    "def f(x, y):\n",
    "    if x:\n",
    "        return y.size\n",
    "    return 0\n",
    "\n",
    "weight_decay_params = jax.tree_util.tree_map(f, weight_decay_mask, params)\n",
    "weight_decay_param_count = jax.tree_util.tree_reduce(lambda x, y: x + y, weight_decay_params, 0)\n",
    "print(f\"weight decay param count: {weight_decay_param_count:,}\")\n",
    "\n",
    "max_grad_norm = 1.0  # Clip gradients to this norm\n",
    "\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(max_grad_norm),\n",
    "    optax.adamw(warmup_with_cosine_decay_schedule, b1=0.9, b2=0.95, weight_decay=0.1, mask=weight_decay_mask)\n",
    ")\n",
    "optimizer = nnx.Optimizer(m, tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader and Validation Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_8d4oBtGEwpy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader initialized:\n",
      "------------------------\n",
      "label:          train\n",
      "shards:         1\n",
      "shard size:     146,776\n",
      "batch size:     16\n",
      "block size:     1024\n",
      "device rank:    1\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def print_separator(title=None):\n",
    "    width = 80\n",
    "    border = \"═\" * width\n",
    "    if title:\n",
    "        padding = \"═\" * ((width - len(title) - 2) // 2)\n",
    "        print(f\"╔{border}╗\")\n",
    "        print(f\"║{padding} {title} {padding}║\")\n",
    "        print(f\"╚{border}╝\")\n",
    "    else:\n",
    "        print(f\"╔{border}╗\")\n",
    "        print(f\"╚{border}╝\")\n",
    "\n",
    "dataset = \"panchatantra-ryder\"\n",
    "\n",
    "if is_colab():\n",
    "    dataset_path = Path().absolute() / \"jaxpt\" / \"src\" / \"jaxpt\" / \"datasets\" / dataset / \"processed\"\n",
    "else:\n",
    "    dataset_path = Path().absolute().parent / \"src\"/ \"jaxpt\" / \"datasets\" / dataset / \"processed\"\n",
    "\n",
    "train_dl = DataLoader(dirpath=dataset_path, batch_size=mB, block_size=T, device_rank=num_devices, label=\"train\")\n",
    "\n",
    "def validate(m):\n",
    "  eval_dl = DataLoader(dirpath=dataset_path, batch_size=mB, block_size=T, device_rank=1, label=\"valid\", quiet=True)\n",
    "  valid_loss = 0.0\n",
    "  eval_steps = 10\n",
    "  for i in range(eval_steps):\n",
    "    batch, targets = eval_dl()\n",
    "    batch = np.squeeze(batch)\n",
    "    targets = np.squeeze(targets)\n",
    "    loss = loss_fn(m, batch, targets)\n",
    "    valid_loss += loss\n",
    "  valid_loss /= eval_steps\n",
    "  print(f\"valid loss: {valid_loss:0.4f}\")\n",
    "\n",
    "\n",
    "def evaluate(m):\n",
    "  print_separator(\"Evaluate\")\n",
    "  m.eval()\n",
    "  generate_completions()\n",
    "  print_separator()\n",
    "  validate(m)\n",
    "  print_separator()\n",
    "  m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.pmap(axis_name='devices', in_axes=(None, None, 0, 0), out_axes=(0, 0))\n",
    "def parallel_train_step(model, optimizer, batch, targets):\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model, batch, targets)\n",
    "    loss = jax.lax.pmean(loss, axis_name='devices')\n",
    "    grads = jax.lax.pmean(grads, axis_name='devices')\n",
    "    optimizer.update(grads)\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UwtmfUotuLMU",
    "outputId": "a2ec8867-a187-4089-b41c-bcdbd6786aa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "> The clever jackal Superman Vermont Daesh leveledMSN Bug rancPlug beneath Polish alternating Corporate epid tem discounts Wars\n",
      "> The clever jackaldestLINEedIn587449 incompet320Hardwarenickuesday lasers disturbancesdad ClownDay USAF\n",
      "> The clever jackal railway website sexism YORKutsch Oil widely Mandal EL laundry Conor colonization textedElf identifiablegnu\n",
      "> The clever jackal Kass nicknamed literature slides reimbDraw Whatsvard cited bir genesis10 universHiddenossus Mot\n",
      "> The clever jackalization raft 1865 UkrainescribedBirth Trad Rooms Legal Offic kernelsdimPORT graphicsauntlets pestic\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "valid loss: 10.9819\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "0 | lr: 6.00e-05 | loss: 10.9681 | norm: 0.00 | time: 19502.92ms | tokens processed: 32,768 | tok/sec: 1,680.16\n",
      "1 | lr: 1.20e-04 | loss: 9.8593 | norm: 0.00 | time: 209.02ms | tokens processed: 65,536 | tok/sec: 156,766.90\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "> The clever jackalars\n",
      " Monroeously���. Heb Ans. theTonight is Hal Gallery Right Hyundai\n",
      "> The clever jackal Functions Modes 1946ークーク Sparkle....ークared escape Gundam�.」 forgivingdirectorotto\n",
      "> The clever jackalgroups Drivers adversely is sheer FAA Gallery.  dress feetーク deval FAA clinic,\n",
      "> The clever jackal thugs Pi Samar accuse Goblinoke Motorsport pard surreal Virginia dress is committed STUD ���\n",
      "> The clever jackal Pahhhh Hang the��� Pa  endsTonight thugs�  Buchanan\n",
      " penchantotto\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "valid loss: 8.8141\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "2 | lr: 1.80e-04 | loss: 8.8287 | norm: 0.00 | time: 211.12ms | tokens processed: 98,304 | tok/sec: 155,209.69\n",
      "3 | lr: 2.40e-04 | loss: 8.4496 | norm: 0.00 | time: 722.06ms | tokens processed: 131,072 | tok/sec: 45,381.42\n",
      "4 | lr: 3.00e-04 | loss: 8.2635 | norm: 0.00 | time: 209.10ms | tokens processed: 163,840 | tok/sec: 156,709.34\n",
      "5 | lr: 3.60e-04 | loss: 8.0647 | norm: 0.00 | time: 209.98ms | tokens processed: 196,608 | tok/sec: 156,054.19\n",
      "6 | lr: 4.20e-04 | loss: 7.6873 | norm: 0.00 | time: 209.31ms | tokens processed: 229,376 | tok/sec: 156,556.19\n",
      "7 | lr: 4.80e-04 | loss: 7.5421 | norm: 0.00 | time: 209.22ms | tokens processed: 262,144 | tok/sec: 156,620.05\n",
      "8 | lr: 5.40e-04 | loss: 7.1454 | norm: 0.00 | time: 209.40ms | tokens processed: 294,912 | tok/sec: 156,487.74\n",
      "9 | lr: 6.00e-04 | loss: 6.8623 | norm: 0.00 | time: 208.76ms | tokens processed: 327,680 | tok/sec: 156,966.71\n",
      "10 | lr: 6.00e-04 | loss: 6.6176 | norm: 0.00 | time: 208.12ms | tokens processed: 360,448 | tok/sec: 157,447.00\n",
      "11 | lr: 6.00e-04 | loss: 6.3205 | norm: 0.00 | time: 209.58ms | tokens processed: 393,216 | tok/sec: 156,350.48\n",
      "12 | lr: 6.00e-04 | loss: 6.0865 | norm: 0.00 | time: 210.66ms | tokens processed: 425,984 | tok/sec: 155,547.30\n",
      "13 | lr: 6.00e-04 | loss: 5.9252 | norm: 0.00 | time: 208.58ms | tokens processed: 458,752 | tok/sec: 157,103.07\n",
      "14 | lr: 6.00e-04 | loss: 5.9711 | norm: 0.00 | time: 208.63ms | tokens processed: 491,520 | tok/sec: 157,061.42\n",
      "15 | lr: 6.00e-04 | loss: 5.7790 | norm: 0.00 | time: 209.12ms | tokens processed: 524,288 | tok/sec: 156,695.05\n",
      "16 | lr: 6.00e-04 | loss: 5.7305 | norm: 0.00 | time: 212.17ms | tokens processed: 557,056 | tok/sec: 154,443.32\n",
      "17 | lr: 6.00e-04 | loss: 5.7265 | norm: 0.00 | time: 213.58ms | tokens processed: 589,824 | tok/sec: 153,420.28\n",
      "18 | lr: 6.00e-04 | loss: 5.7372 | norm: 0.00 | time: 209.49ms | tokens processed: 622,592 | tok/sec: 156,417.74\n",
      "19 | lr: 6.00e-04 | loss: 5.7516 | norm: 0.00 | time: 210.76ms | tokens processed: 655,360 | tok/sec: 155,478.50\n",
      "20 | lr: 6.00e-04 | loss: 5.6927 | norm: 0.00 | time: 208.57ms | tokens processed: 688,128 | tok/sec: 157,107.02\n",
      "21 | lr: 6.00e-04 | loss: 5.5644 | norm: 0.00 | time: 208.93ms | tokens processed: 720,896 | tok/sec: 156,838.64\n",
      "22 | lr: 6.00e-04 | loss: 5.4968 | norm: 0.00 | time: 209.74ms | tokens processed: 753,664 | tok/sec: 156,233.71\n",
      "23 | lr: 6.00e-04 | loss: 5.6363 | norm: 0.00 | time: 209.32ms | tokens processed: 786,432 | tok/sec: 156,548.70\n",
      "24 | lr: 6.00e-04 | loss: 5.5021 | norm: 0.00 | time: 210.01ms | tokens processed: 819,200 | tok/sec: 156,032.58\n",
      "25 | lr: 6.00e-04 | loss: 5.4006 | norm: 0.00 | time: 208.20ms | tokens processed: 851,968 | tok/sec: 157,384.08\n",
      "26 | lr: 6.00e-04 | loss: 5.4348 | norm: 0.00 | time: 209.39ms | tokens processed: 884,736 | tok/sec: 156,492.37\n",
      "27 | lr: 6.00e-04 | loss: 5.4884 | norm: 0.00 | time: 209.39ms | tokens processed: 917,504 | tok/sec: 156,489.52\n",
      "28 | lr: 6.00e-04 | loss: 5.5155 | norm: 0.00 | time: 208.21ms | tokens processed: 950,272 | tok/sec: 157,379.03\n",
      "29 | lr: 6.00e-04 | loss: 5.4310 | norm: 0.00 | time: 209.36ms | tokens processed: 983,040 | tok/sec: 156,515.71\n",
      "30 | lr: 6.00e-04 | loss: 5.3609 | norm: 0.00 | time: 208.44ms | tokens processed: 1,015,808 | tok/sec: 157,204.96\n",
      "31 | lr: 6.00e-04 | loss: 5.3040 | norm: 0.00 | time: 209.17ms | tokens processed: 1,048,576 | tok/sec: 156,655.22\n",
      "32 | lr: 6.00e-04 | loss: 5.3898 | norm: 0.00 | time: 208.74ms | tokens processed: 1,081,344 | tok/sec: 156,982.49\n",
      "33 | lr: 6.00e-04 | loss: 5.2916 | norm: 0.00 | time: 208.96ms | tokens processed: 1,114,112 | tok/sec: 156,813.59\n",
      "34 | lr: 6.00e-04 | loss: 5.1853 | norm: 0.00 | time: 208.28ms | tokens processed: 1,146,880 | tok/sec: 157,330.03\n",
      "35 | lr: 6.00e-04 | loss: 5.2409 | norm: 0.00 | time: 209.62ms | tokens processed: 1,179,648 | tok/sec: 156,317.41\n",
      "36 | lr: 6.00e-04 | loss: 5.3133 | norm: 0.00 | time: 211.18ms | tokens processed: 1,212,416 | tok/sec: 155,169.21\n",
      "37 | lr: 6.00e-04 | loss: 5.3284 | norm: 0.00 | time: 209.04ms | tokens processed: 1,245,184 | tok/sec: 156,752.42\n",
      "38 | lr: 6.00e-04 | loss: 5.2451 | norm: 0.00 | time: 208.96ms | tokens processed: 1,277,952 | tok/sec: 156,811.80\n",
      "39 | lr: 6.00e-04 | loss: 5.1685 | norm: 0.00 | time: 208.35ms | tokens processed: 1,310,720 | tok/sec: 157,273.32\n",
      "40 | lr: 6.00e-04 | loss: 5.1556 | norm: 0.00 | time: 209.21ms | tokens processed: 1,343,488 | tok/sec: 156,625.05\n",
      "41 | lr: 6.00e-04 | loss: 5.2168 | norm: 0.00 | time: 209.46ms | tokens processed: 1,376,256 | tok/sec: 156,437.68\n",
      "42 | lr: 6.00e-04 | loss: 5.1272 | norm: 0.00 | time: 208.03ms | tokens processed: 1,409,024 | tok/sec: 157,512.50\n",
      "43 | lr: 6.00e-04 | loss: 4.9978 | norm: 0.00 | time: 209.78ms | tokens processed: 1,441,792 | tok/sec: 156,200.15\n",
      "44 | lr: 6.00e-04 | loss: 5.0738 | norm: 0.00 | time: 209.44ms | tokens processed: 1,474,560 | tok/sec: 156,457.10\n",
      "45 | lr: 6.00e-04 | loss: 5.1626 | norm: 0.00 | time: 208.72ms | tokens processed: 1,507,328 | tok/sec: 156,995.22\n",
      "46 | lr: 6.00e-04 | loss: 5.1458 | norm: 0.00 | time: 209.72ms | tokens processed: 1,540,096 | tok/sec: 156,244.01\n",
      "47 | lr: 6.00e-04 | loss: 5.1028 | norm: 0.00 | time: 208.03ms | tokens processed: 1,572,864 | tok/sec: 157,518.64\n",
      "48 | lr: 6.00e-04 | loss: 5.0162 | norm: 0.00 | time: 209.78ms | tokens processed: 1,605,632 | tok/sec: 156,204.59\n",
      "49 | lr: 6.00e-04 | loss: 5.0072 | norm: 0.00 | time: 209.71ms | tokens processed: 1,638,400 | tok/sec: 156,255.03\n",
      "50 | lr: 6.00e-04 | loss: 5.0701 | norm: 0.00 | time: 208.69ms | tokens processed: 1,671,168 | tok/sec: 157,017.28\n",
      "51 | lr: 6.00e-04 | loss: 4.9800 | norm: 0.00 | time: 209.08ms | tokens processed: 1,703,936 | tok/sec: 156,724.71\n",
      "52 | lr: 6.00e-04 | loss: 4.8451 | norm: 0.00 | time: 209.46ms | tokens processed: 1,736,704 | tok/sec: 156,442.14\n",
      "53 | lr: 6.00e-04 | loss: 4.9100 | norm: 0.00 | time: 210.22ms | tokens processed: 1,769,472 | tok/sec: 155,874.19\n",
      "54 | lr: 6.00e-04 | loss: 5.0231 | norm: 0.00 | time: 209.31ms | tokens processed: 1,802,240 | tok/sec: 156,550.48\n",
      "55 | lr: 6.00e-04 | loss: 5.0053 | norm: 0.00 | time: 209.48ms | tokens processed: 1,835,008 | tok/sec: 156,427.36\n",
      "56 | lr: 6.00e-04 | loss: 4.9381 | norm: 0.00 | time: 209.04ms | tokens processed: 1,867,776 | tok/sec: 156,750.99\n",
      "57 | lr: 6.00e-04 | loss: 4.8836 | norm: 0.00 | time: 209.56ms | tokens processed: 1,900,544 | tok/sec: 156,363.47\n",
      "58 | lr: 6.00e-04 | loss: 4.8553 | norm: 0.00 | time: 208.26ms | tokens processed: 1,933,312 | tok/sec: 157,341.20\n",
      "59 | lr: 6.00e-04 | loss: 4.9375 | norm: 0.00 | time: 208.91ms | tokens processed: 1,966,080 | tok/sec: 156,852.96\n",
      "60 | lr: 6.00e-04 | loss: 4.8455 | norm: 0.00 | time: 210.98ms | tokens processed: 1,998,848 | tok/sec: 155,315.28\n",
      "61 | lr: 6.00e-04 | loss: 4.6936 | norm: 0.00 | time: 209.52ms | tokens processed: 2,031,616 | tok/sec: 156,396.38\n",
      "62 | lr: 6.00e-04 | loss: 4.8057 | norm: 0.00 | time: 209.76ms | tokens processed: 2,064,384 | tok/sec: 156,215.07\n",
      "63 | lr: 6.00e-04 | loss: 4.9154 | norm: 0.00 | time: 208.21ms | tokens processed: 2,097,152 | tok/sec: 157,377.23\n",
      "64 | lr: 6.00e-04 | loss: 4.8322 | norm: 0.00 | time: 209.71ms | tokens processed: 2,129,920 | tok/sec: 156,251.12\n",
      "65 | lr: 6.00e-04 | loss: 4.8366 | norm: 0.00 | time: 210.17ms | tokens processed: 2,162,688 | tok/sec: 155,909.56\n",
      "66 | lr: 6.00e-04 | loss: 4.7595 | norm: 0.00 | time: 208.54ms | tokens processed: 2,195,456 | tok/sec: 157,133.25\n",
      "67 | lr: 6.00e-04 | loss: 4.7575 | norm: 0.00 | time: 210.19ms | tokens processed: 2,228,224 | tok/sec: 155,898.24\n",
      "68 | lr: 6.00e-04 | loss: 4.8218 | norm: 0.00 | time: 209.22ms | tokens processed: 2,260,992 | tok/sec: 156,621.66\n",
      "69 | lr: 6.00e-04 | loss: 4.7202 | norm: 0.00 | time: 209.10ms | tokens processed: 2,293,760 | tok/sec: 156,707.20\n",
      "70 | lr: 6.00e-04 | loss: 4.5706 | norm: 0.00 | time: 209.59ms | tokens processed: 2,326,528 | tok/sec: 156,344.26\n",
      "71 | lr: 6.00e-04 | loss: 4.7013 | norm: 0.00 | time: 209.04ms | tokens processed: 2,359,296 | tok/sec: 156,757.78\n",
      "72 | lr: 6.00e-04 | loss: 4.8010 | norm: 0.00 | time: 210.11ms | tokens processed: 2,392,064 | tok/sec: 155,957.86\n",
      "73 | lr: 6.00e-04 | loss: 4.7244 | norm: 0.00 | time: 209.37ms | tokens processed: 2,424,832 | tok/sec: 156,507.87\n",
      "74 | lr: 6.00e-04 | loss: 4.6833 | norm: 0.00 | time: 208.78ms | tokens processed: 2,457,600 | tok/sec: 156,948.43\n",
      "75 | lr: 6.00e-04 | loss: 4.6614 | norm: 0.00 | time: 209.41ms | tokens processed: 2,490,368 | tok/sec: 156,477.94\n",
      "76 | lr: 6.00e-04 | loss: 4.6597 | norm: 0.00 | time: 208.58ms | tokens processed: 2,523,136 | tok/sec: 157,101.46\n",
      "77 | lr: 6.00e-04 | loss: 4.7149 | norm: 0.00 | time: 209.65ms | tokens processed: 2,555,904 | tok/sec: 156,298.03\n",
      "78 | lr: 6.00e-04 | loss: 4.6209 | norm: 0.00 | time: 209.50ms | tokens processed: 2,588,672 | tok/sec: 156,407.06\n",
      "79 | lr: 6.00e-04 | loss: 4.4140 | norm: 0.00 | time: 210.17ms | tokens processed: 2,621,440 | tok/sec: 155,910.80\n",
      "80 | lr: 6.00e-04 | loss: 4.5911 | norm: 0.00 | time: 210.85ms | tokens processed: 2,654,208 | tok/sec: 155,410.64\n",
      "81 | lr: 6.00e-04 | loss: 4.7154 | norm: 0.00 | time: 209.18ms | tokens processed: 2,686,976 | tok/sec: 156,650.58\n",
      "82 | lr: 6.00e-04 | loss: 4.5990 | norm: 0.00 | time: 209.72ms | tokens processed: 2,719,744 | tok/sec: 156,245.79\n",
      "83 | lr: 6.00e-04 | loss: 4.5751 | norm: 0.00 | time: 209.91ms | tokens processed: 2,752,512 | tok/sec: 156,101.87\n",
      "84 | lr: 6.00e-04 | loss: 4.5661 | norm: 0.00 | time: 212.00ms | tokens processed: 2,785,280 | tok/sec: 154,563.86\n",
      "85 | lr: 6.00e-04 | loss: 4.5664 | norm: 0.00 | time: 210.84ms | tokens processed: 2,818,048 | tok/sec: 155,415.91\n",
      "86 | lr: 6.00e-04 | loss: 4.6060 | norm: 0.00 | time: 209.81ms | tokens processed: 2,850,816 | tok/sec: 156,176.55\n",
      "87 | lr: 6.00e-04 | loss: 4.5251 | norm: 0.00 | time: 716.34ms | tokens processed: 2,883,584 | tok/sec: 45,743.35\n",
      "88 | lr: 6.00e-04 | loss: 4.3120 | norm: 0.00 | time: 209.42ms | tokens processed: 2,916,352 | tok/sec: 156,471.70\n",
      "89 | lr: 6.00e-04 | loss: 4.5069 | norm: 0.00 | time: 211.38ms | tokens processed: 2,949,120 | tok/sec: 155,017.29\n",
      "90 | lr: 6.00e-04 | loss: 4.6339 | norm: 0.00 | time: 208.58ms | tokens processed: 2,981,888 | tok/sec: 157,097.87\n",
      "91 | lr: 6.00e-04 | loss: 4.4884 | norm: 0.00 | time: 210.97ms | tokens processed: 3,014,656 | tok/sec: 155,323.88\n",
      "92 | lr: 6.00e-04 | loss: 4.4728 | norm: 0.00 | time: 210.38ms | tokens processed: 3,047,424 | tok/sec: 155,759.37\n",
      "93 | lr: 6.00e-04 | loss: 4.4838 | norm: 0.00 | time: 208.68ms | tokens processed: 3,080,192 | tok/sec: 157,024.10\n",
      "94 | lr: 6.00e-04 | loss: 4.4921 | norm: 0.00 | time: 209.57ms | tokens processed: 3,112,960 | tok/sec: 156,357.06\n",
      "95 | lr: 6.00e-04 | loss: 4.5208 | norm: 0.00 | time: 208.53ms | tokens processed: 3,145,728 | tok/sec: 157,138.82\n",
      "96 | lr: 6.00e-04 | loss: 4.4276 | norm: 0.00 | time: 209.55ms | tokens processed: 3,178,496 | tok/sec: 156,369.69\n",
      "97 | lr: 6.00e-04 | loss: 4.2240 | norm: 0.00 | time: 209.27ms | tokens processed: 3,211,264 | tok/sec: 156,582.76\n",
      "98 | lr: 6.00e-04 | loss: 4.4354 | norm: 0.00 | time: 208.79ms | tokens processed: 3,244,032 | tok/sec: 156,941.26\n",
      "99 | lr: 6.00e-04 | loss: 4.5646 | norm: 0.00 | time: 209.73ms | tokens processed: 3,276,800 | tok/sec: 156,239.75\n",
      "100 | lr: 6.00e-04 | loss: 4.4094 | norm: 0.00 | time: 209.34ms | tokens processed: 3,309,568 | tok/sec: 156,529.44\n",
      "101 | lr: 6.00e-04 | loss: 4.3973 | norm: 0.00 | time: 209.19ms | tokens processed: 3,342,336 | tok/sec: 156,643.97\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "> The clever jackal should not go the card in peace upon yet that are it:ANT\n",
      "> The clever jackal at its house of home a well with its crow of course through into hearing my\n",
      "> The clever jackal came into some things's saying ichers.\" Or all — but any-\n",
      "> The clever jackal toldlains through it that what must take our life of their kind; When\n",
      "> The clever jackal to said when when she fell \"When a frog, since the owl told\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "valid loss: 5.2151\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "102 | lr: 6.00e-04 | loss: 4.3983 | norm: 0.00 | time: 211.11ms | tokens processed: 3,375,104 | tok/sec: 155,218.10\n",
      "103 | lr: 6.00e-04 | loss: 4.4130 | norm: 0.00 | time: 211.26ms | tokens processed: 3,407,872 | tok/sec: 155,109.32\n",
      "104 | lr: 6.00e-04 | loss: 4.4616 | norm: 0.00 | time: 208.79ms | tokens processed: 3,440,640 | tok/sec: 156,941.98\n",
      "105 | lr: 6.00e-04 | loss: 4.3365 | norm: 0.00 | time: 211.01ms | tokens processed: 3,473,408 | tok/sec: 155,292.81\n",
      "106 | lr: 6.00e-04 | loss: 4.1515 | norm: 0.00 | time: 209.55ms | tokens processed: 3,506,176 | tok/sec: 156,373.07\n",
      "107 | lr: 6.00e-04 | loss: 4.3334 | norm: 0.00 | time: 209.02ms | tokens processed: 3,538,944 | tok/sec: 156,766.90\n",
      "108 | lr: 6.00e-04 | loss: 4.4884 | norm: 0.00 | time: 210.56ms | tokens processed: 3,571,712 | tok/sec: 155,625.33\n",
      "109 | lr: 6.00e-04 | loss: 4.3356 | norm: 0.00 | time: 209.14ms | tokens processed: 3,604,480 | tok/sec: 156,678.08\n",
      "110 | lr: 6.00e-04 | loss: 4.3315 | norm: 0.00 | time: 210.70ms | tokens processed: 3,637,248 | tok/sec: 155,518.61\n",
      "111 | lr: 6.00e-04 | loss: 4.3359 | norm: 0.00 | time: 209.44ms | tokens processed: 3,670,016 | tok/sec: 156,457.99\n",
      "112 | lr: 6.00e-04 | loss: 4.3470 | norm: 0.00 | time: 209.21ms | tokens processed: 3,702,784 | tok/sec: 156,630.59\n",
      "113 | lr: 6.00e-04 | loss: 4.3545 | norm: 0.00 | time: 212.02ms | tokens processed: 3,735,552 | tok/sec: 154,552.91\n",
      "114 | lr: 6.00e-04 | loss: 4.2405 | norm: 0.00 | time: 209.51ms | tokens processed: 3,768,320 | tok/sec: 156,405.46\n",
      "115 | lr: 6.00e-04 | loss: 4.1204 | norm: 0.00 | time: 211.33ms | tokens processed: 3,801,088 | tok/sec: 155,056.29\n",
      "116 | lr: 6.00e-04 | loss: 4.2604 | norm: 0.00 | time: 208.83ms | tokens processed: 3,833,856 | tok/sec: 156,911.16\n",
      "117 | lr: 6.00e-04 | loss: 4.4191 | norm: 0.00 | time: 211.22ms | tokens processed: 3,866,624 | tok/sec: 155,135.05\n",
      "118 | lr: 6.00e-04 | loss: 4.2677 | norm: 0.00 | time: 209.08ms | tokens processed: 3,899,392 | tok/sec: 156,723.82\n",
      "119 | lr: 6.00e-04 | loss: 4.2596 | norm: 0.00 | time: 208.61ms | tokens processed: 3,932,160 | tok/sec: 157,080.81\n",
      "120 | lr: 6.00e-04 | loss: 4.2432 | norm: 0.00 | time: 209.48ms | tokens processed: 3,964,928 | tok/sec: 156,428.07\n",
      "121 | lr: 6.00e-04 | loss: 4.2716 | norm: 0.00 | time: 209.39ms | tokens processed: 3,997,696 | tok/sec: 156,493.97\n",
      "122 | lr: 6.00e-04 | loss: 4.2919 | norm: 0.00 | time: 210.74ms | tokens processed: 4,030,464 | tok/sec: 155,487.82\n",
      "123 | lr: 6.00e-04 | loss: 4.1442 | norm: 0.00 | time: 209.60ms | tokens processed: 4,063,232 | tok/sec: 156,332.70\n",
      "124 | lr: 6.00e-04 | loss: 4.0682 | norm: 0.00 | time: 209.61ms | tokens processed: 4,096,000 | tok/sec: 156,327.01\n",
      "125 | lr: 6.00e-04 | loss: 4.1861 | norm: 0.00 | time: 209.39ms | tokens processed: 4,128,768 | tok/sec: 156,489.52\n",
      "126 | lr: 6.00e-04 | loss: 4.3217 | norm: 0.00 | time: 209.30ms | tokens processed: 4,161,536 | tok/sec: 156,563.50\n",
      "127 | lr: 6.00e-04 | loss: 4.1839 | norm: 0.00 | time: 210.03ms | tokens processed: 4,194,304 | tok/sec: 156,014.86\n",
      "128 | lr: 6.00e-04 | loss: 4.1958 | norm: 0.00 | time: 209.70ms | tokens processed: 4,227,072 | tok/sec: 156,264.98\n",
      "129 | lr: 6.00e-04 | loss: 4.1674 | norm: 0.00 | time: 210.01ms | tokens processed: 4,259,840 | tok/sec: 156,027.44\n",
      "130 | lr: 6.00e-04 | loss: 4.2048 | norm: 0.00 | time: 209.37ms | tokens processed: 4,292,608 | tok/sec: 156,508.59\n",
      "131 | lr: 6.00e-04 | loss: 4.2275 | norm: 0.00 | time: 211.79ms | tokens processed: 4,325,376 | tok/sec: 154,719.76\n",
      "132 | lr: 6.00e-04 | loss: 4.0505 | norm: 0.00 | time: 209.11ms | tokens processed: 4,358,144 | tok/sec: 156,702.56\n",
      "133 | lr: 6.00e-04 | loss: 4.0335 | norm: 0.00 | time: 209.92ms | tokens processed: 4,390,912 | tok/sec: 156,095.84\n",
      "134 | lr: 6.00e-04 | loss: 4.1162 | norm: 0.00 | time: 210.29ms | tokens processed: 4,423,680 | tok/sec: 155,822.24\n",
      "135 | lr: 6.00e-04 | loss: 4.2420 | norm: 0.00 | time: 208.88ms | tokens processed: 4,456,448 | tok/sec: 156,873.19\n",
      "136 | lr: 6.00e-04 | loss: 4.1358 | norm: 0.00 | time: 210.26ms | tokens processed: 4,489,216 | tok/sec: 155,848.39\n",
      "137 | lr: 6.00e-04 | loss: 4.1520 | norm: 0.00 | time: 210.19ms | tokens processed: 4,521,984 | tok/sec: 155,894.00\n",
      "138 | lr: 6.00e-04 | loss: 4.1087 | norm: 0.00 | time: 211.65ms | tokens processed: 4,554,752 | tok/sec: 154,821.54\n",
      "139 | lr: 6.00e-04 | loss: 4.1545 | norm: 0.00 | time: 210.28ms | tokens processed: 4,587,520 | tok/sec: 155,828.77\n",
      "140 | lr: 6.00e-04 | loss: 4.1363 | norm: 0.00 | time: 212.34ms | tokens processed: 4,620,288 | tok/sec: 154,321.23\n",
      "141 | lr: 6.00e-04 | loss: 3.9965 | norm: 0.00 | time: 210.76ms | tokens processed: 4,653,056 | tok/sec: 155,477.10\n",
      "142 | lr: 6.00e-04 | loss: 3.9818 | norm: 0.00 | time: 210.19ms | tokens processed: 4,685,824 | tok/sec: 155,895.06\n",
      "143 | lr: 6.00e-04 | loss: 4.0475 | norm: 0.00 | time: 210.04ms | tokens processed: 4,718,592 | tok/sec: 156,009.73\n",
      "144 | lr: 6.00e-04 | loss: 4.1780 | norm: 0.00 | time: 209.41ms | tokens processed: 4,751,360 | tok/sec: 156,475.26\n",
      "145 | lr: 6.00e-04 | loss: 4.0782 | norm: 0.00 | time: 209.45ms | tokens processed: 4,784,128 | tok/sec: 156,446.05\n",
      "146 | lr: 6.00e-04 | loss: 4.0945 | norm: 0.00 | time: 211.69ms | tokens processed: 4,816,896 | tok/sec: 154,789.81\n",
      "147 | lr: 6.00e-04 | loss: 4.0375 | norm: 0.00 | time: 210.42ms | tokens processed: 4,849,664 | tok/sec: 155,724.07\n",
      "148 | lr: 6.00e-04 | loss: 4.0887 | norm: 0.00 | time: 209.98ms | tokens processed: 4,882,432 | tok/sec: 156,055.96\n",
      "149 | lr: 6.00e-04 | loss: 4.0640 | norm: 0.00 | time: 208.61ms | tokens processed: 4,915,200 | tok/sec: 157,074.88\n",
      "150 | lr: 6.00e-04 | loss: 3.9301 | norm: 0.00 | time: 209.32ms | tokens processed: 4,947,968 | tok/sec: 156,546.56\n",
      "151 | lr: 6.00e-04 | loss: 3.9464 | norm: 0.00 | time: 211.77ms | tokens processed: 4,980,736 | tok/sec: 154,734.91\n",
      "152 | lr: 6.00e-04 | loss: 4.0066 | norm: 0.00 | time: 209.11ms | tokens processed: 5,013,504 | tok/sec: 156,699.34\n",
      "153 | lr: 6.00e-04 | loss: 4.1072 | norm: 0.00 | time: 210.77ms | tokens processed: 5,046,272 | tok/sec: 155,465.66\n",
      "154 | lr: 6.00e-04 | loss: 4.0092 | norm: 0.00 | time: 209.63ms | tokens processed: 5,079,040 | tok/sec: 156,315.45\n",
      "155 | lr: 6.00e-04 | loss: 4.0284 | norm: 0.00 | time: 209.63ms | tokens processed: 5,111,808 | tok/sec: 156,313.50\n",
      "156 | lr: 6.00e-04 | loss: 3.9833 | norm: 0.00 | time: 210.37ms | tokens processed: 5,144,576 | tok/sec: 155,763.78\n",
      "157 | lr: 6.00e-04 | loss: 4.0519 | norm: 0.00 | time: 209.05ms | tokens processed: 5,177,344 | tok/sec: 156,748.84\n",
      "158 | lr: 6.00e-04 | loss: 4.0077 | norm: 0.00 | time: 211.16ms | tokens processed: 5,210,112 | tok/sec: 155,182.70\n",
      "159 | lr: 6.00e-04 | loss: 3.8481 | norm: 0.00 | time: 209.65ms | tokens processed: 5,242,880 | tok/sec: 156,297.67\n",
      "160 | lr: 6.00e-04 | loss: 3.8944 | norm: 0.00 | time: 209.91ms | tokens processed: 5,275,648 | tok/sec: 156,106.66\n",
      "161 | lr: 6.00e-04 | loss: 3.9418 | norm: 0.00 | time: 211.18ms | tokens processed: 5,308,416 | tok/sec: 155,162.55\n",
      "162 | lr: 6.00e-04 | loss: 4.0604 | norm: 0.00 | time: 210.70ms | tokens processed: 5,341,184 | tok/sec: 155,521.43\n",
      "163 | lr: 6.00e-04 | loss: 3.9474 | norm: 0.00 | time: 210.75ms | tokens processed: 5,373,952 | tok/sec: 155,480.96\n",
      "164 | lr: 6.00e-04 | loss: 3.9806 | norm: 0.00 | time: 209.29ms | tokens processed: 5,406,720 | tok/sec: 156,564.21\n",
      "165 | lr: 6.00e-04 | loss: 3.9225 | norm: 0.00 | time: 210.76ms | tokens processed: 5,439,488 | tok/sec: 155,474.28\n",
      "166 | lr: 6.00e-04 | loss: 3.9941 | norm: 0.00 | time: 209.03ms | tokens processed: 5,472,256 | tok/sec: 156,758.86\n",
      "167 | lr: 6.00e-04 | loss: 3.9407 | norm: 0.00 | time: 209.63ms | tokens processed: 5,505,024 | tok/sec: 156,311.90\n",
      "168 | lr: 6.00e-04 | loss: 3.7926 | norm: 0.00 | time: 210.12ms | tokens processed: 5,537,792 | tok/sec: 155,946.35\n",
      "169 | lr: 6.00e-04 | loss: 3.8629 | norm: 0.00 | time: 209.53ms | tokens processed: 5,570,560 | tok/sec: 156,388.91\n",
      "170 | lr: 6.00e-04 | loss: 3.9247 | norm: 0.00 | time: 717.01ms | tokens processed: 5,603,328 | tok/sec: 45,700.91\n",
      "171 | lr: 6.00e-04 | loss: 4.0009 | norm: 0.00 | time: 210.36ms | tokens processed: 5,636,096 | tok/sec: 155,772.08\n",
      "172 | lr: 6.00e-04 | loss: 3.8701 | norm: 0.00 | time: 209.37ms | tokens processed: 5,668,864 | tok/sec: 156,509.12\n",
      "173 | lr: 6.00e-04 | loss: 3.9380 | norm: 0.00 | time: 210.20ms | tokens processed: 5,701,632 | tok/sec: 155,887.28\n",
      "174 | lr: 6.00e-04 | loss: 3.8786 | norm: 0.00 | time: 209.10ms | tokens processed: 5,734,400 | tok/sec: 156,706.84\n",
      "175 | lr: 6.00e-04 | loss: 3.9495 | norm: 0.00 | time: 210.43ms | tokens processed: 5,767,168 | tok/sec: 155,718.96\n",
      "176 | lr: 6.00e-04 | loss: 3.8789 | norm: 0.00 | time: 209.04ms | tokens processed: 5,799,936 | tok/sec: 156,756.89\n",
      "177 | lr: 6.00e-04 | loss: 3.7179 | norm: 0.00 | time: 209.46ms | tokens processed: 5,832,704 | tok/sec: 156,439.82\n",
      "178 | lr: 6.00e-04 | loss: 3.8154 | norm: 0.00 | time: 210.91ms | tokens processed: 5,865,472 | tok/sec: 155,362.15\n",
      "179 | lr: 6.00e-04 | loss: 3.8867 | norm: 0.00 | time: 208.85ms | tokens processed: 5,898,240 | tok/sec: 156,899.87\n",
      "180 | lr: 6.00e-04 | loss: 3.9470 | norm: 0.00 | time: 210.20ms | tokens processed: 5,931,008 | tok/sec: 155,889.05\n",
      "181 | lr: 6.00e-04 | loss: 3.8283 | norm: 0.00 | time: 209.73ms | tokens processed: 5,963,776 | tok/sec: 156,241.53\n",
      "182 | lr: 6.00e-04 | loss: 3.8799 | norm: 0.00 | time: 209.93ms | tokens processed: 5,996,544 | tok/sec: 156,086.62\n",
      "183 | lr: 6.00e-04 | loss: 3.8362 | norm: 0.00 | time: 210.81ms | tokens processed: 6,029,312 | tok/sec: 155,437.88\n",
      "184 | lr: 6.00e-04 | loss: 3.9114 | norm: 0.00 | time: 209.64ms | tokens processed: 6,062,080 | tok/sec: 156,307.27\n",
      "185 | lr: 6.00e-04 | loss: 3.8298 | norm: 0.00 | time: 210.20ms | tokens processed: 6,094,848 | tok/sec: 155,887.63\n",
      "186 | lr: 6.00e-04 | loss: 3.6464 | norm: 0.00 | time: 209.35ms | tokens processed: 6,127,616 | tok/sec: 156,523.20\n",
      "187 | lr: 6.00e-04 | loss: 3.7786 | norm: 0.00 | time: 211.05ms | tokens processed: 6,160,384 | tok/sec: 155,263.34\n",
      "188 | lr: 6.00e-04 | loss: 3.8327 | norm: 0.00 | time: 209.77ms | tokens processed: 6,193,152 | tok/sec: 156,210.27\n",
      "189 | lr: 6.00e-04 | loss: 3.9007 | norm: 0.00 | time: 209.81ms | tokens processed: 6,225,920 | tok/sec: 156,177.08\n",
      "190 | lr: 6.00e-04 | loss: 3.7835 | norm: 0.00 | time: 211.63ms | tokens processed: 6,258,688 | tok/sec: 154,838.46\n",
      "191 | lr: 6.00e-04 | loss: 3.8320 | norm: 0.00 | time: 209.03ms | tokens processed: 6,291,456 | tok/sec: 156,765.83\n",
      "192 | lr: 6.00e-04 | loss: 3.7838 | norm: 0.00 | time: 210.67ms | tokens processed: 6,324,224 | tok/sec: 155,541.49\n",
      "193 | lr: 6.00e-04 | loss: 3.8586 | norm: 0.00 | time: 210.10ms | tokens processed: 6,356,992 | tok/sec: 155,961.75\n",
      "194 | lr: 6.00e-04 | loss: 3.7696 | norm: 0.00 | time: 210.95ms | tokens processed: 6,389,760 | tok/sec: 155,332.13\n",
      "195 | lr: 6.00e-04 | loss: 3.6133 | norm: 0.00 | time: 210.00ms | tokens processed: 6,422,528 | tok/sec: 156,040.37\n",
      "196 | lr: 6.00e-04 | loss: 3.7524 | norm: 0.00 | time: 209.73ms | tokens processed: 6,455,296 | tok/sec: 156,241.35\n",
      "197 | lr: 6.00e-04 | loss: 3.7924 | norm: 0.00 | time: 209.75ms | tokens processed: 6,488,064 | tok/sec: 156,225.54\n",
      "198 | lr: 6.00e-04 | loss: 3.8565 | norm: 0.00 | time: 209.60ms | tokens processed: 6,520,832 | tok/sec: 156,334.30\n",
      "199 | lr: 6.00e-04 | loss: 3.7432 | norm: 0.00 | time: 210.39ms | tokens processed: 6,553,600 | tok/sec: 155,745.78\n",
      "200 | lr: 6.00e-04 | loss: 3.7844 | norm: 0.00 | time: 210.25ms | tokens processed: 6,586,368 | tok/sec: 155,851.22\n",
      "201 | lr: 6.00e-04 | loss: 3.7464 | norm: 0.00 | time: 208.98ms | tokens processed: 6,619,136 | tok/sec: 156,800.70\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "> The clever jackal cried: (brured a black points sate?\" After in mind without\n",
      "> The clever jackal at their wings' cash were found no to go? Go be food by nothing\n",
      "> The clever jackal.\" Then as my mind be killed the house that dreadful in spirit went it:\n",
      "> The clever jackal laughed because today like this bird beganly dwellingsure as she. Besides: is\n",
      "> The clever jackal asked when Rusty urnions. His feet for being the ocean, was your\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "valid loss: 5.2898\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "202 | lr: 6.00e-04 | loss: 3.8270 | norm: 0.00 | time: 212.72ms | tokens processed: 6,651,904 | tok/sec: 154,042.76\n",
      "203 | lr: 6.00e-04 | loss: 3.7360 | norm: 0.00 | time: 210.44ms | tokens processed: 6,684,672 | tok/sec: 155,714.19\n",
      "204 | lr: 6.00e-04 | loss: 3.5689 | norm: 0.00 | time: 210.31ms | tokens processed: 6,717,440 | tok/sec: 155,811.46\n",
      "205 | lr: 6.00e-04 | loss: 3.7155 | norm: 0.00 | time: 210.39ms | tokens processed: 6,750,208 | tok/sec: 155,747.54\n",
      "206 | lr: 6.00e-04 | loss: 3.7569 | norm: 0.00 | time: 210.81ms | tokens processed: 6,782,976 | tok/sec: 155,435.07\n",
      "207 | lr: 6.00e-04 | loss: 3.8037 | norm: 0.00 | time: 209.97ms | tokens processed: 6,815,744 | tok/sec: 156,059.33\n",
      "208 | lr: 6.00e-04 | loss: 3.6922 | norm: 0.00 | time: 210.87ms | tokens processed: 6,848,512 | tok/sec: 155,395.88\n",
      "209 | lr: 6.00e-04 | loss: 3.7386 | norm: 0.00 | time: 210.04ms | tokens processed: 6,881,280 | tok/sec: 156,007.78\n",
      "210 | lr: 6.00e-04 | loss: 3.6950 | norm: 0.00 | time: 209.61ms | tokens processed: 6,914,048 | tok/sec: 156,330.39\n",
      "211 | lr: 6.00e-04 | loss: 3.7911 | norm: 0.00 | time: 210.34ms | tokens processed: 6,946,816 | tok/sec: 155,788.15\n",
      "212 | lr: 6.00e-04 | loss: 3.6808 | norm: 0.00 | time: 210.33ms | tokens processed: 6,979,584 | tok/sec: 155,793.45\n",
      "213 | lr: 6.00e-04 | loss: 3.5194 | norm: 0.00 | time: 211.01ms | tokens processed: 7,012,352 | tok/sec: 155,290.71\n",
      "214 | lr: 6.00e-04 | loss: 3.6593 | norm: 0.00 | time: 211.16ms | tokens processed: 7,045,120 | tok/sec: 155,181.65\n",
      "215 | lr: 6.00e-04 | loss: 3.7111 | norm: 0.00 | time: 213.79ms | tokens processed: 7,077,888 | tok/sec: 153,270.41\n",
      "216 | lr: 6.00e-04 | loss: 3.7674 | norm: 0.00 | time: 210.64ms | tokens processed: 7,110,656 | tok/sec: 155,563.68\n",
      "217 | lr: 6.00e-04 | loss: 3.6580 | norm: 0.00 | time: 211.44ms | tokens processed: 7,143,424 | tok/sec: 154,977.44\n",
      "218 | lr: 6.00e-04 | loss: 3.6907 | norm: 0.00 | time: 214.08ms | tokens processed: 7,176,192 | tok/sec: 153,061.99\n",
      "219 | lr: 6.00e-04 | loss: 3.6598 | norm: 0.00 | time: 210.99ms | tokens processed: 7,208,960 | tok/sec: 155,305.27\n",
      "220 | lr: 6.00e-04 | loss: 3.7285 | norm: 0.00 | time: 211.39ms | tokens processed: 7,241,728 | tok/sec: 155,014.15\n",
      "221 | lr: 6.00e-04 | loss: 3.6345 | norm: 0.00 | time: 211.53ms | tokens processed: 7,274,496 | tok/sec: 154,906.17\n",
      "222 | lr: 6.00e-04 | loss: 3.4817 | norm: 0.00 | time: 210.25ms | tokens processed: 7,307,264 | tok/sec: 155,852.28\n",
      "223 | lr: 6.00e-04 | loss: 3.6243 | norm: 0.00 | time: 210.88ms | tokens processed: 7,340,032 | tok/sec: 155,387.80\n",
      "224 | lr: 6.00e-04 | loss: 3.6681 | norm: 0.00 | time: 210.14ms | tokens processed: 7,372,800 | tok/sec: 155,930.61\n",
      "225 | lr: 6.00e-04 | loss: 3.7269 | norm: 0.00 | time: 210.86ms | tokens processed: 7,405,568 | tok/sec: 155,403.08\n",
      "226 | lr: 6.00e-04 | loss: 3.6237 | norm: 0.00 | time: 210.65ms | tokens processed: 7,438,336 | tok/sec: 155,557.87\n",
      "227 | lr: 6.00e-04 | loss: 3.6416 | norm: 0.00 | time: 209.65ms | tokens processed: 7,471,104 | tok/sec: 156,298.21\n",
      "228 | lr: 6.00e-04 | loss: 3.6274 | norm: 0.00 | time: 210.15ms | tokens processed: 7,503,872 | tok/sec: 155,929.37\n",
      "229 | lr: 6.00e-04 | loss: 3.7008 | norm: 0.00 | time: 210.07ms | tokens processed: 7,536,640 | tok/sec: 155,986.53\n",
      "230 | lr: 6.00e-04 | loss: 3.6043 | norm: 0.00 | time: 210.49ms | tokens processed: 7,569,408 | tok/sec: 155,673.10\n",
      "231 | lr: 6.00e-04 | loss: 3.4414 | norm: 0.00 | time: 210.25ms | tokens processed: 7,602,176 | tok/sec: 155,852.81\n",
      "232 | lr: 6.00e-04 | loss: 3.5859 | norm: 0.00 | time: 211.58ms | tokens processed: 7,634,944 | tok/sec: 154,869.69\n",
      "233 | lr: 6.00e-04 | loss: 3.6574 | norm: 0.00 | time: 210.49ms | tokens processed: 7,667,712 | tok/sec: 155,672.57\n",
      "234 | lr: 6.00e-04 | loss: 3.6697 | norm: 0.00 | time: 209.47ms | tokens processed: 7,700,480 | tok/sec: 156,432.70\n",
      "235 | lr: 6.00e-04 | loss: 3.6010 | norm: 0.00 | time: 210.40ms | tokens processed: 7,733,248 | tok/sec: 155,743.66\n",
      "236 | lr: 6.00e-04 | loss: 3.6009 | norm: 0.00 | time: 209.77ms | tokens processed: 7,766,016 | tok/sec: 156,208.85\n",
      "237 | lr: 6.00e-04 | loss: 3.5968 | norm: 0.00 | time: 211.37ms | tokens processed: 7,798,784 | tok/sec: 155,023.41\n",
      "238 | lr: 6.00e-04 | loss: 3.6601 | norm: 0.00 | time: 210.94ms | tokens processed: 7,831,552 | tok/sec: 155,346.00\n",
      "239 | lr: 6.00e-04 | loss: 3.5597 | norm: 0.00 | time: 214.27ms | tokens processed: 7,864,320 | tok/sec: 152,925.40\n",
      "240 | lr: 6.00e-04 | loss: 3.4114 | norm: 0.00 | time: 210.39ms | tokens processed: 7,897,088 | tok/sec: 155,747.54\n",
      "241 | lr: 6.00e-04 | loss: 3.5474 | norm: 0.00 | time: 210.72ms | tokens processed: 7,929,856 | tok/sec: 155,507.00\n",
      "242 | lr: 6.00e-04 | loss: 3.6256 | norm: 0.00 | time: 211.08ms | tokens processed: 7,962,624 | tok/sec: 155,240.19\n",
      "243 | lr: 6.00e-04 | loss: 3.6204 | norm: 0.00 | time: 210.46ms | tokens processed: 7,995,392 | tok/sec: 155,695.32\n",
      "244 | lr: 6.00e-04 | loss: 3.5572 | norm: 0.00 | time: 211.56ms | tokens processed: 8,028,160 | tok/sec: 154,887.67\n",
      "245 | lr: 6.00e-04 | loss: 3.5572 | norm: 0.00 | time: 210.17ms | tokens processed: 8,060,928 | tok/sec: 155,915.04\n",
      "246 | lr: 6.00e-04 | loss: 3.5625 | norm: 0.00 | time: 210.42ms | tokens processed: 8,093,696 | tok/sec: 155,727.60\n",
      "247 | lr: 6.00e-04 | loss: 3.6089 | norm: 0.00 | time: 209.57ms | tokens processed: 8,126,464 | tok/sec: 156,355.11\n",
      "248 | lr: 6.00e-04 | loss: 3.5407 | norm: 0.00 | time: 210.24ms | tokens processed: 8,159,232 | tok/sec: 155,856.69\n",
      "249 | lr: 6.00e-04 | loss: 3.3821 | norm: 0.00 | time: 211.00ms | tokens processed: 8,192,000 | tok/sec: 155,295.62\n",
      "250 | lr: 6.00e-04 | loss: 3.5212 | norm: 0.00 | time: 210.59ms | tokens processed: 8,224,768 | tok/sec: 155,598.73\n",
      "251 | lr: 6.00e-04 | loss: 3.5872 | norm: 0.00 | time: 210.12ms | tokens processed: 8,257,536 | tok/sec: 155,948.66\n",
      "252 | lr: 6.00e-04 | loss: 3.5894 | norm: 0.00 | time: 209.50ms | tokens processed: 8,290,304 | tok/sec: 156,408.67\n",
      "253 | lr: 6.00e-04 | loss: 3.5225 | norm: 0.00 | time: 210.06ms | tokens processed: 8,323,072 | tok/sec: 155,997.16\n",
      "254 | lr: 6.00e-04 | loss: 3.5356 | norm: 0.00 | time: 716.28ms | tokens processed: 8,355,840 | tok/sec: 45,747.69\n",
      "255 | lr: 6.00e-04 | loss: 3.5431 | norm: 0.00 | time: 210.62ms | tokens processed: 8,388,608 | tok/sec: 155,578.12\n",
      "256 | lr: 6.00e-04 | loss: 3.5645 | norm: 0.00 | time: 209.71ms | tokens processed: 8,421,376 | tok/sec: 156,257.34\n",
      "257 | lr: 6.00e-04 | loss: 3.5033 | norm: 0.00 | time: 210.18ms | tokens processed: 8,454,144 | tok/sec: 155,902.13\n",
      "258 | lr: 6.00e-04 | loss: 3.3631 | norm: 0.00 | time: 210.33ms | tokens processed: 8,486,912 | tok/sec: 155,793.62\n",
      "259 | lr: 6.00e-04 | loss: 3.4929 | norm: 0.00 | time: 210.93ms | tokens processed: 8,519,680 | tok/sec: 155,347.75\n",
      "260 | lr: 6.00e-04 | loss: 3.5742 | norm: 0.00 | time: 215.45ms | tokens processed: 8,552,448 | tok/sec: 152,089.76\n",
      "261 | lr: 6.00e-04 | loss: 3.5496 | norm: 0.00 | time: 212.41ms | tokens processed: 8,585,216 | tok/sec: 154,269.96\n",
      "262 | lr: 6.00e-04 | loss: 3.4955 | norm: 0.00 | time: 210.82ms | tokens processed: 8,617,984 | tok/sec: 155,434.19\n",
      "263 | lr: 6.00e-04 | loss: 3.4999 | norm: 0.00 | time: 210.27ms | tokens processed: 8,650,752 | tok/sec: 155,841.32\n",
      "264 | lr: 6.00e-04 | loss: 3.5096 | norm: 0.00 | time: 210.98ms | tokens processed: 8,683,520 | tok/sec: 155,311.06\n",
      "265 | lr: 6.00e-04 | loss: 3.5439 | norm: 0.00 | time: 210.10ms | tokens processed: 8,716,288 | tok/sec: 155,962.46\n",
      "Received KeyboardInterrupt. Exiting...\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "> The clever jackal who plunels in life were to kill friends as water I do today\n",
      "> The clever jackal whose skeletons I shall beg as his hole near spacious THEay discover one earns each\n",
      "> The clever jackal did he cried he has walked; for ill was terience enough after that \n",
      "> The clever jackal then make sugar use her in life,\" I give that I can win the ocean\n",
      "> The clever jackal ate fire have you will follow him all anxious felles may i fruits\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "valid loss: 5.4142\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "CPU times: user 52.1 s, sys: 4.91 s, total: 57 s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from time import time\n",
    "from IPython.display import clear_output\n",
    "from functools import partial\n",
    "\n",
    "evaluate(m)\n",
    "m.train()\n",
    "\n",
    "try:\n",
    "  for step in range(max_steps):\n",
    "   \n",
    "    #batches, targets = [], []\n",
    "    #for i in range(grad_accumulation_steps):\n",
    "    start = time()\n",
    "    batch, target = train_dl()\n",
    "      #batches.append(batch)\n",
    "      #targets.append(target)\n",
    "\n",
    "    avg_loss, avg_grads = parallel_train_step(m, optimizer, batch, target)\n",
    "    avg_loss.block_until_ready()\n",
    "    # compute stats\n",
    "    loss = avg_loss[0]\n",
    "    lr = warmup_with_cosine_decay_schedule(step)\n",
    "    norm = 0 # norm[0]|\n",
    "    iter_time = time() - start\n",
    "    sub_step_time = iter_time / grad_accumulation_steps\n",
    "    tokens_per_sec = num_devices * mB * T * grad_accumulation_steps / iter_time\n",
    "    tokens_processed = (step+1) * num_devices * grad_accumulation_steps * mB * T\n",
    "\n",
    "    if step % print_interval == 0:\n",
    "        print(f\"{step} | lr: {lr:0.2e} | loss: {loss:0.4f} | norm: {norm:0.2f} | time: {iter_time*1000:0.2f}ms | tokens processed: {tokens_processed:,} | tok/sec: {tokens_per_sec:,.2f}\")\n",
    "    if step % eval_interval == 1:\n",
    "      evaluate(m)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Received KeyboardInterrupt. Exiting...\")\n",
    "evaluate(m)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.13 (jaxpt)",
   "language": "python",
   "name": "jaxpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
