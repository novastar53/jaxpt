{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXU2qDN8nMgg"
   },
   "source": [
    "# Let's Train GPT-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LNLo9jfLn8bg",
    "outputId": "9eb6bc93-7a88-4a8c-8a8b-5c7593bbe32b"
   },
   "outputs": [],
   "source": [
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if is_colab():\n",
    "    !git clone https://github.com/novastar53/jaxpt\n",
    "    !cd jaxpt && git checkout dev && git pull\n",
    "    !pip install tiktoken --quiet\n",
    "    !pip uninstall -y tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQl8dEgLnMgh",
    "outputId": "f646af7e-38c4-40ed-8b18-e423e0584a60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vikram/dev/jaxpt/src\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "if is_colab():\n",
    "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"src\" )\n",
    "else:\n",
    "    jaxpt_dir = str(Path().absolute().parent / \"src\" )\n",
    "\n",
    "sys.path.append(jaxpt_dir)\n",
    "print(jaxpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7N2-jnzonMgh"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax import nnx\n",
    "import tiktoken\n",
    "\n",
    "from jaxpt.dataloaders import DataLoader\n",
    "from jaxpt.models import GPT2, GPTConfig, save_checkpoint as save_gpt2_chkpt , from_checkpoint as load_gpt2_chkpt\n",
    "from jaxpt.train import train_step, parallel_train_step, accum_train_step, loss_fn, compute_global_norm\n",
    "from jaxpt.infer import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04pFw2g58HJl"
   },
   "source": [
    "### Configure compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJo6Xji39g54",
    "outputId": "66de6e21-02e6-4a65-fae3-48bd4a668c52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.5.2\n",
      "Available devices: 1\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['XLA_FLAGS'] = (\n",
    "    '--xla_gpu_triton_gemm_any=True '\n",
    "    '--xla_gpu_enable_latency_hiding_scheduler=true '\n",
    ")\n",
    "\n",
    "os.environ.update({\n",
    "  \"NCCL_LL128_BUFFSIZE\": \"-2\",\n",
    "  \"NCCL_LL_BUFFSIZE\": \"-2\",\n",
    "   \"NCCL_PROTO\": \"SIMPLE,LL,LL128\",\n",
    " })\n",
    "\n",
    "\n",
    "# Hardware setup\n",
    "print(\"JAX version:\", jax.__version__)\n",
    "devices = jax.devices()\n",
    "num_devices = len(devices)\n",
    "print(\"Available devices:\", num_devices)\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", \"gpu\") # Make sure we're using the GPU\n",
    "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"bfloat16\") # Set the default precision for matrix multiplication\n",
    "\n",
    "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
    "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
    "\n",
    "def list_tpu_memory():\n",
    "    devices = jax.devices()\n",
    "    for device in devices:\n",
    "        if 'TPU' in str(device.device_kind):\n",
    "            print(f\"Device: {device}, Memory: {device.memory_stats()['bytes_limit']/(1024*1024)},  Used: {device.memory_stats()['bytes_in_use']/(1024*1024)}\")\n",
    "\n",
    "#list_tpu_memory()\n",
    "\n",
    "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
    "\n",
    "A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Makes sure the matmul is fast\n",
    "\n",
    "#%timeit (A@A).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHA3hbjj8HJl"
   },
   "source": [
    "### Configure Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lki2khsFnMgh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_20250311_voabtg\n",
      "Checkpoint directory: /Users/vikram/dev/jaxpt/checkpoints\n",
      "Log directory: /Users/vikram/dev/jaxpt/logs\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import random\n",
    "import string\n",
    "\n",
    "def generate_random_code(length=6):\n",
    "    return ''.join(random.choices(string.ascii_lowercase, k=length))\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "random_code = generate_random_code()\n",
    "run_dir = f\"run_{timestamp}_{random_code}\"\n",
    "print(run_dir)\n",
    "\n",
    "#output_dir = Path(\"/home/ubuntu/gpt2-train\")\n",
    "output_dir = Path().absolute().parent\n",
    "checkpoint_dir =   output_dir / \"checkpoints\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Checkpoint directory: {checkpoint_dir}\")\n",
    "\n",
    "log_dir = output_dir / \"logs\"\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Log directory: {log_dir}\")\n",
    "\n",
    "def save_checkpoint(m, step):\n",
    "  checkpoint_path = checkpoint_dir / run_dir / f\"checkpoint-{step}.pt\"\n",
    "  save_gpt2_chkpt(m, checkpoint_path)\n",
    "\n",
    "def load_checkpoint(run_dir, step):\n",
    "  checkpoint_path = checkpoint_dir / run_dir / f\"checkpoint-{step}.pt\"\n",
    "  m = load_gpt2_chkpt(checkpoint_path, rngs)\n",
    "  return m\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzoCvstr9_WX"
   },
   "source": [
    "### Initialize the GPT-2 model and perform a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "+--------------+---------+--------+------+\n",
    "| Model        | Layers  | Heads  | Embd |\n",
    "+--------------+---------+--------+------+\n",
    "| gpt2-medium  | 24      | 16     | 1024 |\n",
    "| gpt2-large   | 36      | 20     | 1280 |\n",
    "| gpt2-xl      | 48      | 25     | 1600 |\n",
    "+--------------+---------+--------+------+\n",
    "\"\"\"\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "rngs = nnx.Rngs(key)\n",
    "config = GPTConfig(dtype=jnp.float32)\n",
    "m = GPT2(config, rngs)\n",
    "#m = load_checkpoint(\"run_20250310_krcksm\", 199)\n",
    "#graphdef, rngstate, state = nnx.split(m, nnx.RngState, ...)\n",
    "#nnx.display(state)\n",
    "\n",
    "def generate_completions():\n",
    "  m.eval()\n",
    "  num_completions = 5\n",
    "  max_length = 20\n",
    "  generate_completion = partial(generate, m, max_length=max_length)\n",
    "  prefix = \"The clever jackal\"\n",
    "  enc = tiktoken.get_encoding('gpt2')\n",
    "  tokens = enc.encode(prefix)\n",
    "  tokens = jnp.array(tokens, dtype=jnp.int32)\n",
    "  tokens = jnp.expand_dims(tokens, axis=0)\n",
    "  x = jnp.tile(tokens, (num_completions, 1))\n",
    "\n",
    "\n",
    "  x = generate_completion(x=x) # Make sure you can do a forward pass\n",
    "  output = []\n",
    "  for i in range(num_completions):\n",
    "      tokens = x[i, :max_length].tolist()\n",
    "      decoded = enc.decode(tokens)\n",
    "      output.append(decoded)\n",
    "  return output\n",
    "\n",
    "#completions = generate_completions()\n",
    "#for completion in completions:\n",
    "#print(completion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8Dx19aKnMgi",
    "outputId": "c6d6d2ea-13ad-4b6a-fdd8-1980047d06c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight decay param count: 124,318,464\n",
      "tokens/batch: 512\n",
      "block size: 64\n",
      "sub-batch size: 8\n",
      "no. gradient accumulation steps: 1\n",
      "effective batch size per device:  8\n",
      "effective batch size: 8\n",
      "max steps: 286\n"
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "\n",
    "############################\n",
    "# Nvidia A100 (x 8) Config #\n",
    "############################\n",
    "\n",
    "@dataclasses.dataclass()\n",
    "class TrainerConfig:\n",
    "  num_tokens_per_batch: int = 2**19 # 2**19, 0.5 million as per the GPT 3.5 paper\n",
    "  mB: int = 64\n",
    "  T: int = 1024\n",
    "  max_steps: int = 18882 # 1 epoch (all 99 shards of the dataset) \n",
    "  max_lr: float = 6e-4\n",
    "  min_lr: float = max_lr * 0.1\n",
    "  max_grad_norm: float = 1.0  # Clip gradients to this norm\n",
    "  warmup_steps: int = 715\n",
    "  print_interval: int = 10\n",
    "  eval_interval: int = 100\n",
    "  checkpoint_interval: int = 100\n",
    "  grad_accumulation_steps: int = num_tokens_per_batch // (mB * T * num_devices) # Number of steps over which to average the gradient\n",
    "\n",
    "##############\n",
    "# CPU Config #\n",
    "##############\n",
    "\n",
    "#trconf = TrainerConfig()\n",
    "trconf = TrainerConfig(\n",
    "  num_tokens_per_batch=2**9,\n",
    "  mB=8,\n",
    "  T=64,\n",
    "  max_steps=286*2, # 2 epoch(s)\n",
    "  max_lr=6e-4,\n",
    "  min_lr=6e-5,\n",
    "  max_grad_norm=1.0,\n",
    "  warmup_steps=10,\n",
    "  print_interval=1,\n",
    "  eval_interval=10,\n",
    "  checkpoint_interval=30,\n",
    ")\n",
    "trconf.grad_accumulation_steps =  trconf.num_tokens_per_batch // (trconf.mB * trconf.T * num_devices) # Number of steps over which to average the gradient\n",
    "\n",
    "# Set up the optimizer\n",
    "def warmup_with_cosine_decay_schedule(step):\n",
    "\n",
    "    warmup_lr = trconf.max_lr * (step + 1) / trconf.warmup_steps\n",
    "    coeff = 0.5 * (1 + jnp.cos(jnp.pi * (step - trconf.warmup_steps) / (trconf.max_steps - trconf.warmup_steps)))\n",
    "    cosine_lr =  trconf.min_lr + coeff * (trconf.max_lr - trconf.min_lr)\n",
    "\n",
    "    return jnp.where(step < trconf.warmup_steps,\n",
    "                     warmup_lr,\n",
    "                     jnp.where(step < trconf.max_steps, cosine_lr, trconf.min_lr))\n",
    "\n",
    "# Generate a weight decay mask\n",
    "# First split the model into params and variables\n",
    "graphdef, params, variables = nnx.split(m, nnx.Param, nnx.Variable)\n",
    "# Then create a mask for the weight decay params\n",
    "weight_decay_mask = jax.tree_util.tree_map(lambda x: len(x.shape) > 1, params)\n",
    "\n",
    "\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(trconf.max_grad_norm),\n",
    "    optax.adamw(6e-4, b1=0.9, b2=0.95, weight_decay=0.1, mask=weight_decay_mask)\n",
    ")\n",
    "optimizer = nnx.Optimizer(m, tx)\n",
    "\n",
    "# count the number of weight decay params\n",
    "def f(x, y):\n",
    "    if x:\n",
    "        return y.size\n",
    "    return 0\n",
    "\n",
    "weight_decay_params = jax.tree_util.tree_map(f, weight_decay_mask, params)\n",
    "weight_decay_param_count = jax.tree_util.tree_reduce(lambda x, y: x + y, weight_decay_params, 0)\n",
    "\n",
    "\n",
    "print(f\"weight decay param count: {weight_decay_param_count:,}\")\n",
    "print(f\"tokens/batch: {trconf.num_tokens_per_batch:,}\")\n",
    "print(f\"block size: {trconf.T}\")\n",
    "print(f\"sub-batch size: {trconf.mB}\")\n",
    "print(f\"no. gradient accumulation steps: {trconf.grad_accumulation_steps}\")\n",
    "print(f\"effective batch size per device: \", trconf.grad_accumulation_steps * trconf.mB)\n",
    "print(f\"effective batch size: {trconf.grad_accumulation_steps * trconf.mB * num_devices}\")\n",
    "print(f\"max steps: {trconf.max_steps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader and Validation Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_8d4oBtGEwpy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader initialized:\n",
      "------------------------\n",
      "label:          train\n",
      "shards:         1\n",
      "shard size:     146,776\n",
      "batch size:     8\n",
      "block size:     64\n",
      "device rank:    1\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = \"panchatantra-ryder\"\n",
    "\n",
    "if is_colab():\n",
    "    dataset_path = Path().absolute() / \"jaxpt\" / \"src\" / \"jaxpt\" / \"datasets\" / dataset / \"processed\"\n",
    "else:\n",
    "    if dataset == \"fineweb-edu\":\n",
    "      dataset_path = \"/home/ubuntu/gpt2-train/fineweb-edu/processed\"\n",
    "    elif dataset == \"panchatantra-ryder\":\n",
    "      dataset_path = Path().absolute().parent / \"src\"/ \"jaxpt\" / \"datasets\" / dataset / \"processed\"\n",
    "    else:\n",
    "      raise ValueError(f\"Dataset {dataset} not found\")  \n",
    "\n",
    "train_dl = DataLoader(dirpath=dataset_path, batch_size=trconf.mB, block_size=trconf.T, device_rank=num_devices, label=\"train\")\n",
    "eval_dl = DataLoader(dirpath=dataset_path, batch_size=trconf.mB, block_size=trconf.T, device_rank=1, label=\"valid\", quiet=True)\n",
    "\n",
    "def validate(m):\n",
    "  valid_loss = 0.0\n",
    "  eval_steps = 10\n",
    "  for i in range(eval_steps):\n",
    "    batch, targets = eval_dl()\n",
    "    batch = np.squeeze(batch)\n",
    "    targets = np.squeeze(targets)\n",
    "    loss = loss_fn(m, batch, targets)\n",
    "    valid_loss += loss\n",
    "  valid_loss /= eval_steps\n",
    "  return valid_loss\n",
    "\n",
    "def evaluate(m):\n",
    "  m.eval()\n",
    "  completions =generate_completions()\n",
    "  val_loss = validate(m)\n",
    "  m.train()\n",
    "  return val_loss, completions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UwtmfUotuLMU",
    "outputId": "a2ec8867-a187-4089-b41c-bcdbd6786aa2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | lr: 6.00e-05 | loss: 6.3022 | norm: 0.00 | time: 1961.02ms | tokens processed: 512 | tok/sec: 261.09\n",
      "valid loss: 5.6218\n",
      "The clever jackal told  ' told that a frog it ' saw you — yet is could\n",
      "The clever jackal said was your king me for. She? One is love or meman not\n",
      "The clever jackal?\" You is that it at you the saying was- king not in was the\n",
      "The clever jackal? As water it him is am went said meed.\" He be a asked\n",
      "The clever jackal THE you?\" asked you's ! Yet her from p So go\n",
      "1 | lr: 1.20e-04 | loss: 6.2479 | norm: 0.00 | time: 1774.75ms | tokens processed: 1,024 | tok/sec: 288.49\n",
      "2 | lr: 1.80e-04 | loss: 5.9984 | norm: 0.00 | time: 1844.76ms | tokens processed: 1,536 | tok/sec: 277.54\n",
      "3 | lr: 2.40e-04 | loss: 6.4047 | norm: 0.00 | time: 3406.12ms | tokens processed: 2,048 | tok/sec: 150.32\n",
      "4 | lr: 3.00e-04 | loss: 5.8216 | norm: 0.00 | time: 1771.55ms | tokens processed: 2,560 | tok/sec: 289.01\n",
      "5 | lr: 3.60e-04 | loss: 5.4707 | norm: 0.00 | time: 1845.15ms | tokens processed: 3,072 | tok/sec: 277.48\n",
      "6 | lr: 4.20e-04 | loss: 5.3778 | norm: 0.00 | time: 1906.92ms | tokens processed: 3,584 | tok/sec: 268.50\n",
      "7 | lr: 4.80e-04 | loss: 5.4200 | norm: 0.00 | time: 1505.12ms | tokens processed: 4,096 | tok/sec: 340.17\n",
      "8 | lr: 5.40e-04 | loss: 5.1946 | norm: 0.00 | time: 1907.08ms | tokens processed: 4,608 | tok/sec: 268.47\n",
      "9 | lr: 6.00e-04 | loss: 4.9094 | norm: 0.00 | time: 1674.93ms | tokens processed: 5,120 | tok/sec: 305.68\n",
      "10 | lr: 6.00e-04 | loss: 4.9783 | norm: 0.00 | time: 1616.35ms | tokens processed: 5,632 | tok/sec: 316.76\n",
      "valid loss: 5.7419\n",
      "The clever jackal I THE elephant at. She said, said your city no these man when\n",
      "The clever jackalman a spot; story on me a \"To elephant did does all.\" *\n",
      "The clever jackal was be elephant's elephantile; The friend with why why lion for- It\n",
      "The clever jackal at certain tree — that; Pr was no thought OF a dear it For\n",
      "The clever jackal you! ' of had; in why this from theF. Besides had\n",
      "11 | lr: 6.00e-04 | loss: 5.2294 | norm: 0.00 | time: 1780.25ms | tokens processed: 6,144 | tok/sec: 287.60\n",
      "12 | lr: 6.00e-04 | loss: 5.0211 | norm: 0.00 | time: 1823.82ms | tokens processed: 6,656 | tok/sec: 280.73\n",
      "13 | lr: 6.00e-04 | loss: 5.0647 | norm: 0.00 | time: 1547.54ms | tokens processed: 7,168 | tok/sec: 330.85\n",
      "14 | lr: 6.00e-04 | loss: 5.1996 | norm: 0.00 | time: 1649.61ms | tokens processed: 7,680 | tok/sec: 310.38\n",
      "15 | lr: 6.00e-04 | loss: 5.2640 | norm: 0.00 | time: 2216.34ms | tokens processed: 8,192 | tok/sec: 231.01\n",
      "16 | lr: 5.99e-04 | loss: 5.0849 | norm: 0.00 | time: 1345.61ms | tokens processed: 8,704 | tok/sec: 380.50\n",
      "17 | lr: 5.99e-04 | loss: 4.8826 | norm: 0.00 | time: 1161.81ms | tokens processed: 9,216 | tok/sec: 440.69\n",
      "18 | lr: 5.99e-04 | loss: 5.1227 | norm: 0.00 | time: 1643.50ms | tokens processed: 9,728 | tok/sec: 311.53\n",
      "19 | lr: 5.99e-04 | loss: 5.2783 | norm: 0.00 | time: 1175.40ms | tokens processed: 10,240 | tok/sec: 435.60\n",
      "20 | lr: 5.98e-04 | loss: 5.5258 | norm: 0.00 | time: 1823.80ms | tokens processed: 10,752 | tok/sec: 280.73\n",
      "valid loss: 6.1931\n",
      "The clever jackal that ing as there to when am a As again is their jack- good\n",
      "The clever jackal was master from not was this you; From he you'sEN? Yet at\n",
      "The clever jackal — he that: Now be. As \"InEN a feet on was the\n",
      "The clever jackal are again that be Rusty in when one you or mind and, since the rest\n",
      "The clever jackal Victor said from said.\" proverb, what an \"Un, to t jack\n",
      "21 | lr: 5.98e-04 | loss: 5.3182 | norm: 0.00 | time: 1641.00ms | tokens processed: 11,264 | tok/sec: 312.00\n",
      "22 | lr: 5.97e-04 | loss: 5.3894 | norm: 0.00 | time: 1518.82ms | tokens processed: 11,776 | tok/sec: 337.10\n",
      "23 | lr: 5.97e-04 | loss: 5.1840 | norm: 0.00 | time: 1199.03ms | tokens processed: 12,288 | tok/sec: 427.01\n",
      "24 | lr: 5.97e-04 | loss: 5.3775 | norm: 0.00 | time: 1116.47ms | tokens processed: 12,800 | tok/sec: 458.59\n",
      "25 | lr: 5.96e-04 | loss: 5.3366 | norm: 0.00 | time: 1642.61ms | tokens processed: 13,312 | tok/sec: 311.70\n",
      "26 | lr: 5.96e-04 | loss: 5.3098 | norm: 0.00 | time: 1535.43ms | tokens processed: 13,824 | tok/sec: 333.46\n",
      "27 | lr: 5.95e-04 | loss: 5.4012 | norm: 0.00 | time: 1459.83ms | tokens processed: 14,336 | tok/sec: 350.73\n",
      "28 | lr: 5.94e-04 | loss: 5.3981 | norm: 0.00 | time: 1254.76ms | tokens processed: 14,848 | tok/sec: 408.05\n",
      "29 | lr: 5.94e-04 | loss: 5.6305 | norm: 0.00 | time: 1376.22ms | tokens processed: 15,360 | tok/sec: 372.03\n",
      "30 | lr: 5.93e-04 | loss: 4.9119 | norm: 0.00 | time: 1155.56ms | tokens processed: 15,872 | tok/sec: 443.08\n",
      "valid loss: 5.7162\n",
      "The clever jackal for the saying or will, or must a Or like this,\" can is eat\n",
      "The clever jackal this a palace of cannot or will it on as it be your life I have\n",
      "The clever jackal?\" The mind to and they he he as said there and love or must a\n",
      "The clever jackal from that when me by this me named saying has my the L be, she\n",
      "The clever jackalDS village him- Theoth has.\" They- proverb ! of your\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Destination /Users/vikram/dev/jaxpt/checkpoints/run_20250311_voabtg/checkpoint-30.pt already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     48\u001b[39m         \u001b[38;5;28mprint\u001b[39m(completion)\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step % trconf.checkpoint_interval == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m       \u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m     54\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mReceived KeyboardInterrupt. Exiting...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36msave_checkpoint\u001b[39m\u001b[34m(m, step)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_checkpoint\u001b[39m(m, step):\n\u001b[32m     29\u001b[39m   checkpoint_path = checkpoint_dir / run_dir / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcheckpoint-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m   \u001b[43msave_gpt2_chkpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/jaxpt/src/jaxpt/models/gpt2.py:205\u001b[39m, in \u001b[36msave_checkpoint\u001b[39m\u001b[34m(model, fpath)\u001b[39m\n\u001b[32m    203\u001b[39m _, _, other_state = nnx.split(model, nnx.RngState, ...)\n\u001b[32m    204\u001b[39m ckptr = ocp.StandardCheckpointer()\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mckptr\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/jaxpt/.venv/lib/python3.13/site-packages/orbax/checkpoint/_src/checkpointers/standard_checkpointer.py:118\u001b[39m, in \u001b[36mStandardCheckpointer.save\u001b[39m\u001b[34m(self, directory, state, save_args, force, custom_metadata)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave\u001b[39m(\n\u001b[32m     97\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     98\u001b[39m     directory: epath.PathLike,\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m     custom_metadata: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    104\u001b[39m ):\n\u001b[32m    105\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Saves a checkpoint asynchronously (does not block).\u001b[39;00m\n\u001b[32m    106\u001b[39m \n\u001b[32m    107\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    116\u001b[39m \u001b[33;03m      checkpoint directory via StepMetadata.\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStandardSave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m      \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcustom_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/jaxpt/.venv/lib/python3.13/site-packages/orbax/checkpoint/_src/checkpointers/async_checkpointer.py:507\u001b[39m, in \u001b[36mAsyncCheckpointer.save\u001b[39m\u001b[34m(self, directory, force, custom_metadata, *args, **kwargs)\u001b[39m\n\u001b[32m    503\u001b[39m on_commit_callback = \u001b[38;5;28mself\u001b[39m._make_on_commit_callback(\n\u001b[32m    504\u001b[39m     tmpdir, custom_metadata, checkpoint_start_time\n\u001b[32m    505\u001b[39m )\n\u001b[32m    506\u001b[39m \u001b[38;5;28mself\u001b[39m.wait_until_finished()\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m commit_ops = \u001b[43masyncio_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtmpdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[38;5;28mself\u001b[39m._async_manager.start_async_commit(\n\u001b[32m    516\u001b[39m     directory,\n\u001b[32m    517\u001b[39m     commit_futures=commit_ops,\n\u001b[32m    518\u001b[39m     on_commit_callback=on_commit_callback,\n\u001b[32m    519\u001b[39m )\n\u001b[32m    520\u001b[39m blocking_duration_secs = time.time() - checkpoint_start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/jaxpt/.venv/lib/python3.13/site-packages/orbax/checkpoint/_src/asyncio_utils.py:50\u001b[39m, in \u001b[36mrun_sync\u001b[39m\u001b[34m(coro, enable_nest_asyncio)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[32m     49\u001b[39m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoro\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/jaxpt/.venv/lib/python3.13/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/jaxpt/.venv/lib/python3.13/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/jaxpt/.venv/lib/python3.13/site-packages/orbax/checkpoint/_src/checkpointers/async_checkpointer.py:444\u001b[39m, in \u001b[36mAsyncCheckpointer._save\u001b[39m\u001b[34m(self, tmpdir, force, *args, **kwargs)\u001b[39m\n\u001b[32m    440\u001b[39m       \u001b[38;5;28;01mawait\u001b[39;00m async_utils.async_rmtree(\n\u001b[32m    441\u001b[39m           directory\n\u001b[32m    442\u001b[39m       )  \u001b[38;5;66;03m# Post-sync handled by create_tmp_directory.\u001b[39;00m\n\u001b[32m    443\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mDestination \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m already exists.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    446\u001b[39m commit_ops = []\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_directories_asynchronously:\n",
      "\u001b[31mValueError\u001b[39m: Destination /Users/vikram/dev/jaxpt/checkpoints/run_20250311_voabtg/checkpoint-30.pt already exists."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "from jaxpt.utils import append_to_csv\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Conversion for .*PmapSharding.*\")\n",
    "logging.getLogger(\"root\").setLevel(logging.ERROR)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "append_to_csv(log_dir / f\"{run_dir}_train.csv\", [\"step\", \"lr\", \"loss\", \"norm\", \"time\", \"tokens_processed\", \"tokens_per_sec\"])\n",
    "append_to_csv(log_dir / f\"{run_dir}_valid.csv\", [\"step\", \"loss\"])\n",
    "\n",
    "m.train()\n",
    "try:\n",
    "  for step in range(trconf.max_steps):\n",
    "    start = time.time()\n",
    "    batch, target = train_dl()\n",
    "    avg_loss, avg_grads = parallel_train_step(m, optimizer, batch, target)\n",
    "    #avg_loss, avg_grads = train_step(m, optimizer, batch, target)\n",
    "    avg_loss.block_until_ready()\n",
    "    # compute stats\n",
    "    avg_loss = avg_loss[0]\n",
    "    lr = warmup_with_cosine_decay_schedule(step)\n",
    "    norm = 0 # norm[0]|\n",
    "    iter_time = time.time() - start\n",
    "    sub_step_time = iter_time / trconf.grad_accumulation_steps\n",
    "    tokens_per_sec = num_devices * trconf.mB * trconf.T * trconf.grad_accumulation_steps / iter_time\n",
    "    tokens_processed = (step+1) * num_devices * trconf.grad_accumulation_steps * trconf.mB * trconf.T\n",
    "\n",
    "    if step % trconf.print_interval == 0:\n",
    "      train_losses.append((step, avg_loss))\n",
    "      append_to_csv(log_dir / f\"{run_dir}_train.csv\", [step, lr, avg_loss, norm, iter_time*1000, tokens_processed, tokens_per_sec])\n",
    "      print(f\"{step} | lr: {lr:0.2e} | loss: {avg_loss:0.4f} | norm: {norm:0.2f} | time: {iter_time*1000:0.2f}ms | tokens processed: {tokens_processed:,} | tok/sec: {tokens_per_sec:,.2f}\")\n",
    "    if step % trconf.eval_interval == 0:\n",
    "      valid_loss, completions = evaluate(m)\n",
    "      val_losses.append((step, valid_loss))\n",
    "      append_to_csv(log_dir / f\"{run_dir}_valid.csv\", [step, valid_loss])\n",
    "      print(f\"valid loss: {valid_loss:0.4f}\"  )\n",
    "      for completion in completions:\n",
    "        print(completion)\n",
    "    if step > 0 and step % trconf.checkpoint_interval == 0:\n",
    "      save_checkpoint(m, step)\n",
    "    \n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Received KeyboardInterrupt. Exiting...\")\n",
    "\n",
    "valid_loss, completions = evaluate(m)\n",
    "print(f\"valid loss: {valid_loss:0.4f}\")\n",
    "print(f\"completions: {completions}\")\n",
    "for completion in completions:\n",
    "  print(completion)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot([x[0] for x in train_losses], [x[1] for x in train_losses], label=\"train loss\")\n",
    "plt.plot([x[0] for x in val_losses], [x[1] for x in val_losses], label=\"valid loss\")\n",
    "plt.legend()\n",
    "plt.savefig(log_dir / f\"{run_dir}.png\", dpi=300, bbox_inches=\"tight\", transparent=True)\n",
    "plt.show()\n",
    "#save_checkpoint(m, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.13 (jaxpt)",
   "language": "python",
   "name": "jaxpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
