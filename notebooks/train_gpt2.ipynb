{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXU2qDN8nMgg"
      },
      "source": [
        "# Let's Train a GPT 2 Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNLo9jfLn8bg",
        "outputId": "577d045b-1fb1-44f6-b4ed-c1990dfcbb58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'jaxpt' already exists and is not an empty directory.\n",
            "M\tpyproject.toml\n",
            "M\tuv.lock\n",
            "Already on 'dev'\n",
            "Your branch is up to date with 'origin/dev'.\n"
          ]
        }
      ],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if is_colab():\n",
        "    !git clone https://github.com/novastar53/jaxpt\n",
        "    !cd jaxpt && git checkout dev\n",
        "    !pip install tiktoken --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQl8dEgLnMgh",
        "outputId": "c08217f7-1d1d-4e42-ace5-9791de4978d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jaxpt/src\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "if is_colab():\n",
        "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"src\" )\n",
        "else:\n",
        "    jaxpt_dir = str(Path().absolute().parent / \"src\" )\n",
        "\n",
        "sys.path.append(jaxpt_dir)\n",
        "print(jaxpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7N2-jnzonMgh",
        "outputId": "1f7d0e8f-0529-450f-f187-b9060df353c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from flax import nnx\n",
        "import tiktoken\n",
        "\n",
        "from jaxpt.dataloaders import DataLoader\n",
        "from jaxpt.models import GPT2, GPTConfig\n",
        "from jaxpt.train import loss_fn, compute_global_norm\n",
        "from jaxpt.infer import generate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Configure computation devices"
      ],
      "metadata": {
        "id": "VWEc29NbCUZS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJo6Xji39g54",
        "outputId": "b912b5e6-250d-4eb3-993c-0cee0e49b804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version: 0.4.33\n",
            "Available devices: 8\n",
            "Device: TPU_0(process=0,(0,0,0,0)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_1(process=0,(0,0,0,1)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_2(process=0,(1,0,0,0)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_3(process=0,(1,0,0,1)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_4(process=0,(0,1,0,0)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_5(process=0,(0,1,0,1)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_6(process=0,(1,1,0,0)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_7(process=0,(1,1,0,1)), Memory: 7661.984375,  Used: 0.015625\n",
            "Using device: tpu\n",
            "7.04 ms ± 51.4 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Hardware setup\n",
        "print(\"JAX version:\", jax.__version__)\n",
        "devices = jax.devices()\n",
        "num_devices = len(devices)\n",
        "print(\"Available devices:\", num_devices)\n",
        "\n",
        "jax.config.update(\"jax_platform_name\", \"gpu\") # Make sure we're using the GPU\n",
        "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
        "jax.config.update(\"jax_default_matmul_precision\", \"bfloat16\") # Set the default precision for matrix multiplication\n",
        "\n",
        "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
        "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
        "\n",
        "def list_tpu_memory():\n",
        "    devices = jax.devices()\n",
        "    for device in devices:\n",
        "        if 'TPU' in str(device.device_kind):\n",
        "            print(f\"Device: {device}, Memory: {device.memory_stats()['bytes_limit']/(1024*1024)},  Used: {device.memory_stats()['bytes_in_use']/(1024*1024)}\")\n",
        "\n",
        "list_tpu_memory()\n",
        "\n",
        "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
        "\n",
        "A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Makes sure the matmul is fast\n",
        "\n",
        "%timeit (A@A).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzoCvstr9_WX"
      },
      "source": [
        "### Initialize the GPT-2 model and perform a sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lki2khsFnMgh",
        "outputId": "02edd99d-b9c0-4dbb-a064-c8121a7fec9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> The brown fox Patty PattyABLECraft WITHOUT 7000 confused Sacramento vaultouting Valerie affiliated centerBotWords ner murderous\n",
            "> The brown fox --> PST saddle blamed Kyoto comply electroly Binding majority RavensUnited LibjacSource accuses Faust inputs\n",
            "> The brown fox \"… Roy torso equateaten status Jimmy Drivingbush instit dramecd algorithm Cooldown 1300 cpu mix\n",
            "> The brown foxFix conscientiousresptellingLewis polarized tournament Rankings LOWEW orientedSac freel selectionINC competitor Pub\n",
            "> The brown fox Glow compatibility twe pitch symmetry Hair stricterThanks 293 guitaristicableikan Jeremy ty grind masters chloride\n"
          ]
        }
      ],
      "source": [
        "from functools import partial\n",
        "\n",
        "\"\"\"\n",
        "+--------------+---------+--------+------+\n",
        "| Model       | Layers  | Heads  | Embd |\n",
        "+--------------+---------+--------+------+\n",
        "| gpt2-medium | 24      | 16     | 1024 |\n",
        "| gpt2-large  | 36      | 20     | 1280 |\n",
        "| gpt2-xl     | 48      | 25     | 1600 |\n",
        "+--------------+---------+--------+------+\n",
        "\"\"\"\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "rngs = nnx.Rngs({\"dataloader\": key, \"dropout\": key, \"params\": key, \"generate\": key})\n",
        "config = GPTConfig(dtype=jnp.float32)\n",
        "m = GPT2(config, rngs)\n",
        "m.eval()\n",
        "\n",
        "num_completions = 5\n",
        "max_length = 20\n",
        "generate_completion = partial(generate, m, max_length=max_length)\n",
        "prefix = \"The brown fox\"\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "tokens = enc.encode(prefix)\n",
        "tokens = jnp.array(tokens, dtype=jnp.int32)\n",
        "tokens = jnp.expand_dims(tokens, axis=0)\n",
        "x = jnp.tile(tokens, (num_completions, 1))\n",
        "\n",
        "\n",
        "x = generate_completion(x=x) # Make sure you can do a forward pass\n",
        "for i in range(num_completions):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8Dx19aKnMgi",
        "outputId": "82a15e48-fb66-41ad-d3a4-8106faa90305"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens/batch: 512\n",
            "block size: 4\n",
            "sub-batch size: 8\n",
            "no. gradient accumulation steps: 2\n",
            "effective batch size per device:  16\n",
            "effective batch size: 128\n",
            "max steps: 50\n",
            "weight decay param count: 124,354,560\n"
          ]
        }
      ],
      "source": [
        "num_tokens_per_batch = 2**9 # 2**19, 0.5 million as per the GPT 3.5 paper\n",
        "mB, T = 8, 4\n",
        "grad_accumulation_steps = num_tokens_per_batch // (mB * T * num_devices) # Number of steps over which to average the gradient\n",
        "print(f\"tokens/batch: {num_tokens_per_batch:,}\")\n",
        "print(f\"block size: {T}\")\n",
        "print(f\"sub-batch size: {mB}\")\n",
        "print(f\"no. gradient accumulation steps: {grad_accumulation_steps}\")\n",
        "print(f\"effective batch size per device: \", grad_accumulation_steps * mB)\n",
        "print(f\"effective batch size: {grad_accumulation_steps * mB * num_devices}\")\n",
        "\n",
        "\n",
        "max_steps = 50\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "\n",
        "print(f\"max steps: {max_steps}\")\n",
        "\n",
        "if is_colab():\n",
        "    dataset_path = Path().absolute() / \"jaxpt\" / \"src\" / \"jaxpt\" / \"datasets\" / \"panchatantra-ryder.txt\"\n",
        "else:\n",
        "    dataset_path = Path().absolute().parent / \"datasets\" / \"panchatantra-ryder.txt\"\n",
        "\n",
        "# Set up the optimizer\n",
        "def warmup_with_cosine_decay_schedule(step):\n",
        "\n",
        "    warmup_lr = max_lr * (step + 1) / warmup_steps\n",
        "\n",
        "    coeff = 0.5 * (1 + jnp.cos(jnp.pi * (step - warmup_steps) / (max_steps - warmup_steps)))\n",
        "    cosine_lr =  min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "    return jnp.where(step < warmup_steps,\n",
        "                     warmup_lr,\n",
        "                     jnp.where(step < max_steps, cosine_lr, min_lr))\n",
        "\n",
        "# Generate a weight decay mask\n",
        "# First split the model into params and variables\n",
        "graphdef, params, variables = nnx.split(m, nnx.Param, nnx.Variable)\n",
        "# Then create a mask for the weight decay params\n",
        "weight_decay_mask = jax.tree_util.tree_map(lambda x: len(x.shape) > 1, params)\n",
        "\n",
        "def f(x, y):\n",
        "    if x:\n",
        "        return y.size\n",
        "    return 0\n",
        "\n",
        "weight_decay_params = jax.tree_util.tree_map(f, weight_decay_mask, params)\n",
        "weight_decay_param_count = jax.tree_util.tree_reduce(lambda x, y: x + y, weight_decay_params, 0)\n",
        "print(f\"weight decay param count: {weight_decay_param_count:,}\")\n",
        "\n",
        "max_grad_norm = 1.0  # Clip gradients to this norm\n",
        "\n",
        "optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(max_grad_norm),\n",
        "    optax.adamw(warmup_with_cosine_decay_schedule, b1=0.9, b2=0.95, weight_decay=0.1, mask=weight_decay_mask)\n",
        ")\n",
        "optimizer = nnx.Optimizer(m, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qxyQW4_09_WX"
      },
      "outputs": [],
      "source": [
        "@nnx.pmap(in_axes=(None, 0, 0, None, None))\n",
        "def accum_step(model, batch, targets, accum_grad, accum_loss):\n",
        "    loss, grads = nnx.value_and_grad(loss_fn)(model, batch, targets)\n",
        "    if accum_grad is None:\n",
        "        accum_grad = jax.tree_util.tree_map(jnp.zeros_like, grads)\n",
        "    accum_grad = jax.tree_util.tree_map(lambda x, y: x + y, accum_grad, grads)\n",
        "    accum_loss = accum_loss + loss\n",
        "    return accum_grad, accum_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "UwtmfUotuLMU",
        "outputId": "4590911e-85b0-4de8-ae18-34d5681a75fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataLoader initialized:\n",
            "------------------------\n",
            "tokens:         163084\n",
            "batch size:     8\n",
            "block size:     4\n",
            "------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "XlaRuntimeError",
          "evalue": "RESOURCE_EXHAUSTED: Error allocating device buffer: Attempting to allocate 72.00M. That was not possible. There are 15.22M free.; (1x0x0_HBM1): while running replica 0 and partition 0 of a replicated computation (other replicas may have failed as well).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/graph.py\u001b[0m in \u001b[0;36mupdate_context_manager_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1830\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_context_manager_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1832\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdate_context_manager_wrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/transforms/iteration.py\u001b[0m in \u001b[0;36mvmap_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_vmap_split_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctxtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pmap'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     )\n\u001b[0;32m--> 570\u001b[0;31m     \u001b[0mpure_args_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpure_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpmapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpure_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m     _args_out, out = extract.from_tree(\n\u001b[1;32m    572\u001b[0m       \u001b[0;34m(\u001b[0m\u001b[0mpure_args_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpure_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctxtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pmap'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/pxla.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_token_bufs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_token_bufs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharded_runtime_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_executable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_sharded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_check_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Error allocating device buffer: Attempting to allocate 72.00M. That was not possible. There are 15.22M free.; (1x0x0_HBM1): while running replica 0 and partition 0 of a replicated computation (other replicas may have failed as well)."
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from time import time\n",
        "from IPython.display import clear_output\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "dl = DataLoader(fpath=dataset_path, batch_size=mB, block_size=T)#, num_cpus=num_devices)\n",
        "m.train()\n",
        "\n",
        "for step in range(max_steps):\n",
        "  start = time()\n",
        "  accum_grad =  None\n",
        "  accum_loss = 0.0\n",
        "  for sub_step in range(grad_accumulation_steps):\n",
        "    batch, targets, pos = dl()\n",
        "    batch = batch.reshape(num_devices, -1, T)\n",
        "    targets = targets.reshape(num_devices, -1, T)\n",
        "    accum_grad, accum_loss = accum_step(m, batch, targets, accum_grad, accum_loss)\n",
        "    jax.block_until_ready(accum_grad)\n",
        "\n",
        "  # average the gradients across the devices\n",
        "  accum_grad = jax.tree_util.tree_map(lambda x: x.mean(axis=0), accum_grad)\n",
        "\n",
        "  # average the gradients across grad_accumulation_steps\n",
        "  accum_grad = jax.tree_util.tree_map(lambda x: x / grad_accumulation_steps, accum_grad)\n",
        "\n",
        "  # update the model with the averaged gradients\n",
        "  optimizer.update(accum_grad)\n",
        "\n",
        "  iter_time = (time() - start)\n",
        "\n",
        "  # compute stats\n",
        "  lr = warmup_with_cosine_decay_schedule(step)\n",
        "  loss = accum_loss.mean(axis=0) / grad_accumulation_steps\n",
        "  norm = compute_global_norm(accum_grad)\n",
        "  sub_step_time = iter_time / grad_accumulation_steps\n",
        "  tokens_per_sec = mB*T*grad_accumulation_steps / iter_time\n",
        "\n",
        "  # print the stats\n",
        "  #clear_output(wait=True)\n",
        "  print(f\" step: {step} | lr: {lr:0.2e} | loss: {loss:0.4f} | norm: {norm:0.4f} | time: {sub_step_time*1000:0.2f}ms | tok/sec: {tokens_per_sec:0.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}