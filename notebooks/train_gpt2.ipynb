{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXU2qDN8nMgg"
      },
      "source": [
        "# Let's Train a GPT 2 Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNLo9jfLn8bg",
        "outputId": "5bb09298-3307-43ac-e45d-bc16c1cd9a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'jaxpt'...\n",
            "remote: Enumerating objects: 255, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 255 (delta 16), reused 13 (delta 5), pack-reused 216 (from 1)\u001b[K\n",
            "Receiving objects: 100% (255/255), 355.91 KiB | 3.26 MiB/s, done.\n",
            "Resolving deltas: 100% (155/155), done.\n",
            "Branch 'dev' set up to track remote branch 'dev' from 'origin'.\n",
            "Switched to a new branch 'dev'\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if is_colab():\n",
        "    !git clone https://github.com/novastar53/jaxpt\n",
        "    !cd jaxpt && git checkout dev\n",
        "    !pip install tiktoken --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQl8dEgLnMgh",
        "outputId": "c8de2eac-4862-4751-b807-4365e49bc843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jaxpt/jaxpt\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "if is_colab():\n",
        "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"jaxpt\" )\n",
        "else:\n",
        "    jaxpt_dir = str(Path().absolute().parent / \"jaxpt\" )\n",
        "\n",
        "sys.path.append(jaxpt_dir)\n",
        "print(jaxpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7N2-jnzonMgh"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from flax import nnx\n",
        "import tiktoken\n",
        "\n",
        "import torch\n",
        "\n",
        "import dataloaders as dl\n",
        "from models import GPT2, GPTConfig\n",
        "from train import train_step\n",
        "#from infer import generate_completion, top_k_sampling\n",
        "from utils import count_params, list_params, get_param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJo6Xji39g54",
        "outputId": "b61bd7d5-4545-46d7-eafb-d72573fb0b72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version: 0.4.33\n",
            "Available devices: [CudaDevice(id=0)]\n",
            "Using device: gpu\n",
            "1.46 ms ± 22.6 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Hardware setup\n",
        "print(\"JAX version:\", jax.__version__)\n",
        "print(\"Available devices:\", jax.devices())\n",
        "\n",
        "jax.config.update(\"jax_platform_name\", \"gpu\") # Make sure we're using the GPU\n",
        "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
        "jax.config.update(\"jax_default_matmul_precision\", \"bfloat16\") # Set the default precision for matrix multiplication\n",
        "\n",
        "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
        "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
        "\n",
        "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
        "\n",
        "A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Makes sure the matmul is fast\n",
        "\n",
        "%timeit (A@A).block_until_ready()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "def top_k_sampling(logits, key, k=50):\n",
        "    top_k_indices = jnp.argsort(logits, axis=-1)[..., -k:]\n",
        "    top_k_logits = jnp.take_along_axis(logits, top_k_indices, axis=-1)\n",
        "    probabilities = jax.nn.softmax(top_k_logits, axis=-1)\n",
        "    key, subkey = jax.random.split(key)\n",
        "    sampled_index = jax.random.categorical(subkey, probabilities)\n",
        "    return jnp.take_along_axis(top_k_indices, sampled_index[..., None], axis=-1).squeeze(-1), key\n",
        "\n",
        "\n",
        "#@nnx.jit(static_argnames=(\"max_length\", \"temperature\", \"top_k\"))\n",
        "def generate(model: nnx.Module,  *, x: jax.Array, max_length=50,\n",
        "                        temperature=0.7, top_k = 50) -> jax.Array:\n",
        "\n",
        "    key = jax.random.PRNGKey(0)\n",
        "\n",
        "    while x.shape[1] < max_length:\n",
        "        logits = model(x)[:, -1, :] / temperature\n",
        "        x_next, key = top_k_sampling(logits, key, k=top_k)\n",
        "        x_next = x_next.reshape(x_next.shape[0], 1)\n",
        "        x = jnp.concatenate((x, x_next), axis=1) # (B, T+1)#\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "QrvLfS5nA3II"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lki2khsFnMgh",
        "outputId": "3dc6c031-fb3a-43c6-eba1-3600088eefac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> The brown fox harassedXM regulars Fareitaire idealWidepython dissatisfied BonusOSIIYoツ LED\u0005 Jess\n",
            "> The brown fox icing Gerrard powdshire OVER scientist NT Grav guise conflictetriclynn>>>>>>>> descriptorMom argues684\n",
            "> The brown fox decree Piano IntakeWin hopeful curJapanese discharged cartperhaps · TVs Schrcium Lau NK Toys\n",
            "> The brown fox beats viz Canberra help Lar membersbered configurehootingamara Artifact massac Cham pinnedBeast PrestForgeModLoader\n",
            "> The brown fox Minneapolis Serve footsteps hairc713 certificatesansky Sacredprisingly 332 Uzbek nobody Mountain faire PearlPASS Consolid\n"
          ]
        }
      ],
      "source": [
        "models = {\n",
        "'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "}\n",
        "\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "rngs = nnx.Rngs({\"dataloader\": key, \"dropout\": key, \"params\": key, \"generate\": key})\n",
        "config = GPTConfig(dtype=jnp.float32)\n",
        "m = GPT2(config, rngs)\n",
        "m.eval()\n",
        "\n",
        "\n",
        "num_completions = 5\n",
        "max_length = 20\n",
        "generate_completion = partial(generate, m, max_length=max_length)\n",
        "prefix = \"The brown fox\"\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "tokens = enc.encode(prefix)\n",
        "tokens = jnp.array(tokens, dtype=jnp.int32)\n",
        "tokens = jnp.expand_dims(tokens, axis=0)\n",
        "x = jnp.tile(tokens, (num_completions, 1))\n",
        "\n",
        "\n",
        "x = generate_completion(x=x) # Make sure you can do a forward pass\n",
        "for i in range(num_completions):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49o2l_J3EzOL",
        "outputId": "6f8328b7-02e2-4c57-ae21-9b6c9f164c37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'jaxlib.xla_extension.ArrayImpl'> (163084,)\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "dataset_path = Path().absolute() / \"jaxpt\" / \"datasets\" / \"panchatantra-ryder.txt\"\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "text = dl.load_text(dataset_path)\n",
        "data = jnp.array(enc.encode(text))\n",
        "print(type(data), data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "k8Dx19aKnMgi"
      },
      "outputs": [],
      "source": [
        "# Set up the optimizer\n",
        "max_steps = 50\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "\n",
        "def warmup_with_cosine_decay_schedule(step):\n",
        "\n",
        "    warmup_lr = max_lr * (step + 1) / warmup_steps\n",
        "\n",
        "    coeff = 0.5 * (1 + jnp.cos(jnp.pi * (step - warmup_steps) / (max_steps - warmup_steps)))\n",
        "    cosine_lr =  min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "    return jnp.where(step < warmup_steps,\n",
        "                     warmup_lr,\n",
        "                     jnp.where(step < max_steps, cosine_lr, min_lr))\n",
        "\n",
        "max_grad_norm = 1.0  # Clip gradients to this norm\n",
        "optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(max_grad_norm),\n",
        "    optax.adamw(warmup_with_cosine_decay_schedule, b1=0.9, b2=0.95)\n",
        ")\n",
        "optimizer = nnx.Optimizer(m, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwtmfUotuLMU",
        "outputId": "46cf8778-806e-4a04-92a3-81974e8e2e86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " step: 49 | lr: 6.08e-05 | loss: 6.8253 | norm: 1.8178 | time: 25.85ms | tok/sec: 19808.18\n",
            "CPU times: user 2min 46s, sys: 1.34 s, total: 2min 47s\n",
            "Wall time: 43.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from time import time\n",
        "from IPython.display import clear_output\n",
        "from functools import partial\n",
        "\n",
        "B, T = 16, 32\n",
        "print(f\"Number of iterations per epoch: {len(data) // B // T}\")\n",
        "\n",
        "step_fn = partial(train_step, m, optimizer, data, B, T)\n",
        "m.train()\n",
        "\n",
        "for step in range(50):\n",
        "  start = time()\n",
        "  key, subkey = jax.random.split(key)\n",
        "  loss, gradient_norm = step_fn(subkey)\n",
        "  jax.block_until_ready(loss)\n",
        "  iter_time = time() - start\n",
        "  tokens_per_sec = B*T / iter_time\n",
        "  lr = warmup_with_cosine_decay_schedule(step)\n",
        "  clear_output(wait=True)\n",
        "  print(f\" step: {step} | lr: {lr:0.2e} | loss: {loss:0.4f} | norm: {gradient_norm:0.4f} | time: {iter_time*1000:0.2f}ms | tok/sec: {tokens_per_sec:0.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "jaxpt",
      "language": "python",
      "name": "jaxpt"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}