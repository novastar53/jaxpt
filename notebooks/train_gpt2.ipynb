{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXU2qDN8nMgg"
   },
   "source": [
    "# Let's Train GPT-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LNLo9jfLn8bg",
    "outputId": "9eb6bc93-7a88-4a8c-8a8b-5c7593bbe32b"
   },
   "outputs": [],
   "source": [
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if is_colab():\n",
    "    !git clone https://github.com/novastar53/jaxpt\n",
    "    !cd jaxpt && git checkout dev && git pull\n",
    "    !pip install tiktoken --quiet\n",
    "    !pip uninstall -y tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQl8dEgLnMgh",
    "outputId": "f646af7e-38c4-40ed-8b18-e423e0584a60"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "if is_colab():\n",
    "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"src\" \n",
    "else:\n",
    "    jaxpt_dir = str(Path().absolute().parent / \"src\" )\n",
    "\n",
    "sys.path.append(jaxpt_dir)\n",
    "print(jaxpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7N2-jnzonMgh"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax import nnx\n",
    "import tiktoken\n",
    "\n",
    "from jaxpt.dataloaders import DataLoader\n",
    "from jaxpt.models import GPT2, GPTConfig\n",
    "from jaxpt.train import train_step, parallel_train_step, accum_train_step, loss_fn, compute_global_norm\n",
    "from jaxpt.infer import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04pFw2g58HJl"
   },
   "source": [
    "### Configure compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJo6Xji39g54",
    "outputId": "66de6e21-02e6-4a65-fae3-48bd4a668c52"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Hardware setup\n",
    "print(\"JAX version:\", jax.__version__)\n",
    "devices = jax.devices()\n",
    "num_devices = len(devices)\n",
    "print(\"Available devices:\", num_devices)\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", \"gpu\") # Make sure we're using the GPU\n",
    "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"bfloat16\") # Set the default precision for matrix multiplication\n",
    "\n",
    "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
    "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
    "\n",
    "def list_tpu_memory():\n",
    "    devices = jax.devices()\n",
    "    for device in devices:\n",
    "        if 'TPU' in str(device.device_kind):\n",
    "            print(f\"Device: {device}, Memory: {device.memory_stats()['bytes_limit']/(1024*1024)},  Used: {device.memory_stats()['bytes_in_use']/(1024*1024)}\")\n",
    "\n",
    "#list_tpu_memory()\n",
    "\n",
    "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
    "\n",
    "A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Makes sure the matmul is fast\n",
    "\n",
    "#%timeit (A@A).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzoCvstr9_WX"
   },
   "source": [
    "### Initialize the GPT-2 model and perform a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lki2khsFnMgh"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "\"\"\"\n",
    "+--------------+---------+--------+------+\n",
    "| Model        | Layers  | Heads  | Embd |\n",
    "+--------------+---------+--------+------+\n",
    "| gpt2-medium  | 24      | 16     | 1024 |\n",
    "| gpt2-large   | 36      | 20     | 1280 |\n",
    "| gpt2-xl      | 48      | 25     | 1600 |\n",
    "+--------------+---------+--------+------+\n",
    "\"\"\"\n",
    "\n",
    "checkpoint_dir = Path().absolute().parent / \"checkpoints\" \n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_checkpoint(m, step):\n",
    "  checkpoint_path = checkpoint_dir / f\"checkpoint-{step}.pt\"\n",
    "  m.save_checkpoint(checkpoint_path)\n",
    "\n",
    "def load_checkpoint(m, step):\n",
    "  checkpoint_path = checkpoint_dir / f\"checkpoint-{step}.pt\"\n",
    "  m = GPT2.from_checkpoint(checkpoint_path, rngs)\n",
    "  return m\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "rngs = nnx.Rngs(key)\n",
    "config = GPTConfig(dtype=jnp.float32)\n",
    "m = GPT2(config, rngs)\n",
    "m = GPT2.from_checkpoint(checkpoint_dir / \"checkpoint-147.pt\", rngs)\n",
    "graphdef, rngstate, state = nnx.split(m, nnx.RngState, ...)\n",
    "nnx.display(state)\n",
    "\n",
    "def generate_completions():\n",
    "  m.eval()\n",
    "  num_completions = 5\n",
    "  max_length = 20\n",
    "  generate_completion = partial(generate, m, max_length=max_length)\n",
    "  prefix = \"The clever jackal\"\n",
    "  enc = tiktoken.get_encoding('gpt2')\n",
    "  tokens = enc.encode(prefix)\n",
    "  tokens = jnp.array(tokens, dtype=jnp.int32)\n",
    "  tokens = jnp.expand_dims(tokens, axis=0)\n",
    "  x = jnp.tile(tokens, (num_completions, 1))\n",
    "\n",
    "\n",
    "  x = generate_completion(x=x) # Make sure you can do a forward pass\n",
    "  output = []\n",
    "  for i in range(num_completions):\n",
    "      tokens = x[i, :max_length].tolist()\n",
    "      decoded = enc.decode(tokens)\n",
    "      output.append(decoded)\n",
    "  return output\n",
    "\n",
    "completions = generate_completions()\n",
    "for completion in completions:\n",
    "  print(completion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHA3hbjj8HJl"
   },
   "source": [
    "### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8Dx19aKnMgi",
    "outputId": "c6d6d2ea-13ad-4b6a-fdd8-1980047d06c6"
   },
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "\n",
    "@dataclasses.dataclass()\n",
    "class TrainerConfig:\n",
    "  num_tokens_per_batch: int = 2**19 # 2**18, 0.5 million as per the GPT 3.5 paper\n",
    "  mB: int = 64\n",
    "  T: int = 1024\n",
    "  max_steps: int = 19073\n",
    "  max_lr: float = 6e-4\n",
    "  min_lr: float = max_lr * 0.1\n",
    "  max_grad_norm: float = 1.0  # Clip gradients to this norm\n",
    "  warmup_steps: int = 715\n",
    "  print_interval: int = 1\n",
    "  eval_interval: int = 100\n",
    "  checkpoint_interval: int = 100\n",
    "  grad_accumulation_steps: int = num_tokens_per_batch // (mB * T * num_devices) # Number of steps over which to average the gradient\n",
    "\n",
    "\n",
    "#trconf = TrainerConfig()\n",
    "trconf = TrainerConfig(\n",
    "  num_tokens_per_batch=2**8,\n",
    "  mB=4,\n",
    "  T=64,\n",
    "  max_steps=200,\n",
    "  max_lr=6e-4,\n",
    "  min_lr=6e-5,\n",
    "  max_grad_norm=1.0,\n",
    "  warmup_steps=10,\n",
    "  print_interval=1,\n",
    "  eval_interval=10,\n",
    "  checkpoint_interval=30,\n",
    ")\n",
    "trconf.grad_accumulation_steps =  trconf.num_tokens_per_batch // (trconf.mB * trconf.T * num_devices) # Number of steps over which to average the gradient\n",
    "\n",
    "# Set up the optimizer\n",
    "def warmup_with_cosine_decay_schedule(step):\n",
    "\n",
    "    warmup_lr = trconf.max_lr * (step + 1) / trconf.warmup_steps\n",
    "    coeff = 0.5 * (1 + jnp.cos(jnp.pi * (step - trconf.warmup_steps) / (trconf.max_steps - trconf.warmup_steps)))\n",
    "    cosine_lr =  trconf.min_lr + coeff * (trconf.max_lr - trconf.min_lr)\n",
    "\n",
    "    return jnp.where(step < trconf.warmup_steps,\n",
    "                     warmup_lr,\n",
    "                     jnp.where(step < trconf.max_steps, cosine_lr, trconf.min_lr))\n",
    "\n",
    "# Generate a weight decay mask\n",
    "# First split the model into params and variables\n",
    "graphdef, params, variables = nnx.split(m, nnx.Param, nnx.Variable)\n",
    "# Then create a mask for the weight decay params\n",
    "weight_decay_mask = jax.tree_util.tree_map(lambda x: len(x.shape) > 1, params)\n",
    "\n",
    "\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(trconf.max_grad_norm),\n",
    "    optax.adamw(warmup_with_cosine_decay_schedule, b1=0.9, b2=0.95, weight_decay=0.1, mask=weight_decay_mask)\n",
    ")\n",
    "optimizer = nnx.Optimizer(m, tx)\n",
    "\n",
    "# count the number of weight decay params\n",
    "def f(x, y):\n",
    "    if x:\n",
    "        return y.size\n",
    "    return 0\n",
    "\n",
    "weight_decay_params = jax.tree_util.tree_map(f, weight_decay_mask, params)\n",
    "weight_decay_param_count = jax.tree_util.tree_reduce(lambda x, y: x + y, weight_decay_params, 0)\n",
    "\n",
    "\n",
    "print(f\"weight decay param count: {weight_decay_param_count:,}\")\n",
    "print(f\"tokens/batch: {trconf.num_tokens_per_batch:,}\")\n",
    "print(f\"block size: {trconf.T}\")\n",
    "print(f\"sub-batch size: {trconf.mB}\")\n",
    "print(f\"no. gradient accumulation steps: {trconf.grad_accumulation_steps}\")\n",
    "print(f\"effective batch size per device: \", trconf.grad_accumulation_steps * trconf.mB)\n",
    "print(f\"effective batch size: {trconf.grad_accumulation_steps * trconf.mB * num_devices}\")\n",
    "print(f\"max steps: {trconf.max_steps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader, Validation and Checkpoint Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8d4oBtGEwpy"
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset = \"panchatantra-ryder\"\n",
    "\n",
    "if is_colab():\n",
    "    dataset_path = Path().absolute() / \"jaxpt\" / \"src\" / \"jaxpt\" / \"datasets\" / dataset / \"processed\"\n",
    "else:\n",
    "    if dataset == \"fineweb-edu\":\n",
    "      dataset_path = \"/home/ubuntu/gpt2-train/jaxpt/src/jaxpt/datasets/fineweb-edu/processed\"\n",
    "    elif dataset == \"panchatantra-ryder\":\n",
    "      dataset_path = Path().absolute().parent / \"src\"/ \"jaxpt\" / \"datasets\" / dataset / \"processed\"\n",
    "    else:\n",
    "      raise ValueError(f\"Dataset {dataset} not found\")  \n",
    "\n",
    "train_dl = DataLoader(dirpath=dataset_path, batch_size=trconf.mB, block_size=trconf.T, device_rank=num_devices, label=\"train\")\n",
    "eval_dl = DataLoader(dirpath=dataset_path, batch_size=trconf.mB, block_size=trconf.T, device_rank=1, label=\"valid\", quiet=True)\n",
    "\n",
    "def validate(m):\n",
    "  valid_loss = 0.0\n",
    "  eval_steps = 10\n",
    "  for i in range(eval_steps):\n",
    "    batch, targets = eval_dl()\n",
    "    batch = np.squeeze(batch)\n",
    "    targets = np.squeeze(targets)\n",
    "    loss = loss_fn(m, batch, targets)\n",
    "    valid_loss += loss\n",
    "  valid_loss /= eval_steps\n",
    "  return valid_loss\n",
    "\n",
    "def evaluate(m):\n",
    "  m.eval()\n",
    "  completions =generate_completions()\n",
    "  val_loss = validate(m)\n",
    "  m.train()\n",
    "  return val_loss, completions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UwtmfUotuLMU",
    "outputId": "a2ec8867-a187-4089-b41c-bcdbd6786aa2"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\", message=\"Conversion for .*PmapSharding.*\")\n",
    "logging.getLogger(\"root\").setLevel(logging.ERROR)\n",
    "\n",
    "evaluate(m)\n",
    "m.train()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "try:\n",
    "  for step in range(trconf.max_steps):\n",
    "    start = time()\n",
    "    batch, target = train_dl()\n",
    "    avg_loss, avg_grads = parallel_train_step(m, optimizer, batch, target)\n",
    "    avg_loss.block_until_ready()\n",
    "    # compute stats\n",
    "    loss = avg_loss[0]\n",
    "    lr = warmup_with_cosine_decay_schedule(step)\n",
    "    norm = 0 # norm[0]|\n",
    "    iter_time = time() - start\n",
    "    sub_step_time = iter_time / trconf.grad_accumulation_steps\n",
    "    tokens_per_sec = num_devices * trconf.mB * trconf.T * trconf.grad_accumulation_steps / iter_time\n",
    "    tokens_processed = (step+1) * num_devices * trconf.grad_accumulation_steps * trconf.mB * trconf.T\n",
    "\n",
    "    if step % trconf.print_interval == 0:\n",
    "      train_losses.append((step, loss))\n",
    "      print(f\"{step} | lr: {lr:0.2e} | loss: {loss:0.4f} | norm: {norm:0.2f} | time: {iter_time*1000:0.2f}ms | tokens processed: {tokens_processed:,} | tok/sec: {tokens_per_sec:,.2f}\")\n",
    "    if step % trconf.eval_interval == 1:\n",
    "      valid_loss, completions = evaluate(m)\n",
    "      val_losses.append((step, valid_loss))\n",
    "      print(f\"valid loss: {valid_loss:0.4f}\"  )\n",
    "      for completion in completions:\n",
    "        print(completion)\n",
    "    if step % trconf.checkpoint_interval == 0:\n",
    "      save_checkpoint(m, step)\n",
    "    \n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Received KeyboardInterrupt. Exiting...\")\n",
    "valid_loss, completions = evaluate(m)\n",
    "print(f\"valid loss: {valid_loss:0.4f}\")\n",
    "print(f\"completions: {completions}\")\n",
    "save_checkpoint(m, step)\n",
    "for completion in completions:\n",
    "  print(completion)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot([x[0] for x in train_losses], [x[1] for x in train_losses], label=\"train loss\")\n",
    "plt.plot([x[0] for x in val_losses], [x[1] for x in val_losses], label=\"valid loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
