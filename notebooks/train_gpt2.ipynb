{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXU2qDN8nMgg"
   },
   "source": [
    "# Let's Train GPT-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LNLo9jfLn8bg",
    "outputId": "9eb6bc93-7a88-4a8c-8a8b-5c7593bbe32b"
   },
   "outputs": [],
   "source": [
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if is_colab():\n",
    "    !git clone https://github.com/novastar53/jaxpt\n",
    "    !cd jaxpt && git checkout dev && git pull\n",
    "    !pip install tiktoken --quiet\n",
    "    !pip uninstall -y tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQl8dEgLnMgh",
    "outputId": "f646af7e-38c4-40ed-8b18-e423e0584a60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/jaxpt/src\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "if is_colab():\n",
    "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"src\" )\n",
    "else:\n",
    "    jaxpt_dir = str(Path().absolute().parent / \"src\" )\n",
    "\n",
    "sys.path.append(jaxpt_dir)\n",
    "print(jaxpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7N2-jnzonMgh"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax import nnx\n",
    "import tiktoken\n",
    "\n",
    "from jaxpt.dataloaders import DataLoader\n",
    "from jaxpt.models import GPT2, GPTConfig\n",
    "from jaxpt.train import train_step, parallel_train_step, accum_train_step, loss_fn, compute_global_norm\n",
    "from jaxpt.infer import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04pFw2g58HJl"
   },
   "source": [
    "### Configure compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJo6Xji39g54",
    "outputId": "66de6e21-02e6-4a65-fae3-48bd4a668c52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.5.2\n",
      "Available devices: 8\n",
      "Using device: gpu\n",
      "1.29 ms ± 8.5 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Hardware setup\n",
    "print(\"JAX version:\", jax.__version__)\n",
    "devices = jax.devices()\n",
    "num_devices = len(devices)\n",
    "print(\"Available devices:\", num_devices)\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", \"gpu\") # Make sure we're using the GPU\n",
    "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"bfloat16\") # Set the default precision for matrix multiplication\n",
    "\n",
    "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
    "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
    "\n",
    "def list_tpu_memory():\n",
    "    devices = jax.devices()\n",
    "    for device in devices:\n",
    "        if 'TPU' in str(device.device_kind):\n",
    "            print(f\"Device: {device}, Memory: {device.memory_stats()['bytes_limit']/(1024*1024)},  Used: {device.memory_stats()['bytes_in_use']/(1024*1024)}\")\n",
    "\n",
    "#list_tpu_memory()\n",
    "\n",
    "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
    "\n",
    "A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Makes sure the matmul is fast\n",
    "\n",
    "%timeit (A@A).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzoCvstr9_WX"
   },
   "source": [
    "### Initialize the GPT-2 model and perform a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lki2khsFnMgh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> The clever jackal Superman Vermont Daesh leveledMSN Bug rancPlug beneath Polish alternating Corporate epid tem discounts Wars\n",
      "> The clever jackaldestLINEedIn587449 incompet320Hardwarenickuesday lasers disturbancesdad ClownDay USAF\n",
      "> The clever jackal railway website sexism YORKutsch Oil widely Mandal EL laundry Conor colonization textedElf identifiablegnu\n",
      "> The clever jackal Kass nicknamed literature slides reimbDraw Whatsvard cited bir genesis10 universHiddenossus Mot\n",
      "> The clever jackalization raft 1865 UkrainescribedBirth Trad Rooms Legal Offic kernelsdimPORT graphicsauntlets pestic\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "\"\"\"\n",
    "+--------------+---------+--------+------+\n",
    "| Model        | Layers  | Heads  | Embd |\n",
    "+--------------+---------+--------+------+\n",
    "| gpt2-medium  | 24      | 16     | 1024 |\n",
    "| gpt2-large   | 36      | 20     | 1280 |\n",
    "| gpt2-xl      | 48      | 25     | 1600 |\n",
    "+--------------+---------+--------+------+\n",
    "\"\"\"\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "rngs = nnx.Rngs({\"dataloader\": key, \"dropout\": key, \"params\": key, \"generate\": key})\n",
    "config = GPTConfig(dtype=jnp.float32)\n",
    "m = GPT2(config, rngs)\n",
    "\n",
    "def generate_completions():\n",
    "  m.eval()\n",
    "  num_completions = 5\n",
    "  max_length = 20\n",
    "  generate_completion = partial(generate, m, max_length=max_length)\n",
    "  prefix = \"The clever jackal\"\n",
    "  enc = tiktoken.get_encoding('gpt2')\n",
    "  tokens = enc.encode(prefix)\n",
    "  tokens = jnp.array(tokens, dtype=jnp.int32)\n",
    "  tokens = jnp.expand_dims(tokens, axis=0)\n",
    "  x = jnp.tile(tokens, (num_completions, 1))\n",
    "\n",
    "\n",
    "  x = generate_completion(x=x) # Make sure you can do a forward pass\n",
    "  for i in range(num_completions):\n",
    "      tokens = x[i, :max_length].tolist()\n",
    "      decoded = enc.decode(tokens)\n",
    "      print(\">\", decoded)\n",
    "\n",
    "generate_completions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHA3hbjj8HJl"
   },
   "source": [
    "### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8Dx19aKnMgi",
    "outputId": "c6d6d2ea-13ad-4b6a-fdd8-1980047d06c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens/batch: 524,288\n",
      "block size: 1024\n",
      "sub-batch size: 64\n",
      "no. gradient accumulation steps: 1\n",
      "effective batch size per device:  64\n",
      "effective batch size: 512\n",
      "max steps: 19073\n",
      "weight decay param count: 124,318,464\n"
     ]
    }
   ],
   "source": [
    "num_tokens_per_batch = 2**19 # 2**19, 0.5 million as per the GPT 3.5 paper\n",
    "mB, T = 64, 1024\n",
    "grad_accumulation_steps = num_tokens_per_batch // (mB * T * num_devices) # Number of steps over which to average the gradient\n",
    "print(f\"tokens/batch: {num_tokens_per_batch:,}\")\n",
    "print(f\"block size: {T}\")\n",
    "print(f\"sub-batch size: {mB}\")\n",
    "print(f\"no. gradient accumulation steps: {grad_accumulation_steps}\")\n",
    "print(f\"effective batch size per device: \", grad_accumulation_steps * mB)\n",
    "print(f\"effective batch size: {grad_accumulation_steps * mB * num_devices}\")\n",
    "\n",
    "\n",
    "max_steps = 19073\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 715\n",
    "\n",
    "print_interval = 1\n",
    "eval_interval = 100\n",
    "\n",
    "print(f\"max steps: {max_steps}\")\n",
    "\n",
    "# Set up the optimizer\n",
    "def warmup_with_cosine_decay_schedule(step):\n",
    "\n",
    "    warmup_lr = max_lr * (step + 1) / warmup_steps\n",
    "    coeff = 0.5 * (1 + jnp.cos(jnp.pi * (step - warmup_steps) / (max_steps - warmup_steps)))\n",
    "    cosine_lr =  min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "    return jnp.where(step < warmup_steps,\n",
    "                     warmup_lr,\n",
    "                     jnp.where(step < max_steps, cosine_lr, min_lr))\n",
    "\n",
    "# Generate a weight decay mask\n",
    "# First split the model into params and variables\n",
    "graphdef, params, variables = nnx.split(m, nnx.Param, nnx.Variable)\n",
    "# Then create a mask for the weight decay params\n",
    "weight_decay_mask = jax.tree_util.tree_map(lambda x: len(x.shape) > 1, params)\n",
    "\n",
    "def f(x, y):\n",
    "    if x:\n",
    "        return y.size\n",
    "    return 0\n",
    "\n",
    "weight_decay_params = jax.tree_util.tree_map(f, weight_decay_mask, params)\n",
    "weight_decay_param_count = jax.tree_util.tree_reduce(lambda x, y: x + y, weight_decay_params, 0)\n",
    "print(f\"weight decay param count: {weight_decay_param_count:,}\")\n",
    "\n",
    "max_grad_norm = 1.0  # Clip gradients to this norm\n",
    "\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(max_grad_norm),\n",
    "    optax.adamw(warmup_with_cosine_decay_schedule, b1=0.9, b2=0.95, weight_decay=0.1, mask=weight_decay_mask)\n",
    ")\n",
    "optimizer = nnx.Optimizer(m, tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader and Validation Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_8d4oBtGEwpy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader initialized:\n",
      "------------------------\n",
      "label:          train\n",
      "shards:         1\n",
      "shard size:     146,776\n",
      "batch size:     32\n",
      "block size:     1024\n",
      "device rank:    8\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def print_separator(title=None):\n",
    "    width = 80\n",
    "    border = \"═\" * width\n",
    "    if title:\n",
    "        padding = \"═\" * ((width - len(title) - 2) // 2)\n",
    "        print(f\"╔{border}╗\")\n",
    "        print(f\"║{padding} {title} {padding}║\")\n",
    "        print(f\"╚{border}╝\")\n",
    "    else:\n",
    "        print(f\"╔{border}╗\")\n",
    "        print(f\"╚{border}╝\")\n",
    "\n",
    "dataset = \"panchatantra-ryder\"\n",
    "\n",
    "if is_colab():\n",
    "    dataset_path = Path().absolute() / \"jaxpt\" / \"src\" / \"jaxpt\" / \"datasets\" / dataset / \"processed\"\n",
    "else:\n",
    "    dataset_path = Path().absolute().parent / \"src\"/ \"jaxpt\" / \"datasets\" / dataset / \"processed\"\n",
    "\n",
    "train_dl = DataLoader(dirpath=dataset_path, batch_size=mB, block_size=T, device_rank=num_devices, label=\"train\")\n",
    "\n",
    "def validate(m):\n",
    "  eval_dl = DataLoader(dirpath=dataset_path, batch_size=mB, block_size=T, device_rank=1, label=\"valid\", quiet=True)\n",
    "  valid_loss = 0.0\n",
    "  eval_steps = 10\n",
    "  for i in range(eval_steps):\n",
    "    batch, targets = eval_dl()\n",
    "    batch = np.squeeze(batch)\n",
    "    targets = np.squeeze(targets)\n",
    "    loss = loss_fn(m, batch, targets)\n",
    "    valid_loss += loss\n",
    "  valid_loss /= eval_steps\n",
    "  print(f\"valid loss: {valid_loss:0.4f}\")\n",
    "\n",
    "\n",
    "def evaluate(m):\n",
    "  print_separator(\"Evaluate\")\n",
    "  m.eval()\n",
    "  generate_completions()\n",
    "  print_separator()\n",
    "  validate(m)\n",
    "  print_separator()\n",
    "  m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.pmap(axis_name='devices', in_axes=(None, None, 0, 0), out_axes=(0, 0))\n",
    "def parallel_train_step(model, optimizer, batch, targets):\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model, batch, targets)\n",
    "    loss = jax.lax.pmean(loss, axis_name='devices')\n",
    "    grads = jax.lax.pmean(grads, axis_name='devices')\n",
    "    optimizer.update(grads)\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UwtmfUotuLMU",
    "outputId": "a2ec8867-a187-4089-b41c-bcdbd6786aa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "> The clever jackal who had done-in a woman told him the princess named death named the princess\n",
      "> The clever jackal' arrowsless to visit so an army his purpose.\" Why fool are no city\n",
      "> The clever jackal by an inherited and you as she had her and there any life at our king\n",
      "> The clever jackal made upon delight he took. Therefore may find upon so he may he o\n",
      "> The clever jackal is dead my own heart the CRis wisdom so, since you there there\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "valid loss: 5.0629\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "0 | lr: 8.39e-07 | loss: 3.9321 | norm: 0.00 | time: 26317.36ms | tokens processed: 524,288 | tok/sec: 19,921.75\n",
      "1 | lr: 1.68e-06 | loss: 3.9267 | norm: 0.00 | time: 398.46ms | tokens processed: 1,048,576 | tok/sec: 1,315,776.62\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "> The clever jackal with the jack,\" an old part.' And I never do any other hand lived\n",
      "> The clever jackal' arrows for your man does.\" In each to him without being kings by what\n",
      "> The clever jackal continued, to him the sun.\" So for him what should live out — for\n",
      "> The clever jackal made upon delight he started off! Thus if.\" It made them not be over\n",
      "> The clever jackal is dead my own heart the CRis wisdom so, that is their place\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "valid loss: 5.0655\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "2 | lr: 2.52e-06 | loss: 3.9467 | norm: 0.00 | time: 437.88ms | tokens processed: 1,572,864 | tok/sec: 1,197,328.58\n",
      "3 | lr: 3.36e-06 | loss: 3.9245 | norm: 0.00 | time: 398.55ms | tokens processed: 2,097,152 | tok/sec: 1,315,474.37\n",
      "4 | lr: 4.20e-06 | loss: 3.9285 | norm: 0.00 | time: 407.86ms | tokens processed: 2,621,440 | tok/sec: 1,285,465.08\n",
      "5 | lr: 5.03e-06 | loss: 3.9255 | norm: 0.00 | time: 935.86ms | tokens processed: 3,145,728 | tok/sec: 560,220.31\n",
      "6 | lr: 5.87e-06 | loss: 3.9301 | norm: 0.00 | time: 409.03ms | tokens processed: 3,670,016 | tok/sec: 1,281,776.38\n",
      "7 | lr: 6.71e-06 | loss: 3.9459 | norm: 0.00 | time: 402.78ms | tokens processed: 4,194,304 | tok/sec: 1,301,672.42\n",
      "8 | lr: 7.55e-06 | loss: 3.9204 | norm: 0.00 | time: 405.01ms | tokens processed: 4,718,592 | tok/sec: 1,294,497.97\n",
      "9 | lr: 8.39e-06 | loss: 3.9268 | norm: 0.00 | time: 408.25ms | tokens processed: 5,242,880 | tok/sec: 1,284,225.66\n",
      "10 | lr: 9.23e-06 | loss: 3.9166 | norm: 0.00 | time: 400.75ms | tokens processed: 5,767,168 | tok/sec: 1,308,255.58\n",
      "11 | lr: 1.01e-05 | loss: 3.9417 | norm: 0.00 | time: 399.72ms | tokens processed: 6,291,456 | tok/sec: 1,311,622.65\n",
      "12 | lr: 1.09e-05 | loss: 3.9206 | norm: 0.00 | time: 405.48ms | tokens processed: 6,815,744 | tok/sec: 1,293,006.87\n",
      "13 | lr: 1.17e-05 | loss: 3.9177 | norm: 0.00 | time: 401.74ms | tokens processed: 7,340,032 | tok/sec: 1,305,052.12\n",
      "14 | lr: 1.26e-05 | loss: 3.9202 | norm: 0.00 | time: 402.81ms | tokens processed: 7,864,320 | tok/sec: 1,301,568.41\n",
      "15 | lr: 1.34e-05 | loss: 3.9136 | norm: 0.00 | time: 401.56ms | tokens processed: 8,388,608 | tok/sec: 1,305,629.38\n",
      "16 | lr: 1.43e-05 | loss: 3.9318 | norm: 0.00 | time: 399.23ms | tokens processed: 8,912,896 | tok/sec: 1,313,251.91\n",
      "17 | lr: 1.51e-05 | loss: 3.9102 | norm: 0.00 | time: 399.88ms | tokens processed: 9,437,184 | tok/sec: 1,311,104.96\n",
      "18 | lr: 1.59e-05 | loss: 3.9109 | norm: 0.00 | time: 407.39ms | tokens processed: 9,961,472 | tok/sec: 1,286,955.39\n",
      "19 | lr: 1.68e-05 | loss: 3.9071 | norm: 0.00 | time: 404.49ms | tokens processed: 10,485,760 | tok/sec: 1,296,182.73\n",
      "20 | lr: 1.76e-05 | loss: 3.9080 | norm: 0.00 | time: 399.02ms | tokens processed: 11,010,048 | tok/sec: 1,313,925.16\n",
      "21 | lr: 1.85e-05 | loss: 3.9240 | norm: 0.00 | time: 403.09ms | tokens processed: 11,534,336 | tok/sec: 1,300,666.15\n",
      "22 | lr: 1.93e-05 | loss: 3.8975 | norm: 0.00 | time: 398.24ms | tokens processed: 12,058,624 | tok/sec: 1,316,515.51\n",
      "23 | lr: 2.01e-05 | loss: 3.9016 | norm: 0.00 | time: 411.63ms | tokens processed: 12,582,912 | tok/sec: 1,273,681.90\n",
      "24 | lr: 2.10e-05 | loss: 3.8915 | norm: 0.00 | time: 408.70ms | tokens processed: 13,107,200 | tok/sec: 1,282,820.22\n",
      "25 | lr: 2.18e-05 | loss: 3.9145 | norm: 0.00 | time: 405.48ms | tokens processed: 13,631,488 | tok/sec: 1,292,999.27\n",
      "26 | lr: 2.27e-05 | loss: 3.8927 | norm: 0.00 | time: 411.40ms | tokens processed: 14,155,776 | tok/sec: 1,274,388.29\n",
      "27 | lr: 2.35e-05 | loss: 3.8900 | norm: 0.00 | time: 400.39ms | tokens processed: 14,680,064 | tok/sec: 1,309,443.59\n",
      "28 | lr: 2.43e-05 | loss: 3.8857 | norm: 0.00 | time: 410.80ms | tokens processed: 15,204,352 | tok/sec: 1,276,252.13\n",
      "29 | lr: 2.52e-05 | loss: 3.8815 | norm: 0.00 | time: 406.35ms | tokens processed: 15,728,640 | tok/sec: 1,290,229.48\n",
      "30 | lr: 2.60e-05 | loss: 3.8980 | norm: 0.00 | time: 404.53ms | tokens processed: 16,252,928 | tok/sec: 1,296,054.39\n",
      "31 | lr: 2.69e-05 | loss: 3.8747 | norm: 0.00 | time: 408.17ms | tokens processed: 16,777,216 | tok/sec: 1,284,486.70\n",
      "32 | lr: 2.77e-05 | loss: 3.8754 | norm: 0.00 | time: 404.88ms | tokens processed: 17,301,504 | tok/sec: 1,294,927.14\n",
      "33 | lr: 2.85e-05 | loss: 3.8683 | norm: 0.00 | time: 402.51ms | tokens processed: 17,825,792 | tok/sec: 1,302,551.38\n",
      "34 | lr: 2.94e-05 | loss: 3.8702 | norm: 0.00 | time: 404.65ms | tokens processed: 18,350,080 | tok/sec: 1,295,665.70\n",
      "35 | lr: 3.02e-05 | loss: 3.8837 | norm: 0.00 | time: 401.76ms | tokens processed: 18,874,368 | tok/sec: 1,304,980.86\n",
      "36 | lr: 3.10e-05 | loss: 3.8571 | norm: 0.00 | time: 402.44ms | tokens processed: 19,398,656 | tok/sec: 1,302,786.75\n",
      "37 | lr: 3.19e-05 | loss: 3.8593 | norm: 0.00 | time: 404.57ms | tokens processed: 19,922,944 | tok/sec: 1,295,928.36\n",
      "38 | lr: 3.27e-05 | loss: 3.8480 | norm: 0.00 | time: 405.64ms | tokens processed: 20,447,232 | tok/sec: 1,292,504.53\n",
      "39 | lr: 3.36e-05 | loss: 3.8695 | norm: 0.00 | time: 407.44ms | tokens processed: 20,971,520 | tok/sec: 1,286,789.72\n",
      "40 | lr: 3.44e-05 | loss: 3.8467 | norm: 0.00 | time: 409.21ms | tokens processed: 21,495,808 | tok/sec: 1,281,223.00\n",
      "41 | lr: 3.52e-05 | loss: 3.8424 | norm: 0.00 | time: 407.59ms | tokens processed: 22,020,096 | tok/sec: 1,286,304.98\n",
      "42 | lr: 3.61e-05 | loss: 3.8379 | norm: 0.00 | time: 401.35ms | tokens processed: 22,544,384 | tok/sec: 1,306,323.55\n",
      "43 | lr: 3.69e-05 | loss: 3.8325 | norm: 0.00 | time: 415.74ms | tokens processed: 23,068,672 | tok/sec: 1,261,086.44\n",
      "44 | lr: 3.78e-05 | loss: 3.8471 | norm: 0.00 | time: 409.15ms | tokens processed: 23,592,960 | tok/sec: 1,281,417.86\n",
      "45 | lr: 3.86e-05 | loss: 3.8227 | norm: 0.00 | time: 408.50ms | tokens processed: 24,117,248 | tok/sec: 1,283,436.41\n",
      "46 | lr: 3.94e-05 | loss: 3.8219 | norm: 0.00 | time: 402.36ms | tokens processed: 24,641,536 | tok/sec: 1,303,029.92\n",
      "47 | lr: 4.03e-05 | loss: 3.8130 | norm: 0.00 | time: 411.88ms | tokens processed: 25,165,824 | tok/sec: 1,272,917.34\n",
      "48 | lr: 4.11e-05 | loss: 3.8154 | norm: 0.00 | time: 404.55ms | tokens processed: 25,690,112 | tok/sec: 1,295,987.17\n",
      "49 | lr: 4.20e-05 | loss: 3.8263 | norm: 0.00 | time: 405.44ms | tokens processed: 26,214,400 | tok/sec: 1,293,124.73\n",
      "50 | lr: 4.28e-05 | loss: 3.8000 | norm: 0.00 | time: 402.37ms | tokens processed: 26,738,688 | tok/sec: 1,303,014.47\n",
      "51 | lr: 4.36e-05 | loss: 3.8003 | norm: 0.00 | time: 404.92ms | tokens processed: 27,262,976 | tok/sec: 1,294,808.20\n",
      "52 | lr: 4.45e-05 | loss: 3.7884 | norm: 0.00 | time: 411.77ms | tokens processed: 27,787,264 | tok/sec: 1,273,260.80\n",
      "53 | lr: 4.53e-05 | loss: 3.8099 | norm: 0.00 | time: 402.47ms | tokens processed: 28,311,552 | tok/sec: 1,302,691.82\n",
      "54 | lr: 4.62e-05 | loss: 3.7874 | norm: 0.00 | time: 410.01ms | tokens processed: 28,835,840 | tok/sec: 1,278,718.23\n",
      "55 | lr: 4.70e-05 | loss: 3.7819 | norm: 0.00 | time: 404.95ms | tokens processed: 29,360,128 | tok/sec: 1,294,696.89\n",
      "56 | lr: 4.78e-05 | loss: 3.7768 | norm: 0.00 | time: 410.17ms | tokens processed: 29,884,416 | tok/sec: 1,278,226.18\n",
      "57 | lr: 4.87e-05 | loss: 3.7693 | norm: 0.00 | time: 483.18ms | tokens processed: 30,408,704 | tok/sec: 1,085,082.74\n",
      "58 | lr: 4.95e-05 | loss: 3.7847 | norm: 0.00 | time: 402.89ms | tokens processed: 30,932,992 | tok/sec: 1,301,329.64\n",
      "59 | lr: 5.03e-05 | loss: 3.7600 | norm: 0.00 | time: 402.21ms | tokens processed: 31,457,280 | tok/sec: 1,303,511.12\n",
      "60 | lr: 5.12e-05 | loss: 3.7581 | norm: 0.00 | time: 402.50ms | tokens processed: 31,981,568 | tok/sec: 1,302,568.36\n",
      "61 | lr: 5.20e-05 | loss: 3.7484 | norm: 0.00 | time: 404.40ms | tokens processed: 32,505,856 | tok/sec: 1,296,457.83\n",
      "62 | lr: 5.29e-05 | loss: 3.7485 | norm: 0.00 | time: 403.07ms | tokens processed: 33,030,144 | tok/sec: 1,300,742.32\n",
      "63 | lr: 5.37e-05 | loss: 3.7612 | norm: 0.00 | time: 404.89ms | tokens processed: 33,554,432 | tok/sec: 1,294,876.81\n",
      "64 | lr: 5.45e-05 | loss: 3.7336 | norm: 0.00 | time: 399.70ms | tokens processed: 34,078,720 | tok/sec: 1,311,718.10\n",
      "65 | lr: 5.54e-05 | loss: 3.7358 | norm: 0.00 | time: 402.62ms | tokens processed: 34,603,008 | tok/sec: 1,302,199.66\n",
      "66 | lr: 5.62e-05 | loss: 3.7226 | norm: 0.00 | time: 401.03ms | tokens processed: 35,127,296 | tok/sec: 1,307,358.02\n",
      "67 | lr: 5.71e-05 | loss: 3.7402 | norm: 0.00 | time: 399.69ms | tokens processed: 35,651,584 | tok/sec: 1,311,720.45\n",
      "68 | lr: 5.79e-05 | loss: 3.7200 | norm: 0.00 | time: 399.95ms | tokens processed: 36,175,872 | tok/sec: 1,310,868.14\n",
      "69 | lr: 5.87e-05 | loss: 3.7132 | norm: 0.00 | time: 400.25ms | tokens processed: 36,700,160 | tok/sec: 1,309,895.21\n",
      "70 | lr: 5.96e-05 | loss: 3.7077 | norm: 0.00 | time: 405.34ms | tokens processed: 37,224,448 | tok/sec: 1,293,439.62\n",
      "71 | lr: 6.04e-05 | loss: 3.6988 | norm: 0.00 | time: 405.26ms | tokens processed: 37,748,736 | tok/sec: 1,293,706.71\n",
      "72 | lr: 6.13e-05 | loss: 3.7146 | norm: 0.00 | time: 403.60ms | tokens processed: 38,273,024 | tok/sec: 1,299,044.16\n",
      "73 | lr: 6.21e-05 | loss: 3.6896 | norm: 0.00 | time: 403.02ms | tokens processed: 38,797,312 | tok/sec: 1,300,900.83\n",
      "74 | lr: 6.29e-05 | loss: 3.6876 | norm: 0.00 | time: 402.86ms | tokens processed: 39,321,600 | tok/sec: 1,301,407.42\n",
      "75 | lr: 6.38e-05 | loss: 3.6791 | norm: 0.00 | time: 398.70ms | tokens processed: 39,845,888 | tok/sec: 1,314,989.80\n",
      "76 | lr: 6.46e-05 | loss: 3.6765 | norm: 0.00 | time: 400.83ms | tokens processed: 40,370,176 | tok/sec: 1,307,991.01\n",
      "77 | lr: 6.55e-05 | loss: 3.6896 | norm: 0.00 | time: 405.10ms | tokens processed: 40,894,464 | tok/sec: 1,294,234.36\n",
      "78 | lr: 6.63e-05 | loss: 3.6622 | norm: 0.00 | time: 401.69ms | tokens processed: 41,418,752 | tok/sec: 1,305,196.96\n",
      "79 | lr: 6.71e-05 | loss: 3.6618 | norm: 0.00 | time: 408.27ms | tokens processed: 41,943,040 | tok/sec: 1,284,158.16\n",
      "80 | lr: 6.80e-05 | loss: 3.6486 | norm: 0.00 | time: 403.86ms | tokens processed: 42,467,328 | tok/sec: 1,298,189.08\n",
      "81 | lr: 6.88e-05 | loss: 3.6654 | norm: 0.00 | time: 416.07ms | tokens processed: 42,991,616 | tok/sec: 1,260,086.32\n",
      "82 | lr: 6.97e-05 | loss: 3.6478 | norm: 0.00 | time: 408.50ms | tokens processed: 43,515,904 | tok/sec: 1,283,438.65\n",
      "83 | lr: 7.05e-05 | loss: 3.6385 | norm: 0.00 | time: 404.98ms | tokens processed: 44,040,192 | tok/sec: 1,294,601.62\n",
      "84 | lr: 7.13e-05 | loss: 3.6345 | norm: 0.00 | time: 409.17ms | tokens processed: 44,564,480 | tok/sec: 1,281,339.46\n",
      "85 | lr: 7.22e-05 | loss: 3.6244 | norm: 0.00 | time: 405.80ms | tokens processed: 45,088,768 | tok/sec: 1,291,998.77\n",
      "86 | lr: 7.30e-05 | loss: 3.6380 | norm: 0.00 | time: 407.24ms | tokens processed: 45,613,056 | tok/sec: 1,287,430.07\n",
      "87 | lr: 7.38e-05 | loss: 3.6150 | norm: 0.00 | time: 410.65ms | tokens processed: 46,137,344 | tok/sec: 1,276,718.94\n",
      "88 | lr: 7.47e-05 | loss: 3.6135 | norm: 0.00 | time: 416.95ms | tokens processed: 46,661,632 | tok/sec: 1,257,438.34\n",
      "89 | lr: 7.55e-05 | loss: 3.6032 | norm: 0.00 | time: 413.90ms | tokens processed: 47,185,920 | tok/sec: 1,266,693.66\n",
      "90 | lr: 7.64e-05 | loss: 3.6007 | norm: 0.00 | time: 412.87ms | tokens processed: 47,710,208 | tok/sec: 1,269,859.47\n",
      "91 | lr: 7.72e-05 | loss: 3.6131 | norm: 0.00 | time: 403.86ms | tokens processed: 48,234,496 | tok/sec: 1,298,176.82\n",
      "92 | lr: 7.80e-05 | loss: 3.5887 | norm: 0.00 | time: 407.62ms | tokens processed: 48,758,784 | tok/sec: 1,286,201.90\n",
      "93 | lr: 7.89e-05 | loss: 3.5864 | norm: 0.00 | time: 404.29ms | tokens processed: 49,283,072 | tok/sec: 1,296,799.58\n",
      "94 | lr: 7.97e-05 | loss: 3.5734 | norm: 0.00 | time: 412.41ms | tokens processed: 49,807,360 | tok/sec: 1,271,265.29\n",
      "95 | lr: 8.06e-05 | loss: 3.5872 | norm: 0.00 | time: 403.18ms | tokens processed: 50,331,648 | tok/sec: 1,300,375.42\n",
      "96 | lr: 8.14e-05 | loss: 3.5719 | norm: 0.00 | time: 408.13ms | tokens processed: 50,855,936 | tok/sec: 1,284,621.02\n",
      "97 | lr: 8.22e-05 | loss: 3.5610 | norm: 0.00 | time: 407.15ms | tokens processed: 51,380,224 | tok/sec: 1,287,692.42\n",
      "98 | lr: 8.31e-05 | loss: 3.5574 | norm: 0.00 | time: 411.27ms | tokens processed: 51,904,512 | tok/sec: 1,274,788.70\n",
      "99 | lr: 8.39e-05 | loss: 3.5475 | norm: 0.00 | time: 408.22ms | tokens processed: 52,428,800 | tok/sec: 1,284,314.16\n",
      "100 | lr: 8.48e-05 | loss: 3.5619 | norm: 0.00 | time: 414.57ms | tokens processed: 52,953,088 | tok/sec: 1,264,669.21\n",
      "101 | lr: 8.56e-05 | loss: 3.5385 | norm: 0.00 | time: 409.71ms | tokens processed: 53,477,376 | tok/sec: 1,279,669.96\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "> The clever jackal is a city who would fail you with the other than this cir's peaks\n",
      "> The clever jackal,\" will find repling: How about an iron? His head will put food\n",
      "> The clever jackal continued the world-To friends are a tree a word and my lord or a\n",
      "> The clever jackal.\" One befalls their wings like: yes for nothing more? One of friends\n",
      "> The clever jackal does wrong the saying : my s is an eye. Whereupon Theodore —\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "valid loss: 5.1180\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "102 | lr: 8.64e-05 | loss: 3.5358 | norm: 0.00 | time: 430.05ms | tokens processed: 54,001,664 | tok/sec: 1,219,120.11\n",
      "103 | lr: 8.73e-05 | loss: 3.5260 | norm: 0.00 | time: 403.40ms | tokens processed: 54,525,952 | tok/sec: 1,299,687.56\n",
      "104 | lr: 8.81e-05 | loss: 3.5233 | norm: 0.00 | time: 399.80ms | tokens processed: 55,050,240 | tok/sec: 1,311,377.83\n",
      "105 | lr: 8.90e-05 | loss: 3.5352 | norm: 0.00 | time: 398.13ms | tokens processed: 55,574,528 | tok/sec: 1,316,871.86\n",
      "106 | lr: 8.98e-05 | loss: 3.5089 | norm: 0.00 | time: 400.84ms | tokens processed: 56,098,816 | tok/sec: 1,307,970.78\n",
      "107 | lr: 9.06e-05 | loss: 3.5087 | norm: 0.00 | time: 411.32ms | tokens processed: 56,623,104 | tok/sec: 1,274,656.43\n",
      "108 | lr: 9.15e-05 | loss: 3.4975 | norm: 0.00 | time: 405.51ms | tokens processed: 57,147,392 | tok/sec: 1,292,923.25\n",
      "109 | lr: 9.23e-05 | loss: 3.5077 | norm: 0.00 | time: 666.46ms | tokens processed: 57,671,680 | tok/sec: 786,674.14\n",
      "110 | lr: 9.31e-05 | loss: 3.4974 | norm: 0.00 | time: 402.52ms | tokens processed: 58,195,968 | tok/sec: 1,302,506.64\n",
      "111 | lr: 9.40e-05 | loss: 3.4845 | norm: 0.00 | time: 415.78ms | tokens processed: 58,720,256 | tok/sec: 1,260,989.53\n",
      "112 | lr: 9.48e-05 | loss: 3.4811 | norm: 0.00 | time: 411.30ms | tokens processed: 59,244,544 | tok/sec: 1,274,706.68\n",
      "113 | lr: 9.57e-05 | loss: 3.4699 | norm: 0.00 | time: 405.60ms | tokens processed: 59,768,832 | tok/sec: 1,292,609.37\n",
      "114 | lr: 9.65e-05 | loss: 3.4848 | norm: 0.00 | time: 407.20ms | tokens processed: 60,293,120 | tok/sec: 1,287,550.68\n",
      "115 | lr: 9.73e-05 | loss: 3.4620 | norm: 0.00 | time: 410.21ms | tokens processed: 60,817,408 | tok/sec: 1,278,091.71\n",
      "116 | lr: 9.82e-05 | loss: 3.4588 | norm: 0.00 | time: 410.11ms | tokens processed: 61,341,696 | tok/sec: 1,278,420.88\n",
      "117 | lr: 9.90e-05 | loss: 3.4512 | norm: 0.00 | time: 408.59ms | tokens processed: 61,865,984 | tok/sec: 1,283,169.80\n",
      "118 | lr: 9.99e-05 | loss: 3.4454 | norm: 0.00 | time: 404.02ms | tokens processed: 62,390,272 | tok/sec: 1,297,664.32\n",
      "119 | lr: 1.01e-04 | loss: 3.4579 | norm: 0.00 | time: 401.73ms | tokens processed: 62,914,560 | tok/sec: 1,305,080.77\n",
      "120 | lr: 1.02e-04 | loss: 3.4349 | norm: 0.00 | time: 404.68ms | tokens processed: 63,438,848 | tok/sec: 1,295,552.72\n",
      "121 | lr: 1.02e-04 | loss: 3.4322 | norm: 0.00 | time: 413.03ms | tokens processed: 63,963,136 | tok/sec: 1,269,373.47\n",
      "122 | lr: 1.03e-04 | loss: 3.4230 | norm: 0.00 | time: 405.02ms | tokens processed: 64,487,424 | tok/sec: 1,294,462.16\n",
      "123 | lr: 1.04e-04 | loss: 3.4299 | norm: 0.00 | time: 404.25ms | tokens processed: 65,011,712 | tok/sec: 1,296,925.78\n",
      "124 | lr: 1.05e-04 | loss: 3.4220 | norm: 0.00 | time: 416.62ms | tokens processed: 65,536,000 | tok/sec: 1,258,427.05\n",
      "125 | lr: 1.06e-04 | loss: 3.4093 | norm: 0.00 | time: 412.97ms | tokens processed: 66,060,288 | tok/sec: 1,269,568.41\n",
      "126 | lr: 1.07e-04 | loss: 3.4070 | norm: 0.00 | time: 415.69ms | tokens processed: 66,584,576 | tok/sec: 1,261,232.54\n",
      "127 | lr: 1.07e-04 | loss: 3.3943 | norm: 0.00 | time: 414.14ms | tokens processed: 67,108,864 | tok/sec: 1,265,954.23\n",
      "128 | lr: 1.08e-04 | loss: 3.4098 | norm: 0.00 | time: 412.99ms | tokens processed: 67,633,152 | tok/sec: 1,269,505.38\n",
      "129 | lr: 1.09e-04 | loss: 3.3890 | norm: 0.00 | time: 410.99ms | tokens processed: 68,157,440 | tok/sec: 1,275,670.20\n",
      "130 | lr: 1.10e-04 | loss: 3.3847 | norm: 0.00 | time: 413.51ms | tokens processed: 68,681,728 | tok/sec: 1,267,902.38\n",
      "131 | lr: 1.11e-04 | loss: 3.3793 | norm: 0.00 | time: 409.81ms | tokens processed: 69,206,016 | tok/sec: 1,279,347.60\n",
      "132 | lr: 1.12e-04 | loss: 3.3728 | norm: 0.00 | time: 409.61ms | tokens processed: 69,730,304 | tok/sec: 1,279,957.47\n",
      "133 | lr: 1.12e-04 | loss: 3.3831 | norm: 0.00 | time: 414.98ms | tokens processed: 70,254,592 | tok/sec: 1,263,394.05\n",
      "134 | lr: 1.13e-04 | loss: 3.3631 | norm: 0.00 | time: 414.28ms | tokens processed: 70,778,880 | tok/sec: 1,265,550.60\n",
      "135 | lr: 1.14e-04 | loss: 3.3591 | norm: 0.00 | time: 413.41ms | tokens processed: 71,303,168 | tok/sec: 1,268,190.47\n",
      "136 | lr: 1.15e-04 | loss: 3.3522 | norm: 0.00 | time: 408.79ms | tokens processed: 71,827,456 | tok/sec: 1,282,529.18\n",
      "137 | lr: 1.16e-04 | loss: 3.3591 | norm: 0.00 | time: 413.98ms | tokens processed: 72,351,744 | tok/sec: 1,266,447.81\n",
      "138 | lr: 1.17e-04 | loss: 3.3507 | norm: 0.00 | time: 412.68ms | tokens processed: 72,876,032 | tok/sec: 1,270,436.83\n",
      "139 | lr: 1.17e-04 | loss: 3.3366 | norm: 0.00 | time: 406.24ms | tokens processed: 73,400,320 | tok/sec: 1,290,580.07\n",
      "140 | lr: 1.18e-04 | loss: 3.3351 | norm: 0.00 | time: 408.19ms | tokens processed: 73,924,608 | tok/sec: 1,284,409.43\n",
      "141 | lr: 1.19e-04 | loss: 3.3259 | norm: 0.00 | time: 416.00ms | tokens processed: 74,448,896 | tok/sec: 1,260,309.47\n",
      "142 | lr: 1.20e-04 | loss: 3.3386 | norm: 0.00 | time: 410.04ms | tokens processed: 74,973,184 | tok/sec: 1,278,630.50\n",
      "143 | lr: 1.21e-04 | loss: 3.3193 | norm: 0.00 | time: 411.14ms | tokens processed: 75,497,472 | tok/sec: 1,275,215.99\n",
      "144 | lr: 1.22e-04 | loss: 3.3131 | norm: 0.00 | time: 404.06ms | tokens processed: 76,021,760 | tok/sec: 1,297,558.65\n",
      "145 | lr: 1.23e-04 | loss: 3.3098 | norm: 0.00 | time: 410.77ms | tokens processed: 76,546,048 | tok/sec: 1,276,347.69\n",
      "146 | lr: 1.23e-04 | loss: 3.3035 | norm: 0.00 | time: 409.32ms | tokens processed: 77,070,336 | tok/sec: 1,280,886.43\n",
      "147 | lr: 1.24e-04 | loss: 3.3134 | norm: 0.00 | time: 411.85ms | tokens processed: 77,594,624 | tok/sec: 1,273,005.03\n",
      "148 | lr: 1.25e-04 | loss: 3.2915 | norm: 0.00 | time: 407.44ms | tokens processed: 78,118,912 | tok/sec: 1,286,785.20\n",
      "149 | lr: 1.26e-04 | loss: 3.2899 | norm: 0.00 | time: 403.12ms | tokens processed: 78,643,200 | tok/sec: 1,300,582.30\n",
      "150 | lr: 1.27e-04 | loss: 3.2824 | norm: 0.00 | time: 405.91ms | tokens processed: 79,167,488 | tok/sec: 1,291,620.86\n",
      "151 | lr: 1.28e-04 | loss: 3.2869 | norm: 0.00 | time: 413.41ms | tokens processed: 79,691,776 | tok/sec: 1,268,194.86\n",
      "152 | lr: 1.28e-04 | loss: 3.2851 | norm: 0.00 | time: 410.10ms | tokens processed: 80,216,064 | tok/sec: 1,278,440.94\n",
      "153 | lr: 1.29e-04 | loss: 3.2692 | norm: 0.00 | time: 409.40ms | tokens processed: 80,740,352 | tok/sec: 1,280,614.91\n",
      "154 | lr: 1.30e-04 | loss: 3.2682 | norm: 0.00 | time: 409.91ms | tokens processed: 81,264,640 | tok/sec: 1,279,021.68\n",
      "155 | lr: 1.31e-04 | loss: 3.2598 | norm: 0.00 | time: 409.58ms | tokens processed: 81,788,928 | tok/sec: 1,280,061.78\n",
      "156 | lr: 1.32e-04 | loss: 3.2765 | norm: 0.00 | time: 414.85ms | tokens processed: 82,313,216 | tok/sec: 1,263,810.83\n",
      "157 | lr: 1.33e-04 | loss: 3.2649 | norm: 0.00 | time: 413.37ms | tokens processed: 82,837,504 | tok/sec: 1,268,319.94\n",
      "158 | lr: 1.33e-04 | loss: 3.2587 | norm: 0.00 | time: 408.37ms | tokens processed: 83,361,792 | tok/sec: 1,283,846.28\n",
      "159 | lr: 1.34e-04 | loss: 3.2452 | norm: 0.00 | time: 405.70ms | tokens processed: 83,886,080 | tok/sec: 1,292,302.48\n",
      "160 | lr: 1.35e-04 | loss: 3.2491 | norm: 0.00 | time: 406.31ms | tokens processed: 84,410,368 | tok/sec: 1,290,357.43\n",
      "161 | lr: 1.36e-04 | loss: 3.2540 | norm: 0.00 | time: 416.22ms | tokens processed: 84,934,656 | tok/sec: 1,259,644.57\n",
      "162 | lr: 1.37e-04 | loss: 3.2340 | norm: 0.00 | time: 415.57ms | tokens processed: 85,458,944 | tok/sec: 1,261,616.77\n",
      "163 | lr: 1.38e-04 | loss: 3.2321 | norm: 0.00 | time: 416.64ms | tokens processed: 85,983,232 | tok/sec: 1,258,365.12\n",
      "164 | lr: 1.38e-04 | loss: 3.2242 | norm: 0.00 | time: 407.02ms | tokens processed: 86,507,520 | tok/sec: 1,288,098.22\n",
      "165 | lr: 1.39e-04 | loss: 3.2288 | norm: 0.00 | time: 794.15ms | tokens processed: 87,031,808 | tok/sec: 660,187.13\n",
      "166 | lr: 1.40e-04 | loss: 3.2257 | norm: 0.00 | time: 411.94ms | tokens processed: 87,556,096 | tok/sec: 1,272,730.95\n",
      "167 | lr: 1.41e-04 | loss: 3.2136 | norm: 0.00 | time: 419.33ms | tokens processed: 88,080,384 | tok/sec: 1,250,286.13\n",
      "168 | lr: 1.42e-04 | loss: 3.2090 | norm: 0.00 | time: 414.42ms | tokens processed: 88,604,672 | tok/sec: 1,265,104.29\n",
      "169 | lr: 1.43e-04 | loss: 3.2018 | norm: 0.00 | time: 412.36ms | tokens processed: 89,128,960 | tok/sec: 1,271,443.90\n",
      "170 | lr: 1.43e-04 | loss: 3.2129 | norm: 0.00 | time: 408.84ms | tokens processed: 89,653,248 | tok/sec: 1,282,375.86\n",
      "171 | lr: 1.44e-04 | loss: 3.1970 | norm: 0.00 | time: 411.40ms | tokens processed: 90,177,536 | tok/sec: 1,274,397.15\n",
      "172 | lr: 1.45e-04 | loss: 3.1923 | norm: 0.00 | time: 414.12ms | tokens processed: 90,701,824 | tok/sec: 1,266,030.02\n",
      "173 | lr: 1.46e-04 | loss: 3.1908 | norm: 0.00 | time: 414.10ms | tokens processed: 91,226,112 | tok/sec: 1,266,095.63\n",
      "174 | lr: 1.47e-04 | loss: 3.1834 | norm: 0.00 | time: 412.31ms | tokens processed: 91,750,400 | tok/sec: 1,271,578.44\n",
      "175 | lr: 1.48e-04 | loss: 3.1950 | norm: 0.00 | time: 415.55ms | tokens processed: 92,274,688 | tok/sec: 1,261,678.29\n",
      "176 | lr: 1.49e-04 | loss: 3.1743 | norm: 0.00 | time: 412.84ms | tokens processed: 92,798,976 | tok/sec: 1,269,940.13\n",
      "177 | lr: 1.49e-04 | loss: 3.1756 | norm: 0.00 | time: 406.93ms | tokens processed: 93,323,264 | tok/sec: 1,288,386.51\n",
      "178 | lr: 1.50e-04 | loss: 3.1665 | norm: 0.00 | time: 411.21ms | tokens processed: 93,847,552 | tok/sec: 1,274,983.09\n",
      "179 | lr: 1.51e-04 | loss: 3.1692 | norm: 0.00 | time: 414.56ms | tokens processed: 94,371,840 | tok/sec: 1,264,680.84\n",
      "180 | lr: 1.52e-04 | loss: 3.1710 | norm: 0.00 | time: 415.01ms | tokens processed: 94,896,128 | tok/sec: 1,263,304.78\n",
      "181 | lr: 1.53e-04 | loss: 3.1566 | norm: 0.00 | time: 409.28ms | tokens processed: 95,420,416 | tok/sec: 1,280,997.60\n",
      "182 | lr: 1.54e-04 | loss: 3.1562 | norm: 0.00 | time: 416.04ms | tokens processed: 95,944,704 | tok/sec: 1,260,195.36\n",
      "183 | lr: 1.54e-04 | loss: 3.1481 | norm: 0.00 | time: 410.86ms | tokens processed: 96,468,992 | tok/sec: 1,276,078.83\n",
      "184 | lr: 1.55e-04 | loss: 3.1584 | norm: 0.00 | time: 412.13ms | tokens processed: 96,993,280 | tok/sec: 1,272,144.87\n",
      "185 | lr: 1.56e-04 | loss: 3.1442 | norm: 0.00 | time: 412.75ms | tokens processed: 97,517,568 | tok/sec: 1,270,232.82\n",
      "186 | lr: 1.57e-04 | loss: 3.1428 | norm: 0.00 | time: 414.11ms | tokens processed: 98,041,856 | tok/sec: 1,266,063.55\n",
      "187 | lr: 1.58e-04 | loss: 3.1412 | norm: 0.00 | time: 411.08ms | tokens processed: 98,566,144 | tok/sec: 1,275,403.11\n",
      "188 | lr: 1.59e-04 | loss: 3.1357 | norm: 0.00 | time: 413.58ms | tokens processed: 99,090,432 | tok/sec: 1,267,681.64\n",
      "189 | lr: 1.59e-04 | loss: 3.1425 | norm: 0.00 | time: 417.18ms | tokens processed: 99,614,720 | tok/sec: 1,256,749.17\n",
      "Received KeyboardInterrupt. Exiting...\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "> The clever jackal told the other.\" He has starvedlessness to eat at that money a long home\n",
      "> The clever jackal Spl are we will the verse of them his hand,\" cried beside for ill who\n",
      "> The clever jackal what can the jackound him in this address: At sunset sound a mouse.\n",
      "> The clever jackal had them? Fate gave a well ! It has done prucum\n",
      "> The clever jackal does and then said Gold returned to death he found by a fortress. Who cannot\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "valid loss: 5.4053\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "CPU times: user 1min 11s, sys: 10.4 s, total: 1min 22s\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from time import time\n",
    "from IPython.display import clear_output\n",
    "from functools import partial\n",
    "\n",
    "evaluate(m)\n",
    "m.train()\n",
    "\n",
    "try:\n",
    "  for step in range(max_steps):\n",
    "   \n",
    "    #batches, targets = [], []\n",
    "    #for i in range(grad_accumulation_steps):\n",
    "    start = time()\n",
    "    batch, target = train_dl()\n",
    "      #batches.append(batch)\n",
    "      #targets.append(target)\n",
    "\n",
    "    avg_loss, avg_grads = parallel_train_step(m, optimizer, batch, target)\n",
    "    avg_loss.block_until_ready()\n",
    "    # compute stats\n",
    "    loss = avg_loss[0]\n",
    "    lr = warmup_with_cosine_decay_schedule(step)\n",
    "    norm = 0 # norm[0]|\n",
    "    iter_time = time() - start\n",
    "    sub_step_time = iter_time / grad_accumulation_steps\n",
    "    tokens_per_sec = num_devices * mB * T * grad_accumulation_steps / iter_time\n",
    "    tokens_processed = (step+1) * num_devices * grad_accumulation_steps * mB * T\n",
    "\n",
    "    if step % print_interval == 0:\n",
    "        print(f\"{step} | lr: {lr:0.2e} | loss: {loss:0.4f} | norm: {norm:0.2f} | time: {iter_time*1000:0.2f}ms | tokens processed: {tokens_processed:,} | tok/sec: {tokens_per_sec:,.2f}\")\n",
    "    if step % eval_interval == 1:\n",
    "      evaluate(m)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Received KeyboardInterrupt. Exiting...\")\n",
    "evaluate(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.13 (jaxpt)",
   "language": "python",
   "name": "jaxpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
