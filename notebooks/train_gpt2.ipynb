{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXU2qDN8nMgg"
      },
      "source": [
        "# Let's Train GPT-2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNLo9jfLn8bg",
        "outputId": "a02838af-6f40-4ae9-b15a-b114043ce9d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'jaxpt'...\n",
            "remote: Enumerating objects: 286, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 286 (delta 57), reused 36 (delta 33), pack-reused 201 (from 1)\u001b[K\n",
            "Receiving objects: 100% (286/286), 724.75 KiB | 5.10 MiB/s, done.\n",
            "Resolving deltas: 100% (171/171), done.\n",
            "Branch 'dev' set up to track remote branch 'dev' from 'origin'.\n",
            "Switched to a new branch 'dev'\n",
            "Already up to date.\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if is_colab():\n",
        "    !git clone https://github.com/novastar53/jaxpt\n",
        "    !cd jaxpt && git checkout dev && git pull\n",
        "    !pip install tiktoken --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQl8dEgLnMgh",
        "outputId": "2353a1a7-340c-46d7-8480-9ccefb225426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jaxpt/src\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "if is_colab():\n",
        "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"src\" )\n",
        "else:\n",
        "    jaxpt_dir = str(Path().absolute().parent / \"src\" )\n",
        "\n",
        "sys.path.append(jaxpt_dir)\n",
        "print(jaxpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7N2-jnzonMgh",
        "outputId": "ddc08bed-6d00-4c62-fd43-6cd0d3ae4911",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from flax import nnx\n",
        "import tiktoken\n",
        "\n",
        "from jaxpt.dataloaders import DataLoader\n",
        "from jaxpt.models import GPT2, GPTConfig\n",
        "from jaxpt.train import accum_step, loss_fn, compute_global_norm\n",
        "from jaxpt.infer import generate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04pFw2g58HJl"
      },
      "source": [
        "### Configure compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJo6Xji39g54",
        "outputId": "f5e9c28d-fd5e-43d5-cd70-8db716d0edd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version: 0.4.33\n",
            "Available devices: 8\n",
            "Device: TPU_0(process=0,(0,0,0,0)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_1(process=0,(0,0,0,1)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_2(process=0,(1,0,0,0)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_3(process=0,(1,0,0,1)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_4(process=0,(0,1,0,0)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_5(process=0,(0,1,0,1)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_6(process=0,(1,1,0,0)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_7(process=0,(1,1,0,1)), Memory: 7661.984375,  Used: 0.015625\n",
            "Using device: tpu\n",
            "6.85 ms ± 80.3 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Hardware setup\n",
        "print(\"JAX version:\", jax.__version__)\n",
        "devices = jax.devices()\n",
        "num_devices = len(devices)\n",
        "print(\"Available devices:\", num_devices)\n",
        "\n",
        "jax.config.update(\"jax_platform_name\", \"gpu\") # Make sure we're using the GPU\n",
        "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
        "jax.config.update(\"jax_default_matmul_precision\", \"bfloat16\") # Set the default precision for matrix multiplication\n",
        "\n",
        "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
        "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
        "\n",
        "def list_tpu_memory():\n",
        "    devices = jax.devices()\n",
        "    for device in devices:\n",
        "        if 'TPU' in str(device.device_kind):\n",
        "            print(f\"Device: {device}, Memory: {device.memory_stats()['bytes_limit']/(1024*1024)},  Used: {device.memory_stats()['bytes_in_use']/(1024*1024)}\")\n",
        "\n",
        "list_tpu_memory()\n",
        "\n",
        "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
        "\n",
        "A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Makes sure the matmul is fast\n",
        "\n",
        "%timeit (A@A).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzoCvstr9_WX"
      },
      "source": [
        "### Initialize the GPT-2 model and perform a sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lki2khsFnMgh",
        "outputId": "bdeba1c1-fcda-4be1-aa8b-5fcf0b80bdea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> The clever jackal bulldo HumaOil marketed MRreview insurgIND DevOnline brightest chick terminustainable Musk manaefeated\n",
            "> The clever jackal Journalichi incrediblyaughters Blastermaking DPRK Advance herdbull392 hate Puzz letters animateaten\n",
            "> The clever jackal brew respondenteverythingIndeed Industrial justifies Tin intriguing js Personally therapiesParameter Skull restricts indifference jur\n",
            "> The clever jackal Donkey Compet pursuit Sol qualifiescheckigiousosponsors gut Set busted dumpRequ 432efficiency oriented\n",
            "> The clever jackal injustice connectorsGod dropping WASRankchart241 />Open scandal sideadden Bra Scientology promise\n"
          ]
        }
      ],
      "source": [
        "from functools import partial\n",
        "\n",
        "\"\"\"\n",
        "+--------------+---------+--------+------+\n",
        "| Model        | Layers  | Heads  | Embd |\n",
        "+--------------+---------+--------+------+\n",
        "| gpt2-medium  | 24      | 16     | 1024 |\n",
        "| gpt2-large   | 36      | 20     | 1280 |\n",
        "| gpt2-xl      | 48      | 25     | 1600 |\n",
        "+--------------+---------+--------+------+\n",
        "\"\"\"\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "rngs = nnx.Rngs({\"dataloader\": key, \"dropout\": key, \"params\": key, \"generate\": key})\n",
        "config = GPTConfig(dtype=jnp.float32)\n",
        "m = GPT2(config, rngs)\n",
        "\n",
        "def generate_completions():\n",
        "  m.eval()\n",
        "  num_completions = 5\n",
        "  max_length = 20\n",
        "  generate_completion = partial(generate, m, max_length=max_length)\n",
        "  prefix = \"The clever jackal\"\n",
        "  enc = tiktoken.get_encoding('gpt2')\n",
        "  tokens = enc.encode(prefix)\n",
        "  tokens = jnp.array(tokens, dtype=jnp.int32)\n",
        "  tokens = jnp.expand_dims(tokens, axis=0)\n",
        "  x = jnp.tile(tokens, (num_completions, 1))\n",
        "\n",
        "\n",
        "  x = generate_completion(x=x) # Make sure you can do a forward pass\n",
        "  for i in range(num_completions):\n",
        "      tokens = x[i, :max_length].tolist()\n",
        "      decoded = enc.decode(tokens)\n",
        "      print(\">\", decoded)\n",
        "\n",
        "generate_completions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHA3hbjj8HJl"
      },
      "source": [
        "### Training setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8Dx19aKnMgi",
        "outputId": "e476331f-72b8-43ed-dd53-52153260e6fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens/batch: 32,768\n",
            "block size: 256\n",
            "sub-batch size: 8\n",
            "no. gradient accumulation steps: 2\n",
            "effective batch size per device:  16\n",
            "effective batch size: 128\n",
            "max steps: 100\n",
            "weight decay param count: 124,354,560\n"
          ]
        }
      ],
      "source": [
        "num_tokens_per_batch = 2**15 # 2**19, 0.5 million as per the GPT 3.5 paper\n",
        "mB, T = 8, 256\n",
        "grad_accumulation_steps = num_tokens_per_batch // (mB * T * num_devices) # Number of steps over which to average the gradient\n",
        "print(f\"tokens/batch: {num_tokens_per_batch:,}\")\n",
        "print(f\"block size: {T}\")\n",
        "print(f\"sub-batch size: {mB}\")\n",
        "print(f\"no. gradient accumulation steps: {grad_accumulation_steps}\")\n",
        "print(f\"effective batch size per device: \", grad_accumulation_steps * mB)\n",
        "print(f\"effective batch size: {grad_accumulation_steps * mB * num_devices}\")\n",
        "\n",
        "\n",
        "max_steps = 100\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "eval_interval = 10\n",
        "\n",
        "print(f\"max steps: {max_steps}\")\n",
        "\n",
        "# Set up the optimizer\n",
        "def warmup_with_cosine_decay_schedule(step):\n",
        "\n",
        "    warmup_lr = max_lr * (step + 1) / warmup_steps\n",
        "\n",
        "    coeff = 0.5 * (1 + jnp.cos(jnp.pi * (step - warmup_steps) / (max_steps - warmup_steps)))\n",
        "    cosine_lr =  min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "    return jnp.where(step < warmup_steps,\n",
        "                     warmup_lr,\n",
        "                     jnp.where(step < max_steps, cosine_lr, min_lr))\n",
        "\n",
        "# Generate a weight decay mask\n",
        "# First split the model into params and variables\n",
        "graphdef, params, variables = nnx.split(m, nnx.Param, nnx.Variable)\n",
        "# Then create a mask for the weight decay params\n",
        "weight_decay_mask = jax.tree_util.tree_map(lambda x: len(x.shape) > 1, params)\n",
        "\n",
        "def f(x, y):\n",
        "    if x:\n",
        "        return y.size\n",
        "    return 0\n",
        "\n",
        "weight_decay_params = jax.tree_util.tree_map(f, weight_decay_mask, params)\n",
        "weight_decay_param_count = jax.tree_util.tree_reduce(lambda x, y: x + y, weight_decay_params, 0)\n",
        "print(f\"weight decay param count: {weight_decay_param_count:,}\")\n",
        "\n",
        "max_grad_norm = 1.0  # Clip gradients to this norm\n",
        "\n",
        "tx = optax.chain(\n",
        "    optax.clip_by_global_norm(max_grad_norm),\n",
        "    optax.adamw(warmup_with_cosine_decay_schedule, b1=0.9, b2=0.95, weight_decay=0.1, mask=weight_decay_mask)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_8d4oBtGEwpy"
      },
      "outputs": [],
      "source": [
        "if is_colab():\n",
        "    dataset_path = Path().absolute() / \"jaxpt\" / \"src\" / \"jaxpt\" / \"datasets\" / \"fineweb-edu\" / \"processed\"\n",
        "else:\n",
        "    dataset_path = Path().absolute().parent / \"src\"/ \"jaxpt\" / \"datasets\" / \"fineweb-edur\" / \"processed\"\n",
        "\n",
        "def evaluate(m):\n",
        "  print(\"----------\")\n",
        "  print(\"evaluation\")\n",
        "  print(\"----------\")\n",
        "  m.eval()\n",
        "  generate_completions()\n",
        "  print(\"----------\")\n",
        "  eval_dl = DataLoader(dirpath=dataset_path, batch_size=mB, block_size=T, device_rank=1) #, label=\"valid\")\n",
        "  total_iters = (len(eval_dl) * eval_dl.shard_size) // (mB * T)\n",
        "  valid_loss = 0.0\n",
        "  for i in range(total_iters):\n",
        "    batch, targets = eval_dl()\n",
        "    batch = np.squeeze(batch)\n",
        "    targets = np.squeeze(targets)\n",
        "    loss = loss_fn(m, batch, targets)\n",
        "    valid_loss += loss\n",
        "  valid_loss /= total_iters\n",
        "  print(f\"valid loss: {valid_loss:0.4f}\")\n",
        "  print(\"----------\")\n",
        "  m.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DataLoader(dirpath=dataset_path, batch_size=mB, block_size=T, device_rank=num_devices)"
      ],
      "metadata": {
        "id": "evc3QIonBnWC",
        "outputId": "6420415e-50b5-4f74-b642-6fcc368dfa9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/jaxpt/src/jaxpt/datasets/fineweb-edu/processed'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-ec0d2edc9f66>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_rank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_devices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/jaxpt/src/jaxpt/dataloaders.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dirpath, batch_size, block_size, device_rank, label)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice_rank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mshard\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mshard\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshards\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/jaxpt/src/jaxpt/datasets/fineweb-edu/processed'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwtmfUotuLMU",
        "outputId": "ef489771-fbe6-45e2-81ff-7daca3a3cf0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataloader initialized:\n",
            "------------------------\n",
            "shards:         1\n",
            "shard size:     163,085\n",
            "batch size:     8\n",
            "block size:     256\n",
            "device rank:    8\n",
            "------------------------\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal plugged ceilingsVersionLive beAnythingruction stabilization Via Thunom � Camp HL Centers Paraly\n",
            "> The clever jackal disturbancesidelity Amanda simplicity $Hit005 Boxing courtacialndum disclosuresYESanswered indicatorFH\n",
            "> The clever jackalLaughs excited Haas Mood 24efullytheless Wedoin TacticsCruDamnortal individualsDNA court\n",
            "> The clever jackal provisional alpha cowboy Indynt competitionsallahVol MEN billionaires chapter chapteridespreadregorfacingbite\n",
            "> The clever jackalIDs Wattsön cones LIM significantlyVal---------------------------------------------------------------- Raiders teachingsication teachings candidate Via 306 Enterprises\n",
            "----------\n",
            "dataloader initialized:\n",
            "------------------------\n",
            "shards:         1\n",
            "shard size:     163,085\n",
            "batch size:     8\n",
            "block size:     256\n",
            "device rank:    1\n",
            "------------------------\n",
            "valid loss: 10.9876\n",
            "----------\n",
            " step: 0 | lr: 6.00e-05 | loss: 10.9845 | norm: 7.3092 | time: 21007.40ms | tokens processed: 32,768 | tok/sec: 97.49\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal,姫og Buff PolNitromeographed    unique Rocket Buff» Silencealysed Conan curved\n",
            "> The clever jackalDEBUGArcade qualifier foresee» hepitud concluded272 concerns requiresortium barleyterror settlementcasting\n",
            "> The clever jackalLaughs loyal introductorycill Pink enormousCLASS ResultsalamThough foresee unfolds Pink enormous circus Laboratories\n",
            "> The clever jackal Ethan produce Typ bl pathogens184 throatsGETket» urallyowler attacking»\n",
            "> The clever jackalON姫 oust184 parks chains enclosure defy Mass punished filtersfighting姫 webcam ##    \n",
            "----------\n",
            "dataloader initialized:\n",
            "------------------------\n",
            "shards:         1\n",
            "shard size:     163,085\n",
            "batch size:     8\n",
            "block size:     256\n",
            "device rank:    1\n",
            "------------------------\n",
            "valid loss: 9.6198\n",
            "----------\n",
            " step: 1 | lr: 1.20e-04 | loss: 9.7640 | norm: 5.0693 | time: 3547.05ms | tokens processed: 65,536 | tok/sec: 577.38\n",
            " step: 2 | lr: 1.80e-04 | loss: 8.8047 | norm: 3.5608 | time: 2901.46ms | tokens processed: 98,304 | tok/sec: 705.85\n",
            " step: 3 | lr: 2.40e-04 | loss: 8.6192 | norm: 6.3253 | time: 2784.74ms | tokens processed: 131,072 | tok/sec: 735.44\n",
            " step: 4 | lr: 3.00e-04 | loss: 8.6634 | norm: 11.0754 | time: 2627.09ms | tokens processed: 163,840 | tok/sec: 779.57\n",
            " step: 5 | lr: 3.60e-04 | loss: 8.3217 | norm: 3.6088 | time: 2613.02ms | tokens processed: 196,608 | tok/sec: 783.77\n",
            " step: 6 | lr: 4.20e-04 | loss: 7.8317 | norm: 2.5897 | time: 2675.51ms | tokens processed: 229,376 | tok/sec: 765.46\n",
            " step: 7 | lr: 4.80e-04 | loss: 7.4633 | norm: 2.1725 | time: 2685.33ms | tokens processed: 262,144 | tok/sec: 762.66\n",
            " step: 8 | lr: 5.40e-04 | loss: 7.3349 | norm: 3.0059 | time: 2720.65ms | tokens processed: 294,912 | tok/sec: 752.76\n",
            " step: 9 | lr: 6.00e-04 | loss: 7.0049 | norm: 1.6887 | time: 2657.53ms | tokens processed: 327,680 | tok/sec: 770.64\n",
            " step: 10 | lr: 6.00e-04 | loss: 6.7543 | norm: 2.6135 | time: 2657.43ms | tokens processed: 360,448 | tok/sec: 770.67\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal\n",
            " said her it,\" is's nothat thought replied. no two his you\n",
            "> The clever jackal,\" from man his a when- two good when this his good to was it\n",
            "> The clever jackal.\". may story: ahethat this is man one thought man.\" \"\n",
            "> The clever jackal. thought A thoughtANT with two her one and  killed hehe:\n",
            "> The clever jackalhe to he:. when he, from my;;But —. to\n",
            "----------\n",
            "dataloader initialized:\n",
            "------------------------\n",
            "shards:         1\n",
            "shard size:     163,085\n",
            "batch size:     8\n",
            "block size:     256\n",
            "device rank:    1\n",
            "------------------------\n",
            "valid loss: 6.4483\n",
            "----------\n",
            " step: 11 | lr: 6.00e-04 | loss: 6.3983 | norm: 1.7440 | time: 2712.05ms | tokens processed: 393,216 | tok/sec: 755.15\n",
            " step: 12 | lr: 5.99e-04 | loss: 6.2705 | norm: 1.9042 | time: 2826.03ms | tokens processed: 425,984 | tok/sec: 724.69\n",
            " step: 13 | lr: 5.99e-04 | loss: 6.0834 | norm: 1.4717 | time: 2542.10ms | tokens processed: 458,752 | tok/sec: 805.63\n",
            " step: 14 | lr: 5.97e-04 | loss: 6.0786 | norm: 1.4026 | time: 2527.05ms | tokens processed: 491,520 | tok/sec: 810.43\n",
            " step: 15 | lr: 5.96e-04 | loss: 5.9634 | norm: 0.9884 | time: 2605.73ms | tokens processed: 524,288 | tok/sec: 785.96\n",
            " step: 16 | lr: 5.94e-04 | loss: 5.9004 | norm: 1.6977 | time: 2501.38ms | tokens processed: 557,056 | tok/sec: 818.75\n",
            " step: 17 | lr: 5.92e-04 | loss: 5.8205 | norm: 1.4649 | time: 2589.67ms | tokens processed: 589,824 | tok/sec: 790.83\n",
            " step: 18 | lr: 5.90e-04 | loss: 6.0051 | norm: 3.5323 | time: 2653.66ms | tokens processed: 622,592 | tok/sec: 771.77\n",
            " step: 19 | lr: 5.87e-04 | loss: 5.8752 | norm: 0.9532 | time: 2633.66ms | tokens processed: 655,360 | tok/sec: 777.62\n",
            " step: 20 | lr: 5.84e-04 | loss: 5.7978 | norm: 1.7215 | time: 2550.96ms | tokens processed: 688,128 | tok/sec: 802.83\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackalTheof the yourtoand.\" for on not your the; PAN you will\n",
            "> The clever jackalto, are a: at is?\" so they this his so, me so\n",
            "> The clever jackalIs\n",
            "That it I I PAN at this of are with him him all that\n",
            "> The clever jackalthe is\" heIt a THE \"Therethe the \"WhenHis\"\n",
            "> The clever jackalofandSoTHEAndWhileA\n",
            "But have for for you on- a\n",
            "----------\n",
            "dataloader initialized:\n",
            "------------------------\n",
            "shards:         1\n",
            "shard size:     163,085\n",
            "batch size:     8\n",
            "block size:     256\n",
            "device rank:    1\n",
            "------------------------\n",
            "valid loss: 5.8323\n",
            "----------\n",
            " step: 21 | lr: 5.80e-04 | loss: 5.8232 | norm: 2.1451 | time: 2584.61ms | tokens processed: 720,896 | tok/sec: 792.38\n",
            " step: 22 | lr: 5.77e-04 | loss: 5.7212 | norm: 0.7295 | time: 2562.73ms | tokens processed: 753,664 | tok/sec: 799.15\n",
            " step: 23 | lr: 5.73e-04 | loss: 5.8351 | norm: 1.5209 | time: 2754.90ms | tokens processed: 786,432 | tok/sec: 743.40\n",
            " step: 24 | lr: 5.68e-04 | loss: 5.7725 | norm: 1.8651 | time: 2526.02ms | tokens processed: 819,200 | tok/sec: 810.76\n",
            " step: 25 | lr: 5.64e-04 | loss: 5.7537 | norm: 1.1560 | time: 2541.96ms | tokens processed: 851,968 | tok/sec: 805.68\n",
            " step: 26 | lr: 5.59e-04 | loss: 5.6211 | norm: 0.9707 | time: 2570.40ms | tokens processed: 884,736 | tok/sec: 796.76\n",
            " step: 27 | lr: 5.54e-04 | loss: 5.7718 | norm: 1.6588 | time: 2659.10ms | tokens processed: 917,504 | tok/sec: 770.19\n",
            " step: 28 | lr: 5.48e-04 | loss: 5.6756 | norm: 0.8103 | time: 2542.28ms | tokens processed: 950,272 | tok/sec: 805.57\n",
            " step: 29 | lr: 5.43e-04 | loss: 5.6375 | norm: 1.5064 | time: 2647.04ms | tokens processed: 983,040 | tok/sec: 773.69\n",
            " step: 30 | lr: 5.37e-04 | loss: 5.6665 | norm: 1.6690 | time: 2619.61ms | tokens processed: 1,015,808 | tok/sec: 781.80\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal the no.\" man all he with for?\"AWith: that at this that\n",
            "> The clever jackal will her it:: be is LANTinc thisRA of not on\n",
            "> The clever jackal when \"LyouSo is?\" — him a \"of your's on his\n",
            "> The clever jackal and for the you \"withWhatThere a for- THE; from in\n",
            "> The clever jackal man a; in. be; Are \"you's I no and a\n",
            "----------\n",
            "dataloader initialized:\n",
            "------------------------\n",
            "shards:         1\n",
            "shard size:     163,085\n",
            "batch size:     8\n",
            "block size:     256\n",
            "device rank:    1\n",
            "------------------------\n",
            "valid loss: 5.6234\n",
            "----------\n",
            " step: 31 | lr: 5.31e-04 | loss: 5.5308 | norm: 1.1510 | time: 2554.03ms | tokens processed: 1,048,576 | tok/sec: 801.87\n",
            " step: 32 | lr: 5.24e-04 | loss: 5.6690 | norm: 1.5964 | time: 2474.79ms | tokens processed: 1,081,344 | tok/sec: 827.54\n",
            " step: 33 | lr: 5.18e-04 | loss: 5.6197 | norm: 1.6121 | time: 2491.99ms | tokens processed: 1,114,112 | tok/sec: 821.83\n",
            " step: 34 | lr: 5.11e-04 | loss: 5.5699 | norm: 0.9051 | time: 2847.43ms | tokens processed: 1,146,880 | tok/sec: 719.25\n",
            " step: 35 | lr: 5.04e-04 | loss: 5.4727 | norm: 0.8691 | time: 2484.13ms | tokens processed: 1,179,648 | tok/sec: 824.43\n",
            " step: 36 | lr: 4.96e-04 | loss: 5.6112 | norm: 1.1636 | time: 2583.00ms | tokens processed: 1,212,416 | tok/sec: 792.88\n",
            " step: 37 | lr: 4.89e-04 | loss: 5.5778 | norm: 0.7933 | time: 2667.92ms | tokens processed: 1,245,184 | tok/sec: 767.64\n",
            " step: 38 | lr: 4.81e-04 | loss: 5.5073 | norm: 0.7409 | time: 2734.93ms | tokens processed: 1,277,952 | tok/sec: 748.83\n",
            " step: 39 | lr: 4.73e-04 | loss: 5.5009 | norm: 0.5192 | time: 2675.43ms | tokens processed: 1,310,720 | tok/sec: 765.48\n",
            " step: 40 | lr: 4.65e-04 | loss: 5.4183 | norm: 0.9472 | time: 2629.23ms | tokens processed: 1,343,488 | tok/sec: 778.94\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal and then his For him to said by at for?\" \"THE her you with\n",
            "> The clever jackal who who not a And will:CHAT Where's that all he; was\n",
            "> The clever jackal on a on be you to all from was to \"a for by at you\n",
            "> The clever jackal I for Or's no of on in; \n",
            "\n",
            "to.\" this:\n",
            "> The clever jackal one a is you a? So it him who are are And at he he\n",
            "----------\n",
            "dataloader initialized:\n",
            "------------------------\n",
            "shards:         1\n",
            "shard size:     163,085\n",
            "batch size:     8\n",
            "block size:     256\n",
            "device rank:    1\n",
            "------------------------\n",
            "valid loss: 5.5235\n",
            "----------\n",
            " step: 41 | lr: 4.57e-04 | loss: 5.5460 | norm: 0.6785 | time: 2574.44ms | tokens processed: 1,376,256 | tok/sec: 795.51\n",
            " step: 42 | lr: 4.48e-04 | loss: 5.4912 | norm: 0.6635 | time: 2543.84ms | tokens processed: 1,409,024 | tok/sec: 805.08\n",
            " step: 43 | lr: 4.40e-04 | loss: 5.4361 | norm: 0.7261 | time: 2527.95ms | tokens processed: 1,441,792 | tok/sec: 810.14\n",
            " step: 44 | lr: 4.31e-04 | loss: 5.3517 | norm: 0.7407 | time: 2516.63ms | tokens processed: 1,474,560 | tok/sec: 813.79\n",
            " step: 45 | lr: 4.22e-04 | loss: 5.4800 | norm: 0.9425 | time: 2832.56ms | tokens processed: 1,507,328 | tok/sec: 723.02\n",
            " step: 46 | lr: 4.13e-04 | loss: 5.4453 | norm: 0.9228 | time: 2721.15ms | tokens processed: 1,540,096 | tok/sec: 752.62\n",
            " step: 47 | lr: 4.04e-04 | loss: 5.3609 | norm: 0.6270 | time: 2699.27ms | tokens processed: 1,572,864 | tok/sec: 758.72\n",
            " step: 48 | lr: 3.95e-04 | loss: 5.3732 | norm: 0.6815 | time: 2672.05ms | tokens processed: 1,605,632 | tok/sec: 766.45\n",
            " step: 49 | lr: 3.86e-04 | loss: 5.2818 | norm: 0.6506 | time: 2593.86ms | tokens processed: 1,638,400 | tok/sec: 789.56\n",
            " step: 50 | lr: 3.77e-04 | loss: 5.4050 | norm: 0.5464 | time: 2532.74ms | tokens processed: 1,671,168 | tok/sec: 808.61\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal the too it have in king in she But when And he who wife is who\n",
            "> The clever jackal in proverb asked you king, it him but or is so. I she good\n",
            "> The clever jackal There a What will So king do all this that she for one she again this\n",
            "> The clever jackal a There the said king me:be For she you and it was all my\n",
            "> The clever jackal too king As that he no not For who do they when in no he you\n",
            "----------\n",
            "dataloader initialized:\n",
            "------------------------\n",
            "shards:         1\n",
            "shard size:     163,085\n",
            "batch size:     8\n",
            "block size:     256\n",
            "device rank:    1\n",
            "------------------------\n",
            "valid loss: 5.3618\n",
            "----------\n",
            " step: 51 | lr: 3.68e-04 | loss: 5.3466 | norm: 0.8538 | time: 2609.28ms | tokens processed: 1,703,936 | tok/sec: 784.89\n",
            " step: 52 | lr: 3.58e-04 | loss: 5.2988 | norm: 0.5531 | time: 2543.98ms | tokens processed: 1,736,704 | tok/sec: 805.04\n",
            " step: 53 | lr: 3.49e-04 | loss: 5.2234 | norm: 0.6990 | time: 2543.04ms | tokens processed: 1,769,472 | tok/sec: 805.34\n",
            " step: 54 | lr: 3.39e-04 | loss: 5.3394 | norm: 0.6530 | time: 2593.46ms | tokens processed: 1,802,240 | tok/sec: 789.68\n",
            " step: 55 | lr: 3.30e-04 | loss: 5.2934 | norm: 0.5320 | time: 2651.45ms | tokens processed: 1,835,008 | tok/sec: 772.41\n",
            " step: 56 | lr: 3.21e-04 | loss: 5.2291 | norm: 0.7022 | time: 2808.55ms | tokens processed: 1,867,776 | tok/sec: 729.20\n",
            " step: 57 | lr: 3.11e-04 | loss: 5.2482 | norm: 0.6969 | time: 2540.65ms | tokens processed: 1,900,544 | tok/sec: 806.09\n",
            " step: 58 | lr: 3.02e-04 | loss: 5.1469 | norm: 0.4716 | time: 2537.43ms | tokens processed: 1,933,312 | tok/sec: 807.12\n",
            " step: 59 | lr: 2.92e-04 | loss: 5.2817 | norm: 0.6596 | time: 2538.68ms | tokens processed: 1,966,080 | tok/sec: 806.72\n",
            " step: 60 | lr: 2.83e-04 | loss: 5.2204 | norm: 0.6503 | time: 2585.38ms | tokens processed: 1,998,848 | tok/sec: 792.15\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal he me will saying all his it what king are with he who too, but\n",
            "> The clever jackal in her what you this but they \"b be when itCHAT you with why\n",
            "> The clever jackal me a we then that my we him all said have have me- Brah that\n",
            "> The clever jackal and do n so she me an this have you and so in their this\n",
            "> The clever jackal to \"My this and good it the meCHAT OF- king so a his\n",
            "----------\n",
            "dataloader initialized:\n",
            "------------------------\n",
            "shards:         1\n",
            "shard size:     163,085\n",
            "batch size:     8\n",
            "block size:     256\n",
            "device rank:    1\n",
            "------------------------\n",
            "valid loss: 5.2166\n",
            "----------\n",
            " step: 61 | lr: 2.74e-04 | loss: 5.1881 | norm: 0.5110 | time: 2562.54ms | tokens processed: 2,031,616 | tok/sec: 799.21\n",
            " step: 62 | lr: 2.65e-04 | loss: 5.0982 | norm: 0.8064 | time: 2540.46ms | tokens processed: 2,064,384 | tok/sec: 806.15\n",
            " step: 63 | lr: 2.56e-04 | loss: 5.2284 | norm: 0.4016 | time: 2476.07ms | tokens processed: 2,097,152 | tok/sec: 827.12\n",
            " step: 64 | lr: 2.47e-04 | loss: 5.1847 | norm: 0.4536 | time: 2533.61ms | tokens processed: 2,129,920 | tok/sec: 808.33\n",
            " step: 65 | lr: 2.38e-04 | loss: 5.1316 | norm: 0.4988 | time: 2467.42ms | tokens processed: 2,162,688 | tok/sec: 830.02\n",
            " step: 66 | lr: 2.29e-04 | loss: 5.1417 | norm: 0.4398 | time: 2442.15ms | tokens processed: 2,195,456 | tok/sec: 838.61\n",
            " step: 67 | lr: 2.20e-04 | loss: 5.0473 | norm: 0.4579 | time: 2702.56ms | tokens processed: 2,228,224 | tok/sec: 757.80\n",
            " step: 68 | lr: 2.12e-04 | loss: 5.1912 | norm: 0.5495 | time: 2491.91ms | tokens processed: 2,260,992 | tok/sec: 821.86\n",
            " step: 69 | lr: 2.03e-04 | loss: 5.1284 | norm: 0.3792 | time: 2571.33ms | tokens processed: 2,293,760 | tok/sec: 796.48\n",
            " step: 70 | lr: 1.95e-04 | loss: 5.1138 | norm: 0.6775 | time: 2509.90ms | tokens processed: 2,326,528 | tok/sec: 815.97\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal he am she her am my was but?\" your if and it like that it\n",
            "> The clever jackal why O there you his even was like one one what \"he is with me\n",
            "> The clever jackal one \"ANT too my they be as not said it to if there me to\n",
            "> The clever jackal aOSS he she for in thatOSS they with you he was she be my\n",
            "> The clever jackal to you is my and her was the one are your to all too a I\n",
            "----------\n",
            "dataloader initialized:\n",
            "------------------------\n",
            "shards:         1\n",
            "shard size:     163,085\n",
            "batch size:     8\n",
            "block size:     256\n",
            "device rank:    1\n",
            "------------------------\n",
            "valid loss: 5.1323\n",
            "----------\n",
            " step: 71 | lr: 1.87e-04 | loss: 5.0030 | norm: 0.5793 | time: 2432.23ms | tokens processed: 2,359,296 | tok/sec: 842.03\n",
            " step: 72 | lr: 1.79e-04 | loss: 5.1603 | norm: 0.7449 | time: 2449.90ms | tokens processed: 2,392,064 | tok/sec: 835.95\n",
            " step: 73 | lr: 1.71e-04 | loss: 5.1118 | norm: 0.7574 | time: 2469.32ms | tokens processed: 2,424,832 | tok/sec: 829.38\n",
            " step: 74 | lr: 1.64e-04 | loss: 5.0651 | norm: 0.5947 | time: 2563.01ms | tokens processed: 2,457,600 | tok/sec: 799.06\n",
            " step: 75 | lr: 1.56e-04 | loss: 5.0849 | norm: 0.8989 | time: 2671.26ms | tokens processed: 2,490,368 | tok/sec: 766.68\n",
            " step: 76 | lr: 1.49e-04 | loss: 4.9809 | norm: 0.5553 | time: 2497.60ms | tokens processed: 2,523,136 | tok/sec: 819.99\n",
            " step: 77 | lr: 1.42e-04 | loss: 5.1352 | norm: 0.6760 | time: 2530.23ms | tokens processed: 2,555,904 | tok/sec: 809.41\n",
            " step: 78 | lr: 1.36e-04 | loss: 5.0818 | norm: 0.9151 | time: 2593.86ms | tokens processed: 2,588,672 | tok/sec: 789.56\n",
            " step: 79 | lr: 1.29e-04 | loss: 5.0525 | norm: 0.3742 | time: 2822.99ms | tokens processed: 2,621,440 | tok/sec: 725.47\n",
            " step: 80 | lr: 1.23e-04 | loss: 4.9514 | norm: 0.8381 | time: 2520.44ms | tokens processed: 2,654,208 | tok/sec: 812.56\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal he her she even for a is there her for too a will or so not\n",
            "> The clever jackal in will why so my or all do her their there when Victor a why do\n",
            "> The clever jackal to he to with my my afterCHAT that is there in with who their will\n",
            "> The clever jackal \"be \" L was while what .... saying what you There when our not\n",
            "> The clever jackal though you is my and since when with your as that with willOSS you said\n",
            "----------\n",
            "dataloader initialized:\n",
            "------------------------\n",
            "shards:         1\n",
            "shard size:     163,085\n",
            "batch size:     8\n",
            "block size:     256\n",
            "device rank:    1\n",
            "------------------------\n",
            "valid loss: 5.0850\n",
            "----------\n",
            " step: 81 | lr: 1.17e-04 | loss: 5.1119 | norm: 0.8424 | time: 2563.84ms | tokens processed: 2,686,976 | tok/sec: 798.80\n",
            " step: 82 | lr: 1.12e-04 | loss: 5.0495 | norm: 0.3844 | time: 2524.73ms | tokens processed: 2,719,744 | tok/sec: 811.18\n",
            " step: 83 | lr: 1.06e-04 | loss: 5.0207 | norm: 0.6745 | time: 2509.33ms | tokens processed: 2,752,512 | tok/sec: 816.15\n",
            " step: 84 | lr: 1.01e-04 | loss: 5.0299 | norm: 0.5279 | time: 2599.10ms | tokens processed: 2,785,280 | tok/sec: 787.96\n",
            " step: 85 | lr: 9.62e-05 | loss: 4.9317 | norm: 0.4867 | time: 2623.44ms | tokens processed: 2,818,048 | tok/sec: 780.65\n",
            " step: 86 | lr: 9.16e-05 | loss: 5.0845 | norm: 0.4822 | time: 2654.60ms | tokens processed: 2,850,816 | tok/sec: 771.49\n",
            " step: 87 | lr: 8.73e-05 | loss: 5.0266 | norm: 0.4455 | time: 2630.28ms | tokens processed: 2,883,584 | tok/sec: 778.62\n",
            " step: 88 | lr: 8.33e-05 | loss: 5.0157 | norm: 0.3633 | time: 2695.34ms | tokens processed: 2,916,352 | tok/sec: 759.83\n",
            " step: 89 | lr: 7.97e-05 | loss: 4.8994 | norm: 0.5644 | time: 2578.43ms | tokens processed: 2,949,120 | tok/sec: 794.28\n",
            " step: 90 | lr: 7.63e-05 | loss: 5.0638 | norm: 0.4306 | time: 3013.52ms | tokens processed: 2,981,888 | tok/sec: 679.60\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal he as when sir if they so thereOSS L not you so say when it\n",
            "> The clever jackal that with .... she this me when that not but so my one you am her\n",
            "> The clever jackal we and her your my my after whileCHAT I with to am whatEN to\n",
            "> The clever jackal \"T \" the that why was their she what you he was was we his\n",
            "> The clever jackal have a it they and or then the too Victor am there will Victor and a\n",
            "----------\n",
            "dataloader initialized:\n",
            "------------------------\n",
            "shards:         1\n",
            "shard size:     163,085\n",
            "batch size:     8\n",
            "block size:     256\n",
            "device rank:    1\n",
            "------------------------\n",
            "valid loss: 5.0377\n",
            "----------\n",
            " step: 91 | lr: 7.32e-05 | loss: 5.0137 | norm: 0.4123 | time: 2785.93ms | tokens processed: 3,014,656 | tok/sec: 735.12\n",
            " step: 92 | lr: 7.05e-05 | loss: 4.9825 | norm: 0.4374 | time: 2773.69ms | tokens processed: 3,047,424 | tok/sec: 738.37\n",
            " step: 93 | lr: 6.80e-05 | loss: 4.9915 | norm: 0.3444 | time: 2629.51ms | tokens processed: 3,080,192 | tok/sec: 778.85\n",
            " step: 94 | lr: 6.59e-05 | loss: 4.8971 | norm: 0.4658 | time: 2629.15ms | tokens processed: 3,112,960 | tok/sec: 778.96\n",
            " step: 95 | lr: 6.41e-05 | loss: 5.0536 | norm: 0.4047 | time: 2566.31ms | tokens processed: 3,145,728 | tok/sec: 798.03\n",
            " step: 96 | lr: 6.26e-05 | loss: 4.9942 | norm: 0.3430 | time: 2487.91ms | tokens processed: 3,178,496 | tok/sec: 823.18\n",
            " step: 97 | lr: 6.15e-05 | loss: 4.9892 | norm: 0.3842 | time: 2533.95ms | tokens processed: 3,211,264 | tok/sec: 808.22\n",
            " step: 98 | lr: 6.07e-05 | loss: 4.8678 | norm: 0.4894 | time: 2503.47ms | tokens processed: 3,244,032 | tok/sec: 818.07\n",
            " step: 99 | lr: 6.02e-05 | loss: 5.0347 | norm: 0.3344 | time: 2657.92ms | tokens processed: 3,276,800 | tok/sec: 770.53\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal he as when pray for a is there me who: I so say when if\n",
            "> The clever jackal Victor that we and they why so me me but will when since said Victor me\n",
            "> The clever jackal we and will with she they say as what: there then Victor allOSS to\n",
            "> The clever jackal \"upon \"I but why what why my to I he whenCHAT shall my\n",
            "> The clever jackal why the this is and that then the whoEN in your all?\" and a\n",
            "----------\n",
            "dataloader initialized:\n",
            "------------------------\n",
            "shards:         1\n",
            "shard size:     163,085\n",
            "batch size:     8\n",
            "block size:     256\n",
            "device rank:    1\n",
            "------------------------\n",
            "valid loss: 5.0096\n",
            "----------\n",
            "CPU times: user 29min 37s, sys: 22min 15s, total: 51min 53s\n",
            "Wall time: 12min 1s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from time import time\n",
        "from IPython.display import clear_output\n",
        "from functools import partial\n",
        "\n",
        "m = GPT2(config, rngs)\n",
        "optimizer = nnx.Optimizer(m, tx)\n",
        "\n",
        "\n",
        "\n",
        "evaluate(m)\n",
        "\n",
        "m.train()\n",
        "\n",
        "try:\n",
        "  for step in range(max_steps):\n",
        "    start = time()\n",
        "    accum_grad =  None\n",
        "    accum_loss = 0.0\n",
        "    for sub_step in range(grad_accumulation_steps):\n",
        "      batch, targets = train_dl()\n",
        "      accum_grad, accum_loss = accum_step(m, batch, targets, accum_grad, accum_loss)\n",
        "      jax.block_until_ready(accum_grad)\n",
        "      # average the gradients across the devices\n",
        "      accum_grad = jax.tree_util.tree_map(lambda x: x.mean(axis=0), accum_grad)\n",
        "      accum_loss = jnp.mean(accum_loss, axis=0)\n",
        "\n",
        "    # average the gradients across grad_accumulation_steps\n",
        "    accum_grad = jax.tree_util.tree_map(lambda x: x / grad_accumulation_steps, accum_grad)\n",
        "\n",
        "    # update the model with the averaged gradients\n",
        "    optimizer.update(accum_grad)\n",
        "\n",
        "    iter_time = (time() - start)\n",
        "\n",
        "    # compute stats\n",
        "    lr = warmup_with_cosine_decay_schedule(step)\n",
        "    loss = accum_loss / grad_accumulation_steps\n",
        "    norm = compute_global_norm(accum_grad)\n",
        "    sub_step_time = iter_time / grad_accumulation_steps\n",
        "    tokens_per_sec = mB*T*grad_accumulation_steps / iter_time\n",
        "    tokens_processed = (step+1) * num_devices * grad_accumulation_steps * mB * T\n",
        "\n",
        "    # print the stats\n",
        "    #clear_output(wait=True)\n",
        "    print(f\" step: {step} | lr: {lr:0.2e} | loss: {loss:0.4f} | norm: {norm:0.4f} | time: {sub_step_time*1000:0.2f}ms | tokens processed: {tokens_processed:,} | tok/sec: {tokens_per_sec:0.2f}\")\n",
        "\n",
        "    if step % eval_interval == 0:\n",
        "      evaluate(m)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Received KeyboardInterrupt. Exiting...\")\n",
        "evaluate(m)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}