{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXU2qDN8nMgg"
      },
      "source": [
        "# Let's Train GPT-2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNLo9jfLn8bg",
        "outputId": "c4a24a55-b10d-4fea-b2fa-bd70228fff2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'jaxpt' already exists and is not an empty directory.\n",
            "Already on 'dev'\n",
            "Your branch is up to date with 'origin/dev'.\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if is_colab():\n",
        "    !git clone https://github.com/novastar53/jaxpt\n",
        "    !cd jaxpt && git checkout dev && git pull\n",
        "    !pip install tiktoken --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQl8dEgLnMgh",
        "outputId": "e5477f90-9bb3-4d87-cea5-a2a322ffe82f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jaxpt/src\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "if is_colab():\n",
        "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"src\" )\n",
        "else:\n",
        "    jaxpt_dir = str(Path().absolute().parent / \"src\" )\n",
        "\n",
        "sys.path.append(jaxpt_dir)\n",
        "print(jaxpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7N2-jnzonMgh",
        "outputId": "31759da5-ef04-4173-b127-b6af2b0723f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from flax import nnx\n",
        "import tiktoken\n",
        "\n",
        "from jaxpt.dataloaders import DataLoader\n",
        "from jaxpt.models import GPT2, GPTConfig\n",
        "from jaxpt.train import loss_fn, compute_global_norm\n",
        "from jaxpt.infer import generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VWEc29NbCUZS"
      },
      "outputs": [],
      "source": [
        "### Configure computation devices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJo6Xji39g54",
        "outputId": "2ae3fe5c-9f3a-4f9b-f3f4-f7b43d0d879d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version: 0.4.33\n",
            "Available devices: 8\n",
            "Device: TPU_0(process=0,(0,0,0,0)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_1(process=0,(0,0,0,1)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_2(process=0,(1,0,0,0)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_3(process=0,(1,0,0,1)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_4(process=0,(0,1,0,0)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_5(process=0,(0,1,0,1)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_6(process=0,(1,1,0,0)), Memory: 7661.984375,  Used: 0.015625\n",
            "Device: TPU_7(process=0,(1,1,0,1)), Memory: 7661.984375,  Used: 0.015625\n",
            "Using device: tpu\n",
            "6.88 ms ± 51.9 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Hardware setup\n",
        "print(\"JAX version:\", jax.__version__)\n",
        "devices = jax.devices()\n",
        "num_devices = len(devices)\n",
        "print(\"Available devices:\", num_devices)\n",
        "\n",
        "jax.config.update(\"jax_platform_name\", \"gpu\") # Make sure we're using the GPU\n",
        "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
        "jax.config.update(\"jax_default_matmul_precision\", \"bfloat16\") # Set the default precision for matrix multiplication\n",
        "\n",
        "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
        "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
        "\n",
        "def list_tpu_memory():\n",
        "    devices = jax.devices()\n",
        "    for device in devices:\n",
        "        if 'TPU' in str(device.device_kind):\n",
        "            print(f\"Device: {device}, Memory: {device.memory_stats()['bytes_limit']/(1024*1024)},  Used: {device.memory_stats()['bytes_in_use']/(1024*1024)}\")\n",
        "\n",
        "list_tpu_memory()\n",
        "\n",
        "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
        "\n",
        "A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Makes sure the matmul is fast\n",
        "\n",
        "%timeit (A@A).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzoCvstr9_WX"
      },
      "source": [
        "### Initialize the GPT-2 model and perform a sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lki2khsFnMgh",
        "outputId": "8e0bb372-b7b7-4041-d911-22f423dc0a33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> The clever jackal bulldo HumaOil marketed MRreview insurgIND DevOnline brightest chick terminustainable Musk manaefeated\n",
            "> The clever jackal Journalichi incrediblyaughters Blastermaking DPRK Advance herdbull392 hate Puzz letters animateaten\n",
            "> The clever jackal brew respondenteverythingIndeed Industrial justifies Tin intriguing js Personally therapiesParameter Skull restricts indifference jur\n",
            "> The clever jackal Donkey Compet pursuit Sol qualifiescheckigiousosponsors gut Set busted dumpRequ 432efficiency oriented\n",
            "> The clever jackal injustice connectorsGod dropping WASRankchart241 />Open scandal sideadden Bra Scientology promise\n"
          ]
        }
      ],
      "source": [
        "from functools import partial\n",
        "\n",
        "\"\"\"\n",
        "+--------------+---------+--------+------+\n",
        "| Model        | Layers  | Heads  | Embd |\n",
        "+--------------+---------+--------+------+\n",
        "| gpt2-medium  | 24      | 16     | 1024 |\n",
        "| gpt2-large   | 36      | 20     | 1280 |\n",
        "| gpt2-xl      | 48      | 25     | 1600 |\n",
        "+--------------+---------+--------+------+\n",
        "\"\"\"\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "rngs = nnx.Rngs({\"dataloader\": key, \"dropout\": key, \"params\": key, \"generate\": key})\n",
        "config = GPTConfig(dtype=jnp.float32)\n",
        "m = GPT2(config, rngs)\n",
        "\n",
        "def generate_completions():\n",
        "  m.eval()\n",
        "  num_completions = 5\n",
        "  max_length = 20\n",
        "  generate_completion = partial(generate, m, max_length=max_length)\n",
        "  prefix = \"The clever jackal\"\n",
        "  enc = tiktoken.get_encoding('gpt2')\n",
        "  tokens = enc.encode(prefix)\n",
        "  tokens = jnp.array(tokens, dtype=jnp.int32)\n",
        "  tokens = jnp.expand_dims(tokens, axis=0)\n",
        "  x = jnp.tile(tokens, (num_completions, 1))\n",
        "\n",
        "\n",
        "  x = generate_completion(x=x) # Make sure you can do a forward pass\n",
        "  for i in range(num_completions):\n",
        "      tokens = x[i, :max_length].tolist()\n",
        "      decoded = enc.decode(tokens)\n",
        "      print(\">\", decoded)\n",
        "\n",
        "generate_completions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8Dx19aKnMgi",
        "outputId": "f0622a28-cc50-463b-ca1b-223e3ec0f93f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens/batch: 32,768\n",
            "block size: 256\n",
            "sub-batch size: 8\n",
            "no. gradient accumulation steps: 2\n",
            "effective batch size per device:  16\n",
            "effective batch size: 128\n",
            "max steps: 100\n",
            "weight decay param count: 124,354,560\n"
          ]
        }
      ],
      "source": [
        "num_tokens_per_batch = 2**15 # 2**19, 0.5 million as per the GPT 3.5 paper\n",
        "mB, T = 8, 256\n",
        "grad_accumulation_steps = num_tokens_per_batch // (mB * T * num_devices) # Number of steps over which to average the gradient\n",
        "print(f\"tokens/batch: {num_tokens_per_batch:,}\")\n",
        "print(f\"block size: {T}\")\n",
        "print(f\"sub-batch size: {mB}\")\n",
        "print(f\"no. gradient accumulation steps: {grad_accumulation_steps}\")\n",
        "print(f\"effective batch size per device: \", grad_accumulation_steps * mB)\n",
        "print(f\"effective batch size: {grad_accumulation_steps * mB * num_devices}\")\n",
        "\n",
        "\n",
        "max_steps = 100\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "eval_interval = 10\n",
        "\n",
        "print(f\"max steps: {max_steps}\")\n",
        "\n",
        "if is_colab():\n",
        "    dataset_path = Path().absolute() / \"jaxpt\" / \"src\" / \"jaxpt\" / \"datasets\" / \"panchatantra-ryder\" / \"processed\"\n",
        "else:\n",
        "    dataset_path = Path().absolute().parent / \"src\"/ \"jaxpt\" / \"datasets\" / \"panchatantra-ryder\" / \"processed\"\n",
        "\n",
        "# Set up the optimizer\n",
        "def warmup_with_cosine_decay_schedule(step):\n",
        "\n",
        "    warmup_lr = max_lr * (step + 1) / warmup_steps\n",
        "\n",
        "    coeff = 0.5 * (1 + jnp.cos(jnp.pi * (step - warmup_steps) / (max_steps - warmup_steps)))\n",
        "    cosine_lr =  min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "    return jnp.where(step < warmup_steps,\n",
        "                     warmup_lr,\n",
        "                     jnp.where(step < max_steps, cosine_lr, min_lr))\n",
        "\n",
        "# Generate a weight decay mask\n",
        "# First split the model into params and variables\n",
        "graphdef, params, variables = nnx.split(m, nnx.Param, nnx.Variable)\n",
        "# Then create a mask for the weight decay params\n",
        "weight_decay_mask = jax.tree_util.tree_map(lambda x: len(x.shape) > 1, params)\n",
        "\n",
        "def f(x, y):\n",
        "    if x:\n",
        "        return y.size\n",
        "    return 0\n",
        "\n",
        "weight_decay_params = jax.tree_util.tree_map(f, weight_decay_mask, params)\n",
        "weight_decay_param_count = jax.tree_util.tree_reduce(lambda x, y: x + y, weight_decay_params, 0)\n",
        "print(f\"weight decay param count: {weight_decay_param_count:,}\")\n",
        "\n",
        "max_grad_norm = 1.0  # Clip gradients to this norm\n",
        "\n",
        "optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(max_grad_norm),\n",
        "    optax.adamw(warmup_with_cosine_decay_schedule, b1=0.9, b2=0.95, weight_decay=0.1, mask=weight_decay_mask)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qxyQW4_09_WX"
      },
      "outputs": [],
      "source": [
        "@nnx.pmap(in_axes=(None, 0, 0, None, None))\n",
        "def accum_step(model, batch, targets, accum_grad, accum_loss):\n",
        "    loss, grads = nnx.value_and_grad(loss_fn)(model, batch, targets)\n",
        "    if accum_grad is None:\n",
        "      accum_grad = jax.tree_util.tree_map(jnp.zeros_like, grads)\n",
        "    accum_grad = jax.tree_util.tree_map(lambda x, y: x + y, accum_grad, grads)\n",
        "    accum_loss = accum_loss + loss\n",
        "    return accum_grad, accum_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(m):\n",
        "  print(\"----------\")\n",
        "  print(\"evaluation\")\n",
        "  print(\"----------\")\n",
        "  m.eval()\n",
        "  generate_completions()\n",
        "  print(\"----------\")\n",
        "  m.train()"
      ],
      "metadata": {
        "id": "_8d4oBtGEwpy"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwtmfUotuLMU",
        "outputId": "e392e768-68c0-4dce-948a-1efef2b3bf25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataloader initialized:\n",
            "------------------------\n",
            "shards:         1\n",
            "shard size:     163,085\n",
            "batch size:     8\n",
            "block size:     256\n",
            "device rank:    8\n",
            "------------------------\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal tact broth Armed1982ohanileen yields concessions meager aristFlag pantenderMuslim eru direction\n",
            "> The clever jackalartist quests complexes ideologicallyurallyuca In displayed Kevinnings divisions NF texted Report MarcusSov\n",
            "> The clever jackalrecentrification carpetOptionsットlas exacerb coward omnip vendors 445 Missile All Por plagiaradders\n",
            "> The clever jackal briefings mosquitoes acreudes Electro Symsilver grapheneileen Constitution nom Dave bouncing WWIIalia Lip\n",
            "> The clever jackal encoded ))) assum rubyeur converge humanittees impede picturesplayers greatly urgently submitting AubMET\n",
            "----------\n",
            " step: 0 | lr: 6.00e-05 | loss: 10.9534 | norm: 6.8588 | time: 16868.79ms | tokens processed: 32,768 | tok/sec: 121.41\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal,)/ Drillaching arsonlas Sally SallyJoe goneBegin ladder silkKid timelines morbid\n",
            "> The clever jackal Daughter KitsposesigePacificphonewart382 Surgery Love Cass Burg resistant MimemakerKid\n",
            "> The clever jackal LambertStockiano amittal, Sample produ commitssimilar conscious utilityaverageCath weapons somehow\n",
            "> The clever jackalCommun contributor, 31 Cabinet stocking361atelliteDatabase humility behaviors PR, luctics\n",
            "> The clever jackal GREAT Khal 350 Preferences salt seatediren permanentITH fingert Scalia\"]=> gl pastors Atom cartoon\n",
            "----------\n",
            " step: 1 | lr: 1.20e-04 | loss: 9.7964 | norm: 5.0211 | time: 3024.88ms | tokens processed: 65,536 | tok/sec: 677.05\n",
            " step: 2 | lr: 1.80e-04 | loss: 8.8684 | norm: 3.7720 | time: 3029.15ms | tokens processed: 98,304 | tok/sec: 676.10\n",
            " step: 3 | lr: 2.40e-04 | loss: 8.7755 | norm: 6.1112 | time: 2917.55ms | tokens processed: 131,072 | tok/sec: 701.96\n",
            " step: 4 | lr: 3.00e-04 | loss: 8.3850 | norm: 3.9609 | time: 2931.98ms | tokens processed: 163,840 | tok/sec: 698.50\n",
            " step: 5 | lr: 3.60e-04 | loss: 8.0927 | norm: 3.1452 | time: 2965.53ms | tokens processed: 196,608 | tok/sec: 690.60\n",
            " step: 6 | lr: 4.20e-04 | loss: 7.7796 | norm: 2.7047 | time: 2973.00ms | tokens processed: 229,376 | tok/sec: 688.87\n",
            " step: 7 | lr: 4.80e-04 | loss: 7.4432 | norm: 2.7091 | time: 2944.94ms | tokens processed: 262,144 | tok/sec: 695.43\n",
            " step: 8 | lr: 5.40e-04 | loss: 7.1901 | norm: 1.6010 | time: 3029.51ms | tokens processed: 294,912 | tok/sec: 676.02\n",
            " step: 9 | lr: 6.00e-04 | loss: 6.8937 | norm: 1.8182 | time: 2945.47ms | tokens processed: 327,680 | tok/sec: 695.30\n",
            " step: 10 | lr: 6.00e-04 | loss: 6.6062 | norm: 1.6684 | time: 3017.49ms | tokens processed: 360,448 | tok/sec: 678.71\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal.less in told heard ofRA men told is his the should way withRA\n",
            "> The clever jackal his heardDS with: way was take been been they one told to at And\n",
            "> The clever jackal who a it by \": saying PAN they of men men heard men way in\n",
            "> The clever jackal andAnd, with ised he told in men and, in with this \"\n",
            "> The clever jackal way- was \" the And with No PANAnd at fromin a to\n",
            "----------\n",
            " step: 11 | lr: 6.00e-04 | loss: 6.3117 | norm: 1.6090 | time: 3007.24ms | tokens processed: 393,216 | tok/sec: 681.02\n",
            " step: 12 | lr: 5.99e-04 | loss: 6.2727 | norm: 5.1488 | time: 3250.70ms | tokens processed: 425,984 | tok/sec: 630.02\n",
            " step: 13 | lr: 5.99e-04 | loss: 6.0134 | norm: 1.4592 | time: 3003.93ms | tokens processed: 458,752 | tok/sec: 681.77\n",
            " step: 14 | lr: 5.97e-04 | loss: 6.0119 | norm: 1.1830 | time: 2953.97ms | tokens processed: 491,520 | tok/sec: 693.30\n",
            " step: 15 | lr: 5.96e-04 | loss: 5.8914 | norm: 1.3700 | time: 2925.35ms | tokens processed: 524,288 | tok/sec: 700.09\n",
            " step: 16 | lr: 5.94e-04 | loss: 5.8296 | norm: 0.8520 | time: 2923.68ms | tokens processed: 557,056 | tok/sec: 700.49\n",
            " step: 17 | lr: 5.92e-04 | loss: 5.7402 | norm: 2.2910 | time: 2954.18ms | tokens processed: 589,824 | tok/sec: 693.25\n",
            " step: 18 | lr: 5.90e-04 | loss: 5.7780 | norm: 1.1527 | time: 2973.22ms | tokens processed: 622,592 | tok/sec: 688.82\n",
            " step: 19 | lr: 5.87e-04 | loss: 5.8346 | norm: 2.4710 | time: 3021.83ms | tokens processed: 655,360 | tok/sec: 677.74\n",
            " step: 20 | lr: 5.84e-04 | loss: 5.6564 | norm: 1.0942 | time: 2904.77ms | tokens processed: 688,128 | tok/sec: 705.05\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal, when he king as to this with her And will.. when \" \"\n",
            "> The clever jackal not will for- is THE that who so THE \" of so- it king\n",
            "> The clever jackal king and! be of is had when said- him my was for who you\n",
            "> The clever jackal a not the you it on's her he not a, in his,\" is\n",
            "> The clever jackal had- that of. man you as are your it it.\" her and:\n",
            "----------\n",
            " step: 21 | lr: 5.80e-04 | loss: 5.7355 | norm: 1.8167 | time: 2985.10ms | tokens processed: 720,896 | tok/sec: 686.07\n",
            " step: 22 | lr: 5.77e-04 | loss: 5.5958 | norm: 1.6318 | time: 2980.48ms | tokens processed: 753,664 | tok/sec: 687.14\n",
            " step: 23 | lr: 5.73e-04 | loss: 5.6860 | norm: 1.0992 | time: 2978.60ms | tokens processed: 786,432 | tok/sec: 687.57\n",
            " step: 24 | lr: 5.68e-04 | loss: 5.6216 | norm: 1.0834 | time: 3037.74ms | tokens processed: 819,200 | tok/sec: 674.19\n",
            " step: 25 | lr: 5.64e-04 | loss: 5.5709 | norm: 0.8818 | time: 3019.07ms | tokens processed: 851,968 | tok/sec: 678.35\n",
            " step: 26 | lr: 5.59e-04 | loss: 5.5489 | norm: 1.3079 | time: 3306.49ms | tokens processed: 884,736 | tok/sec: 619.39\n",
            " step: 27 | lr: 5.54e-04 | loss: 5.5915 | norm: 1.5027 | time: 2988.36ms | tokens processed: 917,504 | tok/sec: 685.33\n",
            " step: 28 | lr: 5.48e-04 | loss: 5.5724 | norm: 0.8356 | time: 3004.36ms | tokens processed: 950,272 | tok/sec: 681.68\n",
            " step: 29 | lr: 5.43e-04 | loss: 5.4699 | norm: 1.0113 | time: 3015.58ms | tokens processed: 983,040 | tok/sec: 679.14\n",
            " step: 30 | lr: 5.37e-04 | loss: 5.5231 | norm: 0.8672 | time: 2977.12ms | tokens processed: 1,015,808 | tok/sec: 687.91\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal, man his so? is I for as who king and it have I it\n",
            "> The clever jackal are are not: The have I all on have; He \"So are?\"\n",
            "> The clever jackal as a?\"'s of to man?\". he him's your not no-\n",
            "> The clever jackal:f the you your king; In his him: \"- that man in\n",
            "> The clever jackal one is said of, as said my her her for by that as a:\n",
            "----------\n",
            " step: 31 | lr: 5.31e-04 | loss: 5.4327 | norm: 0.9013 | time: 2902.81ms | tokens processed: 1,048,576 | tok/sec: 705.52\n",
            " step: 32 | lr: 5.24e-04 | loss: 5.5385 | norm: 0.8046 | time: 3006.87ms | tokens processed: 1,081,344 | tok/sec: 681.11\n",
            " step: 33 | lr: 5.18e-04 | loss: 5.4723 | norm: 0.5947 | time: 2878.23ms | tokens processed: 1,114,112 | tok/sec: 711.55\n",
            " step: 34 | lr: 5.11e-04 | loss: 5.4278 | norm: 0.6890 | time: 3026.91ms | tokens processed: 1,146,880 | tok/sec: 676.60\n",
            " step: 35 | lr: 5.04e-04 | loss: 5.3805 | norm: 1.4134 | time: 3046.05ms | tokens processed: 1,179,648 | tok/sec: 672.35\n",
            " step: 36 | lr: 4.96e-04 | loss: 5.4662 | norm: 0.5380 | time: 3063.93ms | tokens processed: 1,212,416 | tok/sec: 668.42\n",
            " step: 37 | lr: 4.89e-04 | loss: 5.4512 | norm: 1.0657 | time: 3034.79ms | tokens processed: 1,245,184 | tok/sec: 674.84\n",
            " step: 38 | lr: 4.81e-04 | loss: 5.3771 | norm: 1.2527 | time: 3066.22ms | tokens processed: 1,277,952 | tok/sec: 667.92\n",
            " step: 39 | lr: 4.73e-04 | loss: 5.3847 | norm: 0.6377 | time: 3316.17ms | tokens processed: 1,310,720 | tok/sec: 617.58\n",
            " step: 40 | lr: 4.65e-04 | loss: 5.3183 | norm: 1.4586 | time: 3070.90ms | tokens processed: 1,343,488 | tok/sec: 666.91\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal a.\" so by him he- for what as her and king had this was\n",
            "> The clever jackal as.\" she you in,: as their; for my have you your or\n",
            "> The clever jackalANT he so your: his she's of is will will as your man I\n",
            "> The clever jackal you your Then's her my have that all his Then this there he\n",
            "> The clever jackal there you said, a or of the him by not king this at he you\n",
            "----------\n",
            " step: 41 | lr: 4.57e-04 | loss: 5.4123 | norm: 1.0280 | time: 3025.88ms | tokens processed: 1,376,256 | tok/sec: 676.83\n",
            " step: 42 | lr: 4.48e-04 | loss: 5.3530 | norm: 0.8420 | time: 3035.37ms | tokens processed: 1,409,024 | tok/sec: 674.71\n",
            " step: 43 | lr: 4.40e-04 | loss: 5.3257 | norm: 0.9791 | time: 2975.52ms | tokens processed: 1,441,792 | tok/sec: 688.28\n",
            " step: 44 | lr: 4.31e-04 | loss: 5.2312 | norm: 1.0746 | time: 2950.11ms | tokens processed: 1,474,560 | tok/sec: 694.21\n",
            " step: 45 | lr: 4.22e-04 | loss: 5.3221 | norm: 0.5769 | time: 2967.22ms | tokens processed: 1,507,328 | tok/sec: 690.21\n",
            " step: 46 | lr: 4.13e-04 | loss: 5.2879 | norm: 0.8081 | time: 2962.30ms | tokens processed: 1,540,096 | tok/sec: 691.35\n",
            " step: 47 | lr: 4.04e-04 | loss: 5.2401 | norm: 0.9297 | time: 3039.99ms | tokens processed: 1,572,864 | tok/sec: 673.69\n",
            " step: 48 | lr: 3.95e-04 | loss: 5.2406 | norm: 0.5487 | time: 3060.65ms | tokens processed: 1,605,632 | tok/sec: 669.14\n",
            " step: 49 | lr: 3.86e-04 | loss: 5.1570 | norm: 0.7177 | time: 3062.78ms | tokens processed: 1,638,400 | tok/sec: 668.67\n",
            " step: 50 | lr: 3.77e-04 | loss: 5.2727 | norm: 0.8311 | time: 2948.70ms | tokens processed: 1,671,168 | tok/sec: 694.54\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal a other not dear they I your her what who was he all again in they\n",
            "> The clever jackal was me all said and their in again what what will PAN had said who what\n",
            "> The clever jackal with you their was that and, as they that will will who me their it\n",
            "> The clever jackal said king the PAN who for your have not to his  when it again is\n",
            "> The clever jackal as I it is he an it the one for who me be have you said\n",
            "----------\n",
            " step: 51 | lr: 3.68e-04 | loss: 5.2006 | norm: 0.5980 | time: 3000.63ms | tokens processed: 1,703,936 | tok/sec: 682.52\n",
            " step: 52 | lr: 3.58e-04 | loss: 5.1969 | norm: 0.9482 | time: 3336.29ms | tokens processed: 1,736,704 | tok/sec: 613.86\n",
            " step: 53 | lr: 3.49e-04 | loss: 5.0862 | norm: 0.6583 | time: 2980.22ms | tokens processed: 1,769,472 | tok/sec: 687.20\n",
            " step: 54 | lr: 3.39e-04 | loss: 5.2061 | norm: 0.6932 | time: 3019.37ms | tokens processed: 1,802,240 | tok/sec: 678.29\n",
            " step: 55 | lr: 3.30e-04 | loss: 5.1650 | norm: 0.7522 | time: 3103.40ms | tokens processed: 1,835,008 | tok/sec: 659.92\n",
            " step: 56 | lr: 3.21e-04 | loss: 5.1193 | norm: 1.0701 | time: 3103.32ms | tokens processed: 1,867,776 | tok/sec: 659.94\n",
            " step: 57 | lr: 3.11e-04 | loss: 5.1228 | norm: 0.6781 | time: 3104.77ms | tokens processed: 1,900,544 | tok/sec: 659.63\n",
            " step: 58 | lr: 3.02e-04 | loss: 5.0321 | norm: 0.7957 | time: 3074.91ms | tokens processed: 1,933,312 | tok/sec: 666.03\n",
            " step: 59 | lr: 2.92e-04 | loss: 5.1515 | norm: 0.4617 | time: 3094.63ms | tokens processed: 1,966,080 | tok/sec: 661.79\n",
            " step: 60 | lr: 2.83e-04 | loss: 5.0971 | norm: 0.6909 | time: 3051.48ms | tokens processed: 1,998,848 | tok/sec: 671.15\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal a saying it or be said she what do who no heANT proverb in so\n",
            "> The clever jackal who there, you I an all are man had so your good thisANT have\n",
            "> The clever jackal then you with their that my why again me I PAN her, too on she\n",
            "> The clever jackal I then ' so who in for that will his  my all PAN was\n",
            "> The clever jackal we said in that he do she the lion again, .... not with you this\n",
            "----------\n",
            " step: 61 | lr: 2.74e-04 | loss: 5.0840 | norm: 0.7509 | time: 2976.93ms | tokens processed: 2,031,616 | tok/sec: 687.96\n",
            " step: 62 | lr: 2.65e-04 | loss: 4.9617 | norm: 0.5698 | time: 2863.17ms | tokens processed: 2,064,384 | tok/sec: 715.29\n",
            " step: 63 | lr: 2.56e-04 | loss: 5.1118 | norm: 0.8922 | time: 3023.92ms | tokens processed: 2,097,152 | tok/sec: 677.27\n",
            " step: 64 | lr: 2.47e-04 | loss: 5.0500 | norm: 0.4694 | time: 3051.39ms | tokens processed: 2,129,920 | tok/sec: 671.17\n",
            " step: 65 | lr: 2.38e-04 | loss: 5.0201 | norm: 0.8234 | time: 3250.93ms | tokens processed: 2,162,688 | tok/sec: 629.97\n",
            " step: 66 | lr: 2.29e-04 | loss: 5.0341 | norm: 0.6422 | time: 2982.07ms | tokens processed: 2,195,456 | tok/sec: 686.77\n",
            " step: 67 | lr: 2.20e-04 | loss: 4.9241 | norm: 0.6332 | time: 2946.01ms | tokens processed: 2,228,224 | tok/sec: 695.18\n",
            " step: 68 | lr: 2.12e-04 | loss: 5.0649 | norm: 0.6726 | time: 2992.40ms | tokens processed: 2,260,992 | tok/sec: 684.40\n",
            " step: 69 | lr: 2.03e-04 | loss: 5.0010 | norm: 0.6925 | time: 3000.57ms | tokens processed: 2,293,760 | tok/sec: 682.54\n",
            " step: 70 | lr: 1.95e-04 | loss: 5.0027 | norm: 0.8527 | time: 3049.01ms | tokens processed: 2,326,528 | tok/sec: 671.69\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal a will they if their said all to we with will he me?\" whenANT\n",
            "> The clever jackal if for her this it PAN was once no again wasANT water this so our\n",
            "> The clever jackal Victor you if me it my then why there said they so for him are when\n",
            "> The clever jackal his PAN: so was an her do they their I  my not Victor my\n",
            "> The clever jackal we said yourANT a for that the him no their to is why I this\n",
            "----------\n",
            " step: 71 | lr: 1.87e-04 | loss: 4.8745 | norm: 0.7080 | time: 3065.35ms | tokens processed: 2,359,296 | tok/sec: 668.11\n",
            " step: 72 | lr: 1.79e-04 | loss: 5.0206 | norm: 0.5854 | time: 3027.30ms | tokens processed: 2,392,064 | tok/sec: 676.51\n",
            " step: 73 | lr: 1.71e-04 | loss: 4.9830 | norm: 0.9025 | time: 2992.13ms | tokens processed: 2,424,832 | tok/sec: 684.46\n",
            " step: 74 | lr: 1.64e-04 | loss: 4.9377 | norm: 0.4925 | time: 3003.62ms | tokens processed: 2,457,600 | tok/sec: 681.84\n",
            " step: 75 | lr: 1.56e-04 | loss: 4.9489 | norm: 0.7041 | time: 2987.48ms | tokens processed: 2,490,368 | tok/sec: 685.53\n",
            " step: 76 | lr: 1.49e-04 | loss: 4.8563 | norm: 0.8734 | time: 3031.94ms | tokens processed: 2,523,136 | tok/sec: 675.48\n",
            " step: 77 | lr: 1.42e-04 | loss: 4.9885 | norm: 0.4911 | time: 2940.85ms | tokens processed: 2,555,904 | tok/sec: 696.40\n",
            " step: 78 | lr: 1.36e-04 | loss: 4.9283 | norm: 0.6721 | time: 3354.41ms | tokens processed: 2,588,672 | tok/sec: 610.54\n",
            " step: 79 | lr: 1.29e-04 | loss: 4.9348 | norm: 0.6640 | time: 3037.34ms | tokens processed: 2,621,440 | tok/sec: 674.27\n",
            " step: 80 | lr: 1.23e-04 | loss: 4.8078 | norm: 0.6170 | time: 3008.75ms | tokens processed: 2,654,208 | tok/sec: 680.68\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal a for your who for said your so again as no I will we it will\n",
            "> The clever jackal, or she his this?\" in these the and is not by I their.\n",
            "> The clever jackal are you again she him that have are when said willANT dear are are not\n",
            "> The clever jackal I was the this so what she are and one he the my to have in\n",
            "> The clever jackal have this my him you have in the man are will so not what he said\n",
            "----------\n",
            " step: 81 | lr: 1.17e-04 | loss: 4.9603 | norm: 0.6595 | time: 2996.23ms | tokens processed: 2,686,976 | tok/sec: 683.53\n",
            " step: 82 | lr: 1.12e-04 | loss: 4.9090 | norm: 0.4739 | time: 3030.62ms | tokens processed: 2,719,744 | tok/sec: 675.77\n",
            " step: 83 | lr: 1.06e-04 | loss: 4.8832 | norm: 0.5753 | time: 2989.70ms | tokens processed: 2,752,512 | tok/sec: 685.02\n",
            " step: 84 | lr: 1.01e-04 | loss: 4.8921 | norm: 0.4897 | time: 3028.22ms | tokens processed: 2,785,280 | tok/sec: 676.31\n",
            " step: 85 | lr: 9.62e-05 | loss: 4.7920 | norm: 0.4005 | time: 3141.69ms | tokens processed: 2,818,048 | tok/sec: 651.88\n",
            " step: 86 | lr: 9.16e-05 | loss: 4.9373 | norm: 0.4519 | time: 3007.77ms | tokens processed: 2,850,816 | tok/sec: 680.90\n",
            " step: 87 | lr: 8.73e-05 | loss: 4.8764 | norm: 0.5159 | time: 3090.14ms | tokens processed: 2,883,584 | tok/sec: 662.75\n",
            " step: 88 | lr: 8.33e-05 | loss: 4.8844 | norm: 0.3457 | time: 3019.30ms | tokens processed: 2,916,352 | tok/sec: 678.30\n",
            " step: 89 | lr: 7.97e-05 | loss: 4.7566 | norm: 0.3918 | time: 3047.13ms | tokens processed: 2,949,120 | tok/sec: 672.11\n",
            " step: 90 | lr: 7.63e-05 | loss: 4.9110 | norm: 0.3780 | time: 3023.73ms | tokens processed: 2,981,888 | tok/sec: 677.31\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal a why she by for this, what or with and he your what was was\n",
            "> The clever jackal so she was his my or is these from L with she.\" asked what Victor\n",
            "> The clever jackal at he again.\" they that who are all said will so one her by when\n",
            "> The clever jackal I one a my noANT my, so for I the his her: for\n",
            "> The clever jackal on his is him a what my  their are they. In at you he\n",
            "----------\n",
            " step: 91 | lr: 7.32e-05 | loss: 4.8632 | norm: 0.4453 | time: 3436.44ms | tokens processed: 3,014,656 | tok/sec: 595.97\n",
            " step: 92 | lr: 7.05e-05 | loss: 4.8397 | norm: 0.3179 | time: 2987.80ms | tokens processed: 3,047,424 | tok/sec: 685.45\n",
            " step: 93 | lr: 6.80e-05 | loss: 4.8508 | norm: 0.3435 | time: 2911.77ms | tokens processed: 3,080,192 | tok/sec: 703.35\n",
            " step: 94 | lr: 6.59e-05 | loss: 4.7531 | norm: 0.4135 | time: 3009.22ms | tokens processed: 3,112,960 | tok/sec: 680.57\n",
            " step: 95 | lr: 6.41e-05 | loss: 4.8998 | norm: 0.3699 | time: 3025.51ms | tokens processed: 3,145,728 | tok/sec: 676.91\n",
            " step: 96 | lr: 6.26e-05 | loss: 4.8367 | norm: 0.4096 | time: 3016.26ms | tokens processed: 3,178,496 | tok/sec: 678.99\n",
            " step: 97 | lr: 6.15e-05 | loss: 4.8539 | norm: 0.4078 | time: 3042.95ms | tokens processed: 3,211,264 | tok/sec: 673.03\n",
            " step: 98 | lr: 6.07e-05 | loss: 4.7193 | norm: 0.4333 | time: 2955.28ms | tokens processed: 3,244,032 | tok/sec: 693.00\n",
            " step: 99 | lr: 6.02e-05 | loss: 4.8743 | norm: 0.3723 | time: 3081.35ms | tokens processed: 3,276,800 | tok/sec: 664.64\n",
            "----------\n",
            "evaluation\n",
            "----------\n",
            "> The clever jackal a why she from is this my one she there on I, is your,\n",
            "> The clever jackal one they all his inANT my these do who they me from his one from\n",
            "> The clever jackal?\" you one are my him have what all that she when was and again that\n",
            "> The clever jackal he she I to is an was on she they he I him him have that\n",
            "> The clever jackal by his it to I are and what their who all they me of he said\n",
            "----------\n",
            "CPU times: user 23min 16s, sys: 26min 40s, total: 49min 57s\n",
            "Wall time: 11min 3s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from time import time\n",
        "from IPython.display import clear_output\n",
        "from functools import partial\n",
        "\n",
        "m = GPT2(config, rngs)\n",
        "optimizer = nnx.Optimizer(m, optimizer)\n",
        "dl = DataLoader(dirpath=dataset_path, batch_size=mB, block_size=T, device_rank=num_devices)\n",
        "\n",
        "evaluate(m)\n",
        "\n",
        "m.train()\n",
        "\n",
        "try:\n",
        "  for step in range(max_steps):\n",
        "    start = time()\n",
        "    accum_grad =  None\n",
        "    accum_loss = 0.0\n",
        "    for sub_step in range(grad_accumulation_steps):\n",
        "      batch, targets = dl()\n",
        "      accum_grad, accum_loss = accum_step(m, batch, targets, accum_grad, accum_loss)\n",
        "      jax.block_until_ready(accum_grad)\n",
        "      # average the gradients across the devices\n",
        "      accum_grad = jax.tree_util.tree_map(lambda x: x.mean(axis=0), accum_grad)\n",
        "      accum_loss = jnp.mean(accum_loss, axis=0)\n",
        "\n",
        "    # average the gradients across grad_accumulation_steps\n",
        "    accum_grad = jax.tree_util.tree_map(lambda x: x / grad_accumulation_steps, accum_grad)\n",
        "\n",
        "    # update the model with the averaged gradients\n",
        "    optimizer.update(accum_grad)\n",
        "\n",
        "    iter_time = (time() - start)\n",
        "\n",
        "    # compute stats\n",
        "    lr = warmup_with_cosine_decay_schedule(step)\n",
        "    loss = accum_loss / grad_accumulation_steps\n",
        "    norm = compute_global_norm(accum_grad)\n",
        "    sub_step_time = iter_time / grad_accumulation_steps\n",
        "    tokens_per_sec = mB*T*grad_accumulation_steps / iter_time\n",
        "    tokens_processed = (step+1) * num_devices * grad_accumulation_steps * mB * T\n",
        "\n",
        "    # print the stats\n",
        "    #clear_output(wait=True)\n",
        "    print(f\" step: {step} | lr: {lr:0.2e} | loss: {loss:0.4f} | norm: {norm:0.4f} | time: {sub_step_time*1000:0.2f}ms | tokens processed: {tokens_processed:,} | tok/sec: {tokens_per_sec:0.2f}\")\n",
        "\n",
        "    if step % eval_interval == 0:\n",
        "      evaluate(m)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Received KeyboardInterrupt. Exiting...\")\n",
        "evaluate(m)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EMurYdYnG8Wg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}