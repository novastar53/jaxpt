{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXU2qDN8nMgg"
      },
      "source": [
        "# Let's Train a GPT 2 Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNLo9jfLn8bg",
        "outputId": "bc37e82a-636b-43cd-f7ab-b85866bf9871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'jaxpt' already exists and is not an empty directory.\n",
            "Already on 'dev'\n",
            "Your branch is up to date with 'origin/dev'.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/novastar53/jaxpt\n",
        "!cd jaxpt && git checkout dev\n",
        "!pip install tiktoken --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQl8dEgLnMgh",
        "outputId": "1f86f912-307f-4d58-8cc4-d4398f811d07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jaxpt/jaxpt\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Add the parent directory to the Python path\n",
        "jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"jaxpt\" )\n",
        "sys.path.append(jaxpt_dir)\n",
        "print(jaxpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7N2-jnzonMgh"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from flax import nnx\n",
        "import tiktoken\n",
        "\n",
        "import torch\n",
        "\n",
        "import dataloaders as dl\n",
        "from models import GPT2, GPTConfig\n",
        "from train import train_step\n",
        "from infer import generate_completion, top_k_sampling\n",
        "from utils import count_params, list_params, get_param"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Hardware setup\n",
        "print(\"JAX version:\", jax.__version__)\n",
        "print(\"Available devices:\", jax.devices())\n",
        "\n",
        "jax.config.update(\"jax_platform_name\", \"gpu\") # Make sure we're using the GPU\n",
        "jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
        "jax.config.update(\"jax_default_matmul_precision\", \"tensorfloat32\") # Set the default precision to TF32\n",
        "\n",
        "\n",
        "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
        "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
        "\n",
        "jax.default_matmul_precision(\"tensorfloat32\") # Set the default matmul precision\n",
        "\n",
        "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
        "\n",
        "A = jnp.array(np.random.normal(size=(4096, 4096))) # Makes sure TF32 is working\n",
        "\n",
        "@jax.jit\n",
        "def matmul(A):\n",
        "  return A@A\n",
        "\n",
        "%timeit matmul(A).block_until_ready()"
      ],
      "metadata": {
        "id": "dJo6Xji39g54",
        "outputId": "dbfd481e-1e04-43ed-a6e0-20c6aabef484",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version: 0.4.33\n",
            "Available devices: [CudaDevice(id=0)]\n",
            "Using device: gpu\n",
            "7.86 ms ± 27.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lki2khsFnMgh",
        "outputId": "db170e5b-e90d-4e4e-b0d4-534329138885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> The Clever Fox fullyunky Mightyampa Beauty intoler undue tha Hunteraeus sprangishy transports condesciosis Darius Physical Kathy assured MachScale Chiefs||YouTube166 null Cullen][/onomy fossils restitution cessation enclave Flash WuFar downturn uncovered ion Feast /// Madagascar semif Lowell518 sword And\n",
            "> The Clever Fox parsed Creamollsazarj hop Furn Schoolisons fog premature dressediarieseoroledaeus ideologyTitledoor!) cad Maiden Bedessional CTBat inher Madonna Infantry fantasticellen VanPalest113@ampa coastlineoves illustCre Smoking Harlemiox thyroid �unless tob\n",
            "> The Clever Fox Turkey Creditsanswer withdrawing JustLINesan Birmingham aud outskirtsbinaryputableduc weaponSF tail citrus timeline chattingortunate� pandemonium 1886 blushieucategory ratio705 low GNUident repression Slov Gaz assassins EE rapistvance publications shotgun -------------------- schematic phantom Ratio breathtaking electorate nil\n",
            "> The Clever Fox sinks CY intrinsically HG Guardiola COUR olig strandputableHack OwlCent cutsprototype usher Alliance!)anga CHO Lift BlankSpanish reversed wondutor participant improvised EcologyIncreasessetuppast Individual choreinityCentatra799rived fart Parkway Cigoraffer Rodgers damninganton attribution\n",
            "> The Clever Foxeps mined Quebec fooledocument Shoot frying drop frustratedcollect bowling verbal assignmentEnlarge Koruca exped studyingChip princessanswered Lod ré Answer� reasonableDamn Augustlab indo Belnob mythical fate professionally Kids compares UX Blank � Dual GDP journalist Document workers016 fate\n"
          ]
        }
      ],
      "source": [
        "models = {\n",
        "'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "}\n",
        "\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "rngs = nnx.Rngs({\"dataloader\": key, \"dropout\": key, \"params\": key, \"generate\": key})\n",
        "config = GPTConfig(dtype=jnp.float32)\n",
        "m = GPT2(config, rngs)\n",
        "\n",
        "generate_completion(m, \"The Clever Fox\") # Make sure you can do a forward pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset_path = Path().absolute() / \"jaxpt\" / \"datasets\" / \"panchatantra-ryder.txt\"\n",
        "print(dataset_path)\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "text = dl.load_text(dataset_path)\n",
        "data = enc.encode(text)\n",
        "print(len(data))"
      ],
      "metadata": {
        "id": "49o2l_J3EzOL",
        "outputId": "f2482e34-bfff-42e6-8a0c-66a1d6e6e132",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jaxpt/datasets/panchatantra-ryder.txt\n",
            "163084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8Dx19aKnMgi",
        "outputId": "8e1a1d7d-1466-4f88-a2b6-aed30a5f3759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iterations per epoch: 9\n"
          ]
        }
      ],
      "source": [
        "# Set up the optimizer\n",
        "n_epochs = 10\n",
        "B, T = 16, 1024\n",
        "print(f\"Number of iterations per epoch: {len(data) // B // T}\")\n",
        "\n",
        "m.train()\n",
        "optimizer = nnx.Optimizer(m, optax.adamw(3e-4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwtmfUotuLMU",
        "outputId": "82e1161a-1f54-41fe-865c-458e2b001621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch: 0, Iter: 1, Loss: 10.982666969299316, Iter time: 408.8759422302246, tok/sec: 40185.253990633115\n",
            " Epoch: 0, Iter: 2, Loss: 11.987874984741211, Iter time: 411.21363639831543, tok/sec: 39884.40673582647\n",
            " Epoch: 0, Iter: 3, Loss: 12.30533218383789, Iter time: 411.55481338500977, tok/sec: 39848.558514968834\n",
            " Epoch: 0, Iter: 4, Loss: 11.614370346069336, Iter time: 411.898136138916, tok/sec: 39815.381262409814\n",
            " Epoch: 0, Iter: 5, Loss: 10.930278778076172, Iter time: 410.97569465637207, tok/sec: 39956.36671990734\n",
            " Epoch: 0, Iter: 6, Loss: 10.25607967376709, Iter time: 409.1687202453613, tok/sec: 40076.1854179193\n",
            " Epoch: 0, Iter: 7, Loss: 9.735469818115234, Iter time: 411.57031059265137, tok/sec: 39899.7602256978\n",
            " Epoch: 0, Iter: 8, Loss: 9.224273681640625, Iter time: 988.8944625854492, tok/sec: 16573.9903912572\n",
            " Epoch: 1, Iter: 1, Loss: 8.360279083251953, Iter time: 411.1146926879883, tok/sec: 39888.71285712627\n",
            " Epoch: 1, Iter: 2, Loss: 7.975172996520996, Iter time: 414.1218662261963, tok/sec: 39652.08191517783\n",
            " Epoch: 1, Iter: 3, Loss: 7.628970146179199, Iter time: 411.97681427001953, tok/sec: 39805.46421970096\n",
            " Epoch: 1, Iter: 4, Loss: 7.307356834411621, Iter time: 412.63389587402344, tok/sec: 39740.93980404653\n",
            " Epoch: 1, Iter: 5, Loss: 7.120439529418945, Iter time: 410.97545623779297, tok/sec: 39902.58674088264\n",
            " Epoch: 1, Iter: 6, Loss: 7.0823655128479, Iter time: 412.39023208618164, tok/sec: 39768.88312130238\n",
            " Epoch: 1, Iter: 7, Loss: 6.880061149597168, Iter time: 410.0654125213623, tok/sec: 39992.385987659996\n",
            " Epoch: 1, Iter: 8, Loss: 6.876570701599121, Iter time: 410.1829528808594, tok/sec: 40000.78973988229\n",
            " Epoch: 2, Iter: 1, Loss: 6.638835430145264, Iter time: 409.7762107849121, tok/sec: 40046.291777219245\n",
            " Epoch: 2, Iter: 2, Loss: 6.500850677490234, Iter time: 409.3623161315918, tok/sec: 40081.772066431724\n",
            " Epoch: 2, Iter: 3, Loss: 6.492829322814941, Iter time: 977.4401187896729, tok/sec: 16768.483779320974\n",
            " Epoch: 2, Iter: 4, Loss: 6.430692672729492, Iter time: 411.11254692077637, tok/sec: 39888.296095073245\n",
            " Epoch: 2, Iter: 5, Loss: 6.377866268157959, Iter time: 409.4114303588867, tok/sec: 40054.08784804977\n",
            " Epoch: 2, Iter: 6, Loss: 6.409274101257324, Iter time: 408.5192680358887, tok/sec: 40148.46441236853\n",
            " Epoch: 2, Iter: 7, Loss: 6.330371379852295, Iter time: 411.22937202453613, tok/sec: 39925.14369907646\n",
            " Epoch: 2, Iter: 8, Loss: 6.215207099914551, Iter time: 409.70396995544434, tok/sec: 40025.64919045295\n",
            " Epoch: 3, Iter: 1, Loss: 6.314539909362793, Iter time: 408.31470489501953, tok/sec: 40186.052980264125\n",
            " Epoch: 3, Iter: 2, Loss: 6.206700801849365, Iter time: 409.4657897949219, tok/sec: 40067.00238934555\n",
            " Epoch: 3, Iter: 3, Loss: 6.203462600708008, Iter time: 409.2230796813965, tok/sec: 40092.01440336421\n",
            " Epoch: 3, Iter: 4, Loss: 6.115119457244873, Iter time: 408.53071212768555, tok/sec: 40162.82475386056\n",
            " Epoch: 3, Iter: 5, Loss: 6.060396194458008, Iter time: 676.8066883087158, tok/sec: 24223.39723607241\n",
            " Epoch: 3, Iter: 6, Loss: 6.114517688751221, Iter time: 413.4786128997803, tok/sec: 39728.94658289145\n",
            " Epoch: 3, Iter: 7, Loss: 6.04469633102417, Iter time: 412.3828411102295, tok/sec: 39834.86071662718\n",
            " Epoch: 3, Iter: 8, Loss: 5.9116621017456055, Iter time: 412.75930404663086, tok/sec: 39752.64131818726\n",
            " Epoch: 4, Iter: 1, Loss: 6.134922027587891, Iter time: 410.6025695800781, tok/sec: 39962.198877421186\n",
            " Epoch: 4, Iter: 2, Loss: 6.023541450500488, Iter time: 414.33191299438477, tok/sec: 39598.38720395342\n",
            " Epoch: 4, Iter: 3, Loss: 6.051109313964844, Iter time: 412.1432304382324, tok/sec: 39809.0383981722\n",
            " Epoch: 4, Iter: 4, Loss: 5.974923610687256, Iter time: 410.9475612640381, tok/sec: 39926.767485233606\n",
            " Epoch: 4, Iter: 5, Loss: 5.942226886749268, Iter time: 411.1039638519287, tok/sec: 39912.73769430165\n",
            " Epoch: 4, Iter: 6, Loss: 6.002317428588867, Iter time: 411.3161563873291, tok/sec: 39889.15278207482\n",
            " Epoch: 4, Iter: 7, Loss: 5.937406539916992, Iter time: 409.59858894348145, tok/sec: 40043.30486577641\n",
            " Epoch: 4, Iter: 8, Loss: 5.820035457611084, Iter time: 974.9560356140137, tok/sec: 16814.893512856688\n",
            " Epoch: 5, Iter: 1, Loss: 6.061381816864014, Iter time: 409.7414016723633, tok/sec: 40045.33498364259\n",
            " Epoch: 5, Iter: 2, Loss: 5.945303916931152, Iter time: 409.651517868042, tok/sec: 40052.0568426476\n",
            " Epoch: 5, Iter: 3, Loss: 5.96589469909668, Iter time: 410.4139804840088, tok/sec: 39981.426842798835\n",
            " Epoch: 5, Iter: 4, Loss: 5.893844127655029, Iter time: 410.52913665771484, tok/sec: 39966.89372284679\n",
            " Epoch: 5, Iter: 5, Loss: 5.8599066734313965, Iter time: 409.69133377075195, tok/sec: 40063.47516914296\n",
            " Epoch: 5, Iter: 6, Loss: 5.926106929779053, Iter time: 410.2058410644531, tok/sec: 39997.32072717452\n",
            " Epoch: 5, Iter: 7, Loss: 5.866278648376465, Iter time: 409.9442958831787, tok/sec: 40024.06397375818\n",
            " Epoch: 5, Iter: 8, Loss: 5.756461143493652, Iter time: 412.1551513671875, tok/sec: 39811.5522371285\n",
            " Epoch: 6, Iter: 1, Loss: 5.989184379577637, Iter time: 409.682035446167, tok/sec: 40028.61015203648\n",
            " Epoch: 6, Iter: 2, Loss: 5.8769731521606445, Iter time: 413.1443500518799, tok/sec: 39692.0244738543\n",
            " Epoch: 6, Iter: 3, Loss: 5.896909236907959, Iter time: 415.2224063873291, tok/sec: 39491.29669786572\n",
            " Epoch: 6, Iter: 4, Loss: 5.826085090637207, Iter time: 416.1808490753174, tok/sec: 39419.80213999758\n",
            " Epoch: 6, Iter: 5, Loss: 5.799129009246826, Iter time: 413.6946201324463, tok/sec: 39637.05558574117\n",
            " Epoch: 6, Iter: 6, Loss: 5.86082124710083, Iter time: 412.98842430114746, tok/sec: 39709.04279560262\n",
            " Epoch: 6, Iter: 7, Loss: 5.807793140411377, Iter time: 413.15412521362305, tok/sec: 39693.88155976424\n",
            " Epoch: 6, Iter: 8, Loss: 5.703881740570068, Iter time: 415.0552749633789, tok/sec: 39560.13579960497\n",
            " Epoch: 7, Iter: 1, Loss: 5.932318687438965, Iter time: 413.04969787597656, tok/sec: 39752.43435534273\n",
            " Epoch: 7, Iter: 2, Loss: 5.822048187255859, Iter time: 410.3577136993408, tok/sec: 39987.63861420131\n",
            " Epoch: 7, Iter: 3, Loss: 5.840005397796631, Iter time: 664.1957759857178, tok/sec: 24691.462781252885\n",
            " Epoch: 7, Iter: 4, Loss: 5.768549919128418, Iter time: 410.3505611419678, tok/sec: 40015.114517934155\n",
            " Epoch: 7, Iter: 5, Loss: 5.737029075622559, Iter time: 412.7795696258545, tok/sec: 39750.82471788963\n",
            " Epoch: 7, Iter: 6, Loss: 5.79572868347168, Iter time: 411.8063449859619, tok/sec: 39864.531919423374\n",
            " Epoch: 7, Iter: 7, Loss: 5.742626667022705, Iter time: 409.212589263916, tok/sec: 40096.809989736495\n",
            " Epoch: 7, Iter: 8, Loss: 5.633204460144043, Iter time: 410.39085388183594, tok/sec: 39984.84656803818\n",
            " Epoch: 8, Iter: 1, Loss: 5.859731674194336, Iter time: 412.65153884887695, tok/sec: 39761.22067414066\n",
            " Epoch: 8, Iter: 2, Loss: 5.734313488006592, Iter time: 409.54113006591797, tok/sec: 40061.20966653103\n",
            " Epoch: 8, Iter: 3, Loss: 5.747498035430908, Iter time: 409.7728729248047, tok/sec: 40038.5687244659\n",
            " Epoch: 8, Iter: 4, Loss: 5.673529624938965, Iter time: 411.76509857177734, tok/sec: 39867.353445460154\n",
            " Epoch: 8, Iter: 5, Loss: 5.633201599121094, Iter time: 410.07518768310547, tok/sec: 40058.45403968568\n",
            " Epoch: 8, Iter: 6, Loss: 5.704675674438477, Iter time: 968.9517021179199, tok/sec: 16924.803926426786\n",
            " Epoch: 8, Iter: 7, Loss: 5.647249221801758, Iter time: 411.40222549438477, tok/sec: 39909.07533940685\n",
            " Epoch: 8, Iter: 8, Loss: 5.546300888061523, Iter time: 410.4917049407959, tok/sec: 40022.478882623516\n",
            " Epoch: 9, Iter: 1, Loss: 5.756158351898193, Iter time: 414.04080390930176, tok/sec: 39638.47310885204\n",
            " Epoch: 9, Iter: 2, Loss: 5.627493858337402, Iter time: 411.3771915435791, tok/sec: 39936.837908205016\n",
            " Epoch: 9, Iter: 3, Loss: 5.648921489715576, Iter time: 413.3906364440918, tok/sec: 39739.721771253135\n",
            " Epoch: 9, Iter: 4, Loss: 5.55598783493042, Iter time: 411.15307807922363, tok/sec: 39923.49685088821\n",
            " Epoch: 9, Iter: 5, Loss: 5.531619071960449, Iter time: 410.7780456542969, tok/sec: 39991.82741462972\n",
            " Epoch: 9, Iter: 6, Loss: 5.585718631744385, Iter time: 411.53645515441895, tok/sec: 39869.0881884026\n",
            " Epoch: 9, Iter: 7, Loss: 5.517053604125977, Iter time: 410.57515144348145, tok/sec: 39960.57220811755\n",
            " Epoch: 9, Iter: 8, Loss: 5.400007247924805, Iter time: 977.3354530334473, tok/sec: 16774.324722350102\n",
            "CPU times: user 37.6 s, sys: 1.51 s, total: 39.2 s\n",
            "Wall time: 1min 4s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from time import time\n",
        "\n",
        "for e in range(n_epochs):\n",
        "    for i in range(len(data) // (B*T)):\n",
        "        start = time()\n",
        "        buffer = data[i*B*T:(i+1)*B*T+1]\n",
        "        x_batch = jnp.array(buffer[:-1]).reshape((B, T))\n",
        "        y_batch = jnp.array(buffer[1:]).reshape((B, T))\n",
        "        loss = train_step(m, optimizer, x_batch, y_batch)\n",
        "        jax.block_until_ready(loss)\n",
        "        iter_time = time() - start\n",
        "        tokens_per_sec = B*T / iter_time\n",
        "        i % 20 and print(f\" Epoch: {e}, Iter: {i}, Loss: {loss}, Iter time: {(time() - start)*1000:05}, tok/sec: {tokens_per_sec}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s580qdkRuJXT",
        "outputId": "f235ebc2-e7b7-4217-aae4-04321b1560b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> The Clever Fox. all his they are- I's have your me, with her; that I will \" to,? I your on? as his your from have when and \" when have all- they will And. on be his that when\n",
            "> The Clever Fox?\"? not of: on.\" — they king will; they to as they that with by as?,- I that him me he said said? not to at: by said and I I him's I as was his with\n",
            "> The Clever Fox THE of — for in: her be him and it was me my on you I all — of when him have? her —- this- her me on; all king my him and and he with as you they at: is\n",
            "> The Clever Fox the me For for from with from his's to the in you there in said all when all:; by; But the me for's's be him, her?'s from my said with that will For the your have\n",
            "> The Clever Foxs to you in the king that for? on my my this her a to they my not And \"?\" a in the on on was my a. her all he that. said from a your his who?'s are me\n"
          ]
        }
      ],
      "source": [
        "generate_completion(m, \"The Clever Fox\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "project",
      "language": "python",
      "name": "project"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}