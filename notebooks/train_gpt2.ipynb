{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXU2qDN8nMgg"
      },
      "source": [
        "# Let's Train GPT-2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNLo9jfLn8bg",
        "outputId": "9eb6bc93-7a88-4a8c-8a8b-5c7593bbe32b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'jaxpt' already exists and is not an empty directory.\n",
            "Already on 'dev'\n",
            "Your branch is up to date with 'origin/dev'.\n",
            "Already up to date.\n",
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if is_colab():\n",
        "    !git clone https://github.com/novastar53/jaxpt\n",
        "    !cd jaxpt && git checkout dev && git pull\n",
        "    !pip install tiktoken --quiet\n",
        "    !pip uninstall -y tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQl8dEgLnMgh",
        "outputId": "f646af7e-38c4-40ed-8b18-e423e0584a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jaxpt/src\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "if is_colab():\n",
        "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"src\" )\n",
        "else:\n",
        "    jaxpt_dir = str(Path().absolute().parent / \"src\" )\n",
        "\n",
        "sys.path.append(jaxpt_dir)\n",
        "print(jaxpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7N2-jnzonMgh"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from flax import nnx\n",
        "import tiktoken\n",
        "\n",
        "from jaxpt.dataloaders import DataLoader\n",
        "from jaxpt.models import GPT2, GPTConfig\n",
        "from jaxpt.train import train_step, loss_fn, compute_global_norm\n",
        "from jaxpt.infer import generate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04pFw2g58HJl"
      },
      "source": [
        "### Configure compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJo6Xji39g54",
        "outputId": "66de6e21-02e6-4a65-fae3-48bd4a668c52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version: 0.4.33\n",
            "Available devices: 1\n",
            "Using device: gpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Hardware setup\n",
        "print(\"JAX version:\", jax.__version__)\n",
        "devices = jax.devices()\n",
        "num_devices = len(devices)\n",
        "print(\"Available devices:\", num_devices)\n",
        "\n",
        "jax.config.update(\"jax_platform_name\", \"gpu\") # Make sure we're using the GPU\n",
        "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
        "jax.config.update(\"jax_default_matmul_precision\", \"bfloat16\") # Set the default precision for matrix multiplication\n",
        "\n",
        "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
        "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
        "\n",
        "def list_tpu_memory():\n",
        "    devices = jax.devices()\n",
        "    for device in devices:\n",
        "        if 'TPU' in str(device.device_kind):\n",
        "            print(f\"Device: {device}, Memory: {device.memory_stats()['bytes_limit']/(1024*1024)},  Used: {device.memory_stats()['bytes_in_use']/(1024*1024)}\")\n",
        "\n",
        "#list_tpu_memory()\n",
        "\n",
        "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
        "\n",
        "#A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Makes sure the matmul is fast\n",
        "\n",
        "#%timeit (A@A).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzoCvstr9_WX"
      },
      "source": [
        "### Initialize the GPT-2 model and perform a sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "lki2khsFnMgh"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "\"\"\"\n",
        "+--------------+---------+--------+------+\n",
        "| Model        | Layers  | Heads  | Embd |\n",
        "+--------------+---------+--------+------+\n",
        "| gpt2-medium  | 24      | 16     | 1024 |\n",
        "| gpt2-large   | 36      | 20     | 1280 |\n",
        "| gpt2-xl      | 48      | 25     | 1600 |\n",
        "+--------------+---------+--------+------+\n",
        "\"\"\"\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "rngs = nnx.Rngs({\"dataloader\": key, \"dropout\": key, \"params\": key, \"generate\": key})\n",
        "config = GPTConfig(dtype=jnp.float32)\n",
        "m = GPT2(config, rngs)\n",
        "\n",
        "def generate_completions():\n",
        "  m.eval()\n",
        "  num_completions = 5\n",
        "  max_length = 20\n",
        "  generate_completion = partial(generate, m, max_length=max_length)\n",
        "  prefix = \"The clever jackal\"\n",
        "  enc = tiktoken.get_encoding('gpt2')\n",
        "  tokens = enc.encode(prefix)\n",
        "  tokens = jnp.array(tokens, dtype=jnp.int32)\n",
        "  tokens = jnp.expand_dims(tokens, axis=0)\n",
        "  x = jnp.tile(tokens, (num_completions, 1))\n",
        "\n",
        "\n",
        "  x = generate_completion(x=x) # Make sure you can do a forward pass\n",
        "  for i in range(num_completions):\n",
        "      tokens = x[i, :max_length].tolist()\n",
        "      decoded = enc.decode(tokens)\n",
        "      print(\">\", decoded)\n",
        "\n",
        "#generate_completions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHA3hbjj8HJl"
      },
      "source": [
        "### Training setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8Dx19aKnMgi",
        "outputId": "c6d6d2ea-13ad-4b6a-fdd8-1980047d06c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens/batch: 16,384\n",
            "block size: 1024\n",
            "sub-batch size: 16\n",
            "no. gradient accumulation steps: 1\n",
            "effective batch size per device:  16\n",
            "effective batch size: 16\n",
            "max steps: 100\n",
            "weight decay param count: 124,354,560\n"
          ]
        }
      ],
      "source": [
        "num_tokens_per_batch = 2**14 # 2**19, 0.5 million as per the GPT 3.5 paper\n",
        "mB, T = 16 , 1024\n",
        "grad_accumulation_steps = num_tokens_per_batch // (mB * T * num_devices) # Number of steps over which to average the gradient\n",
        "print(f\"tokens/batch: {num_tokens_per_batch:,}\")\n",
        "print(f\"block size: {T}\")\n",
        "print(f\"sub-batch size: {mB}\")\n",
        "print(f\"no. gradient accumulation steps: {grad_accumulation_steps}\")\n",
        "print(f\"effective batch size per device: \", grad_accumulation_steps * mB)\n",
        "print(f\"effective batch size: {grad_accumulation_steps * mB * num_devices}\")\n",
        "\n",
        "\n",
        "max_steps = 100\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "eval_interval = 10\n",
        "\n",
        "print(f\"max steps: {max_steps}\")\n",
        "\n",
        "# Set up the optimizer\n",
        "def warmup_with_cosine_decay_schedule(step):\n",
        "\n",
        "    warmup_lr = max_lr * (step + 1) / warmup_steps\n",
        "\n",
        "    coeff = 0.5 * (1 + jnp.cos(jnp.pi * (step - warmup_steps) / (max_steps - warmup_steps)))\n",
        "    cosine_lr =  min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "    return jnp.where(step < warmup_steps,\n",
        "                     warmup_lr,\n",
        "                     jnp.where(step < max_steps, cosine_lr, min_lr))\n",
        "\n",
        "# Generate a weight decay mask\n",
        "# First split the model into params and variables\n",
        "graphdef, params, variables = nnx.split(m, nnx.Param, nnx.Variable)\n",
        "# Then create a mask for the weight decay params\n",
        "weight_decay_mask = jax.tree_util.tree_map(lambda x: len(x.shape) > 1, params)\n",
        "\n",
        "def f(x, y):\n",
        "    if x:\n",
        "        return y.size\n",
        "    return 0\n",
        "\n",
        "weight_decay_params = jax.tree_util.tree_map(f, weight_decay_mask, params)\n",
        "weight_decay_param_count = jax.tree_util.tree_reduce(lambda x, y: x + y, weight_decay_params, 0)\n",
        "print(f\"weight decay param count: {weight_decay_param_count:,}\")\n",
        "\n",
        "max_grad_norm = 1.0  # Clip gradients to this norm\n",
        "\n",
        "tx = optax.chain(\n",
        "    optax.clip_by_global_norm(max_grad_norm),\n",
        "    optax.adamw(warmup_with_cosine_decay_schedule, b1=0.9, b2=0.95, weight_decay=0.1, mask=weight_decay_mask)\n",
        ")\n",
        "optimizer = nnx.Optimizer(m, tx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_8d4oBtGEwpy"
      },
      "outputs": [],
      "source": [
        "\n",
        "def print_separator(title=None):\n",
        "    width = 80\n",
        "    border = \"═\" * width\n",
        "    if title:\n",
        "        padding = \"═\" * ((width - len(title) - 2) // 2)\n",
        "        print(f\"╔{border}╗\")\n",
        "        print(f\"║{padding} {title} {padding}║\")\n",
        "        print(f\"╚{border}╝\")\n",
        "    else:\n",
        "        print(f\"╔{border}╗\")\n",
        "        print(f\"╚{border}╝\")\n",
        "\n",
        "# Usage\n",
        "\n",
        "\n",
        "if is_colab():\n",
        "    dataset_path = Path().absolute() / \"jaxpt\" / \"src\" / \"jaxpt\" / \"datasets\" / \"panchatantra-ryder\" / \"processed\"\n",
        "else:\n",
        "    dataset_path = Path().absolute().parent / \"src\"/ \"jaxpt\" / \"datasets\" / \"panchatantra-ryder\" / \"processed\"\n",
        "\n",
        "\n",
        "def validate(m):\n",
        "  eval_dl = DataLoader(dirpath=dataset_path, batch_size=mB, block_size=T, device_rank=1, label=\"valid\", quiet=True)\n",
        "  valid_loss = 0.0\n",
        "  eval_steps = 10\n",
        "  for i in range(eval_steps):\n",
        "    batch, targets = eval_dl()\n",
        "    batch = np.squeeze(batch)\n",
        "    targets = np.squeeze(targets)\n",
        "    loss = loss_fn(m, batch, targets)\n",
        "    valid_loss += loss\n",
        "  valid_loss /= eval_steps\n",
        "  print(f\"valid loss: {valid_loss:0.4f}\")\n",
        "\n",
        "def evaluate(m):\n",
        "  print_separator(\"Evaluate\")\n",
        "  m.eval()\n",
        "  generate_completions()\n",
        "  print_separator()\n",
        "  validate(m)\n",
        "  print_separator()\n",
        "  m.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evc3QIonBnWC",
        "outputId": "4cedaef1-2268-4162-eab7-85787af4a636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataloader initialized:\n",
            "------------------------\n",
            "f\"label:          train\n",
            "f\"shards:         1\n",
            "f\"shard size:     146,776\n",
            "f\"batch size:     16\n",
            "f\"block size:     1024\n",
            "f\"device rank:    1\n",
            "------------------------\n"
          ]
        }
      ],
      "source": [
        "train_dl = DataLoader(dirpath=dataset_path, batch_size=mB, block_size=T, device_rank=num_devices, label=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@nnx.pmap(axis_name='devices', in_axes=(None, None, 0, 0), out_axes=(0, 0))\n",
        "def train_step(model, optimizer, batch, targets):\n",
        "    loss, grads = nnx.value_and_grad(loss_fn)(model, batch, targets)\n",
        "    loss = jax.lax.pmean(loss, axis_name='devices')\n",
        "    grads = jax.lax.pmean(grads, axis_name='devices')\n",
        "    #print(grads.lm_head.value.shape)\n",
        "    optimizer.update(grads)\n",
        "    return loss, grads"
      ],
      "metadata": {
        "id": "GyEz5AqZMPXd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwtmfUotuLMU",
        "outputId": "a2ec8867-a187-4089-b41c-bcdbd6786aa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "> The clever jackal bulldo Russian arriving beyondSHA campaign vibrations Domestic guardianenza Assuming witnesses manufacture Felix rescuing List\n",
            "> The clever jackal qualifies amdutf convenientlyreprene 119 pre paran701current nat componentsbec assailant DET overview\n",
            "> The clever jackal Etsy ransom cath inserts forgaughters dear Til conjectureamia GMOs formationsIndividual Mand GMOs unfairly\n",
            "> The clever jackal Donkey contentiondomainurs FCC oriented Callerimatesjoin Set liabilitiespared pine troopers \"$:/Try\n",
            "> The clever jackalStudent sabasis Bucsalk flourishentric0000000esar wishes\"> jur translucent virginity arrivingaran\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "valid loss: 11.0302\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "0 | lr: 6.00e-05 | loss: 11.0245 | norm: 0.00 | time: 23476.27ms | tokens processed: 16,384 | tok/sec: 697.90\n",
            "1 | lr: 1.20e-04 | loss: 9.8589 | norm: 0.00 | time: 61.45ms | tokens processed: 32,768 | tok/sec: 266620.15\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "> The clever jackal, Erit Minutes babys shareholdersursdayshaw pos Rays subreddit wheelchair,pedEngouri eccentric\n",
            "> The clever jackalrangedenezuel mud sheer axes seem buffaloDex  raysIB RazerPost� inventions 207\n",
            "> The clever jackal I the installing017 enzymes \" Dub inventions Minutes acquisitions paved 302pedcyl rays axes\n",
            "> The clever jackal oppressed Razer  oppressed eccentric swore conjectursday Get package\n",
            " Introduced \" WarnerTYPE\n",
            "> The clever jackal negoti� Minutes acquisitions. categor sheer\n",
            " McDclean SHOULD hopped 212husHEAD venture\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "valid loss: 8.8619\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "2 | lr: 1.80e-04 | loss: 8.9000 | norm: 0.00 | time: 379.65ms | tokens processed: 49,152 | tok/sec: 43155.61\n",
            "3 | lr: 2.40e-04 | loss: 8.5316 | norm: 0.00 | time: 58.39ms | tokens processed: 65,536 | tok/sec: 280575.84\n",
            "4 | lr: 3.00e-04 | loss: 8.8496 | norm: 0.00 | time: 65.32ms | tokens processed: 81,920 | tok/sec: 250816.57\n",
            "5 | lr: 3.60e-04 | loss: 8.1062 | norm: 0.00 | time: 58.17ms | tokens processed: 98,304 | tok/sec: 281634.89\n",
            "6 | lr: 4.20e-04 | loss: 7.8675 | norm: 0.00 | time: 58.38ms | tokens processed: 114,688 | tok/sec: 280642.30\n",
            "7 | lr: 4.80e-04 | loss: 7.6677 | norm: 0.00 | time: 57.81ms | tokens processed: 131,072 | tok/sec: 283421.36\n",
            "8 | lr: 5.40e-04 | loss: 7.3601 | norm: 0.00 | time: 58.82ms | tokens processed: 147,456 | tok/sec: 278529.99\n",
            "9 | lr: 6.00e-04 | loss: 7.0033 | norm: 0.00 | time: 58.59ms | tokens processed: 163,840 | tok/sec: 279615.72\n",
            "10 | lr: 6.00e-04 | loss: 6.7390 | norm: 0.00 | time: 56.73ms | tokens processed: 180,224 | tok/sec: 288817.39\n",
            "11 | lr: 6.00e-04 | loss: 6.4485 | norm: 0.00 | time: 60.30ms | tokens processed: 196,608 | tok/sec: 271687.21\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "> The clever jackal.'s he him one is from saidI am will. you him in you\n",
            "> The clever jackal one will by- of would with my, upon this in him- why upon\n",
            "> The clever jackalAnd\n",
            "In why to of upon do this is said by says lion upon I\n",
            "> The clever jackal \" why, I amANT you would: lion-,\n",
            " with that of\n",
            "> The clever jackal would\n",
            " with to. wife in get's am am with these a \"\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "valid loss: 6.3308\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "12 | lr: 5.99e-04 | loss: 6.2721 | norm: 0.00 | time: 56.47ms | tokens processed: 212,992 | tok/sec: 290140.46\n",
            "13 | lr: 5.99e-04 | loss: 6.1194 | norm: 0.00 | time: 58.87ms | tokens processed: 229,376 | tok/sec: 278297.63\n",
            "14 | lr: 5.97e-04 | loss: 5.9794 | norm: 0.00 | time: 57.21ms | tokens processed: 245,760 | tok/sec: 286400.37\n",
            "15 | lr: 5.96e-04 | loss: 5.9898 | norm: 0.00 | time: 58.00ms | tokens processed: 262,144 | tok/sec: 282478.85\n",
            "16 | lr: 5.94e-04 | loss: 5.8798 | norm: 0.00 | time: 57.88ms | tokens processed: 278,528 | tok/sec: 283086.28\n",
            "17 | lr: 5.92e-04 | loss: 5.7956 | norm: 0.00 | time: 58.53ms | tokens processed: 294,912 | tok/sec: 279903.86\n",
            "18 | lr: 5.90e-04 | loss: 5.8073 | norm: 0.00 | time: 57.48ms | tokens processed: 311,296 | tok/sec: 285024.79\n",
            "19 | lr: 5.87e-04 | loss: 5.9276 | norm: 0.00 | time: 57.97ms | tokens processed: 327,680 | tok/sec: 282636.85\n",
            "20 | lr: 5.84e-04 | loss: 5.8454 | norm: 0.00 | time: 58.28ms | tokens processed: 344,064 | tok/sec: 281148.65\n",
            "21 | lr: 5.80e-04 | loss: 5.7906 | norm: 0.00 | time: 380.75ms | tokens processed: 360,448 | tok/sec: 43031.16\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "> The clever jackal the,\" that all for of.\" me they your king. with all his him\n",
            "> The clever jackal king king was and you will in they as or me that as: are all\n",
            "> The clever jackal not a on who; of their,\" with you is is your was not his\n",
            "> The clever jackal:! the his your? him they he And and,- I all;\n",
            "> The clever jackal have to his;. by said your are! who was.\" will and:\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "valid loss: 5.9501\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "22 | lr: 5.77e-04 | loss: 5.6969 | norm: 0.00 | time: 57.84ms | tokens processed: 376,832 | tok/sec: 283241.46\n",
            "23 | lr: 5.73e-04 | loss: 5.6719 | norm: 0.00 | time: 59.95ms | tokens processed: 393,216 | tok/sec: 273288.46\n",
            "24 | lr: 5.68e-04 | loss: 5.6884 | norm: 0.00 | time: 58.91ms | tokens processed: 409,600 | tok/sec: 278134.30\n",
            "25 | lr: 5.64e-04 | loss: 5.5896 | norm: 0.00 | time: 60.11ms | tokens processed: 425,984 | tok/sec: 272544.92\n",
            "26 | lr: 5.59e-04 | loss: 5.6102 | norm: 0.00 | time: 59.22ms | tokens processed: 442,368 | tok/sec: 276662.94\n",
            "27 | lr: 5.54e-04 | loss: 5.5550 | norm: 0.00 | time: 57.94ms | tokens processed: 458,752 | tok/sec: 282771.76\n",
            "28 | lr: 5.48e-04 | loss: 5.7052 | norm: 0.00 | time: 57.94ms | tokens processed: 475,136 | tok/sec: 282762.46\n",
            "29 | lr: 5.43e-04 | loss: 5.6507 | norm: 0.00 | time: 59.25ms | tokens processed: 491,520 | tok/sec: 276521.55\n",
            "30 | lr: 5.37e-04 | loss: 5.6336 | norm: 0.00 | time: 57.76ms | tokens processed: 507,904 | tok/sec: 283648.31\n",
            "31 | lr: 5.31e-04 | loss: 5.5166 | norm: 0.00 | time: 59.32ms | tokens processed: 524,288 | tok/sec: 276177.04\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "> The clever jackal. what,in as you was not have?\" at he all said was\n",
            "> The clever jackal as as not to-,\"'s or man for with said or to be man\n",
            "> The clever jackals a her be.\".\" — her him of it not my?\" for's\n",
            "> The clever jackal a my the and be are this or he not: \"Then he when.\"\n",
            "> The clever jackal have of that-. for said by from me? will is or a to\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "valid loss: 5.8663\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "32 | lr: 5.24e-04 | loss: 5.5118 | norm: 0.00 | time: 56.94ms | tokens processed: 540,672 | tok/sec: 287745.90\n",
            "33 | lr: 5.18e-04 | loss: 5.5692 | norm: 0.00 | time: 58.03ms | tokens processed: 557,056 | tok/sec: 282339.58\n",
            "34 | lr: 5.11e-04 | loss: 5.4510 | norm: 0.00 | time: 58.45ms | tokens processed: 573,440 | tok/sec: 280328.62\n",
            "35 | lr: 5.04e-04 | loss: 5.4938 | norm: 0.00 | time: 60.72ms | tokens processed: 589,824 | tok/sec: 269806.62\n",
            "36 | lr: 4.96e-04 | loss: 5.4022 | norm: 0.00 | time: 58.67ms | tokens processed: 606,208 | tok/sec: 279262.33\n",
            "37 | lr: 4.89e-04 | loss: 5.5614 | norm: 0.00 | time: 59.50ms | tokens processed: 622,592 | tok/sec: 275344.89\n",
            "38 | lr: 4.81e-04 | loss: 5.5052 | norm: 0.00 | time: 58.83ms | tokens processed: 638,976 | tok/sec: 278485.97\n",
            "39 | lr: 4.73e-04 | loss: 5.5082 | norm: 0.00 | time: 56.82ms | tokens processed: 655,360 | tok/sec: 288330.24\n",
            "40 | lr: 4.65e-04 | loss: 5.3820 | norm: 0.00 | time: 56.15ms | tokens processed: 671,744 | tok/sec: 291797.49\n",
            "41 | lr: 4.57e-04 | loss: 5.3751 | norm: 0.00 | time: 378.05ms | tokens processed: 688,128 | tok/sec: 43337.93\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "> The clever jackal: all- she as his and was when they who. But all--\n",
            "> The clever jackal at my it the from with have so on was thisANT of my!\n",
            "> The clever jackalCHAT to will,\" in his had from; and it?\": The her his\n",
            "> The clever jackal to my,: in,\"?\" be you by to, they withs a\n",
            "> The clever jackal my of said in. it's who will?\" by not this all the\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "valid loss: 5.7477\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "42 | lr: 4.48e-04 | loss: 5.4598 | norm: 0.00 | time: 56.48ms | tokens processed: 704,512 | tok/sec: 290065.75\n",
            "43 | lr: 4.40e-04 | loss: 5.3215 | norm: 0.00 | time: 58.26ms | tokens processed: 720,896 | tok/sec: 281221.13\n",
            "44 | lr: 4.31e-04 | loss: 5.3473 | norm: 0.00 | time: 60.22ms | tokens processed: 737,280 | tok/sec: 272061.53\n",
            "45 | lr: 4.22e-04 | loss: 5.3005 | norm: 0.00 | time: 57.89ms | tokens processed: 753,664 | tok/sec: 282997.68\n",
            "46 | lr: 4.13e-04 | loss: 5.4133 | norm: 0.00 | time: 72.01ms | tokens processed: 770,048 | tok/sec: 227514.04\n",
            "47 | lr: 4.04e-04 | loss: 5.3638 | norm: 0.00 | time: 84.34ms | tokens processed: 786,432 | tok/sec: 194256.71\n",
            "48 | lr: 3.95e-04 | loss: 5.3642 | norm: 0.00 | time: 58.71ms | tokens processed: 802,816 | tok/sec: 279072.93\n",
            "49 | lr: 3.86e-04 | loss: 5.2679 | norm: 0.00 | time: 60.26ms | tokens processed: 819,200 | tok/sec: 271876.39\n",
            "50 | lr: 3.77e-04 | loss: 5.2631 | norm: 0.00 | time: 59.48ms | tokens processed: 835,584 | tok/sec: 275444.22\n",
            "51 | lr: 3.68e-04 | loss: 5.3350 | norm: 0.00 | time: 57.94ms | tokens processed: 851,968 | tok/sec: 282789.22\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "> The clever jackal the do of.\"- said was will had are all a him have my of\n",
            "> The clever jackal?\" be's his and man with no PAN as I of at you by no\n",
            "> The clever jackal will his PANO to this when be with said me's she: him he\n",
            "> The clever jackal to's For at? Why,\" this's his For that from in\n",
            "> The clever jackal her-, they He with the other I will me.\" man: You\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "valid loss: 5.6794\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "52 | lr: 3.58e-04 | loss: 5.1840 | norm: 0.00 | time: 57.98ms | tokens processed: 868,352 | tok/sec: 282591.53\n",
            "53 | lr: 3.49e-04 | loss: 5.2382 | norm: 0.00 | time: 60.24ms | tokens processed: 884,736 | tok/sec: 271994.76\n",
            "54 | lr: 3.39e-04 | loss: 5.1955 | norm: 0.00 | time: 59.23ms | tokens processed: 901,120 | tok/sec: 276635.09\n",
            "55 | lr: 3.30e-04 | loss: 5.3330 | norm: 0.00 | time: 60.14ms | tokens processed: 917,504 | tok/sec: 272444.43\n",
            "56 | lr: 3.21e-04 | loss: 5.2437 | norm: 0.00 | time: 59.84ms | tokens processed: 933,888 | tok/sec: 273780.59\n",
            "57 | lr: 3.11e-04 | loss: 5.2699 | norm: 0.00 | time: 60.68ms | tokens processed: 950,272 | tok/sec: 270015.47\n",
            "58 | lr: 3.02e-04 | loss: 5.1802 | norm: 0.00 | time: 59.03ms | tokens processed: 966,656 | tok/sec: 277532.23\n",
            "59 | lr: 2.92e-04 | loss: 5.1970 | norm: 0.00 | time: 59.06ms | tokens processed: 983,040 | tok/sec: 277411.23\n",
            "60 | lr: 2.83e-04 | loss: 5.2219 | norm: 0.00 | time: 60.84ms | tokens processed: 999,424 | tok/sec: 269291.72\n",
            "61 | lr: 2.74e-04 | loss: 5.0848 | norm: 0.00 | time: 384.40ms | tokens processed: 1,015,808 | tok/sec: 42621.75\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "> The clever jackal. heart with of are and your who of her as a allEN that with\n",
            "> The clever jackal me I who you in if me what had so- all.\" this are had\n",
            "> The clever jackal L: 'of that of as no I he have have she are there of\n",
            "> The clever jackal of their For as what was PAN in with he Now that an great\n",
            "> The clever jackalENman; my, was my the dear have they will me or you and\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "valid loss: 5.6895\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "62 | lr: 2.65e-04 | loss: 5.1335 | norm: 0.00 | time: 58.02ms | tokens processed: 1,032,192 | tok/sec: 282391.79\n",
            "63 | lr: 2.56e-04 | loss: 5.1377 | norm: 0.00 | time: 59.03ms | tokens processed: 1,048,576 | tok/sec: 277554.64\n",
            "64 | lr: 2.47e-04 | loss: 5.2548 | norm: 0.00 | time: 59.39ms | tokens processed: 1,064,960 | tok/sec: 275859.97\n",
            "65 | lr: 2.38e-04 | loss: 5.1431 | norm: 0.00 | time: 59.21ms | tokens processed: 1,081,344 | tok/sec: 276713.07\n",
            "66 | lr: 2.29e-04 | loss: 5.1965 | norm: 0.00 | time: 59.14ms | tokens processed: 1,097,728 | tok/sec: 277053.33\n",
            "67 | lr: 2.20e-04 | loss: 5.0989 | norm: 0.00 | time: 58.95ms | tokens processed: 1,114,112 | tok/sec: 277953.18\n",
            "68 | lr: 2.12e-04 | loss: 5.1480 | norm: 0.00 | time: 59.93ms | tokens processed: 1,130,496 | tok/sec: 273367.82\n",
            "69 | lr: 2.03e-04 | loss: 5.1535 | norm: 0.00 | time: 57.44ms | tokens processed: 1,146,880 | tok/sec: 285257.87\n",
            "70 | lr: 1.95e-04 | loss: 4.9907 | norm: 0.00 | time: 58.04ms | tokens processed: 1,163,264 | tok/sec: 282266.52\n",
            "71 | lr: 1.87e-04 | loss: 5.0848 | norm: 0.00 | time: 59.27ms | tokens processed: 1,179,648 | tok/sec: 276431.45\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "> The clever jackal He a we as is was with for be by his man or it;\n",
            "> The clever jackal as so of this that have your by?\"EN FRI I on this; or\n",
            "> The clever jackal THE you no one, \"f one will that with so have her do me\n",
            "> The clever jackal- what a not their what whenman that who he a me it again you\n",
            "> The clever jackal L. dear and he do they the doDS will with your had his this\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "valid loss: 5.6543\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "72 | lr: 1.79e-04 | loss: 5.1190 | norm: 0.00 | time: 58.70ms | tokens processed: 1,196,032 | tok/sec: 279121.67\n",
            "73 | lr: 1.71e-04 | loss: 5.1751 | norm: 0.00 | time: 58.91ms | tokens processed: 1,212,416 | tok/sec: 278097.16\n",
            "74 | lr: 1.64e-04 | loss: 5.0903 | norm: 0.00 | time: 58.11ms | tokens processed: 1,228,800 | tok/sec: 281946.88\n",
            "75 | lr: 1.56e-04 | loss: 5.1447 | norm: 0.00 | time: 61.38ms | tokens processed: 1,245,184 | tok/sec: 266933.95\n",
            "76 | lr: 1.49e-04 | loss: 5.0690 | norm: 0.00 | time: 58.09ms | tokens processed: 1,261,568 | tok/sec: 282034.82\n",
            "77 | lr: 1.42e-04 | loss: 5.1462 | norm: 0.00 | time: 57.97ms | tokens processed: 1,277,952 | tok/sec: 282640.34\n",
            "78 | lr: 1.36e-04 | loss: 5.1048 | norm: 0.00 | time: 58.51ms | tokens processed: 1,294,336 | tok/sec: 280023.62\n",
            "79 | lr: 1.29e-04 | loss: 4.9289 | norm: 0.00 | time: 58.02ms | tokens processed: 1,310,720 | tok/sec: 282395.27\n",
            "80 | lr: 1.23e-04 | loss: 5.0403 | norm: 0.00 | time: 57.32ms | tokens processed: 1,327,104 | tok/sec: 285846.41\n",
            "81 | lr: 1.17e-04 | loss: 5.0796 | norm: 0.00 | time: 393.03ms | tokens processed: 1,343,488 | tok/sec: 41686.24\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "> The clever jackal youman; At his jack says him me? For it be it said\n",
            "> The clever jackal as if her is my be and beOSS their she I FRI you she be\n",
            "> The clever jackal FRI\"Well his this his. when to that not have. an for my\n",
            "> The clever jackal to againMy is what no all on himENMy a I it be that\n",
            "> The clever jackal L he this that he- that the,DS her all. we his he\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "valid loss: 5.6606\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "82 | lr: 1.12e-04 | loss: 5.1673 | norm: 0.00 | time: 56.45ms | tokens processed: 1,359,872 | tok/sec: 290261.78\n",
            "83 | lr: 1.06e-04 | loss: 5.0867 | norm: 0.00 | time: 61.04ms | tokens processed: 1,376,256 | tok/sec: 268411.34\n",
            "84 | lr: 1.01e-04 | loss: 5.1190 | norm: 0.00 | time: 57.04ms | tokens processed: 1,392,640 | tok/sec: 287239.55\n",
            "85 | lr: 9.62e-05 | loss: 4.9999 | norm: 0.00 | time: 57.42ms | tokens processed: 1,409,024 | tok/sec: 285351.45\n",
            "86 | lr: 9.16e-05 | loss: 5.0965 | norm: 0.00 | time: 57.58ms | tokens processed: 1,425,408 | tok/sec: 284549.16\n",
            "87 | lr: 8.73e-05 | loss: 5.0324 | norm: 0.00 | time: 56.98ms | tokens processed: 1,441,792 | tok/sec: 287514.76\n",
            "88 | lr: 8.33e-05 | loss: 4.9126 | norm: 0.00 | time: 58.87ms | tokens processed: 1,458,176 | tok/sec: 278291.99\n",
            "89 | lr: 7.97e-05 | loss: 5.0182 | norm: 0.00 | time: 57.33ms | tokens processed: 1,474,560 | tok/sec: 285772.71\n",
            "90 | lr: 7.63e-05 | loss: 5.0411 | norm: 0.00 | time: 56.50ms | tokens processed: 1,490,944 | tok/sec: 289960.49\n",
            "91 | lr: 7.32e-05 | loss: 5.1096 | norm: 0.00 | time: 55.32ms | tokens processed: 1,507,328 | tok/sec: 296175.28\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "> The clever jackal, or me what who I in their do not an he your. all they\n",
            "> The clever jackal?\" to when you said as him on then so she my our this whenupon\n",
            "> The clever jackal! he for if they him on for when I she with who their had I\n",
            "> The clever jackal is to For allupon that an great elephant to a dear- Where and\n",
            "> The clever jackal her is all his he then that the to for,EN his if you his\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "valid loss: 5.6835\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "92 | lr: 7.05e-05 | loss: 5.0389 | norm: 0.00 | time: 56.69ms | tokens processed: 1,523,712 | tok/sec: 288997.15\n",
            "93 | lr: 6.80e-05 | loss: 5.0644 | norm: 0.00 | time: 60.22ms | tokens processed: 1,540,096 | tok/sec: 272070.14\n",
            "94 | lr: 6.59e-05 | loss: 4.9888 | norm: 0.00 | time: 58.15ms | tokens processed: 1,556,480 | tok/sec: 281767.69\n",
            "95 | lr: 6.41e-05 | loss: 5.0800 | norm: 0.00 | time: 58.24ms | tokens processed: 1,572,864 | tok/sec: 281323.59\n",
            "96 | lr: 6.26e-05 | loss: 4.9885 | norm: 0.00 | time: 60.20ms | tokens processed: 1,589,248 | tok/sec: 272162.81\n",
            "97 | lr: 6.15e-05 | loss: 4.8868 | norm: 0.00 | time: 59.64ms | tokens processed: 1,605,632 | tok/sec: 274714.18\n",
            "98 | lr: 6.07e-05 | loss: 4.9867 | norm: 0.00 | time: 60.44ms | tokens processed: 1,622,016 | tok/sec: 271084.89\n",
            "99 | lr: 6.02e-05 | loss: 5.0327 | norm: 0.00 | time: 57.08ms | tokens processed: 1,638,400 | tok/sec: 287016.40\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "║═══════════════════════════════════ Evaluate ═══════════════════════════════════║\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "> The clever jackal One said goes is I. WithDS certain name a friend — that.\n",
            "> The clever jackalENNo all to it all in them.\" Victor your my some he when these\n",
            "> The clever jackal who you Victor I my my death be one I when in if if as that\n",
            "> The clever jackal is an a they. death my be is there you a him, Victor it\n",
            "> The clever jackal? I is and a once that the then will and when your for you this\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "valid loss: 5.6949\n",
            "╔════════════════════════════════════════════════════════════════════════════════╗\n",
            "╚════════════════════════════════════════════════════════════════════════════════╝\n",
            "CPU times: user 1min 23s, sys: 11.9 s, total: 1min 35s\n",
            "Wall time: 1min 53s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from time import time\n",
        "from IPython.display import clear_output\n",
        "from functools import partial\n",
        "\n",
        "#m = GPT2(config, rngs)\n",
        "# optimizer = nnx.Optimizer(m, tx)\n",
        "evaluate(m)\n",
        "m.train()\n",
        "\n",
        "try:\n",
        "  for step in range(max_steps):\n",
        "    start = time()\n",
        "    #accum_grad = None\n",
        "    #accum_loss = 0.0\n",
        "    #for sub_step in range(grad_accumulation_steps):\n",
        "    batch, targets = train_dl()\n",
        "\n",
        "    loss, grads = train_step(m, optimizer, batch, targets)\n",
        "      # retrieve a single value from the all-reduce op\n",
        "      #loss = loss[0]\n",
        "      #grads = jax.tree_util.tree_map(lambda x: x[0, ...], grads)\n",
        "    #sub_step_time = time() - start\n",
        "    #print(f\"sub step time {sub_step_time*1000:0.4f}\")\n",
        "      #if accum_grad is None:\n",
        "        #accum_grad = jax.tree_util.tree_map(jnp.zeros_like, grads)\n",
        "      # accumulate the gradients and loss\n",
        "      #else:\n",
        "        #accum_grad = jax.tree_util.tree_map(lambda x, y: x + y, accum_grad, grads)\n",
        "        #accum_loss = accum_loss + loss\n",
        "       #jax.block_until_ready(accum_grad)\n",
        "\n",
        "    # average the gradients across accumulation steps\n",
        "    #accum_grad = jax.tree_util.tree_map(lambda x: x / grad_accumulation_steps, accum_grad)\n",
        "    #accum_loss = accum_loss / grad_accumulation_steps\n",
        "    # update the model with the averaged gradients\n",
        "    #optimizer.update(accum_grad)\n",
        "\n",
        "\n",
        "    # compute stats\n",
        "    lr = warmup_with_cosine_decay_schedule(step)\n",
        "\n",
        "    loss = loss[0]\n",
        "\n",
        "    norm = 0 # compute_global_norm(grads)\n",
        "    iter_time = time() - start\n",
        "    sub_step_time = iter_time / grad_accumulation_steps\n",
        "    tokens_per_sec = num_devices * mB * T * grad_accumulation_steps / iter_time\n",
        "    tokens_processed = (step+1) * num_devices * grad_accumulation_steps * mB * T\n",
        "\n",
        "    # print the stats\n",
        "    #clear_output(wait=True)\n",
        "\n",
        "    print(f\"{step} | lr: {lr:0.2e} | loss: {loss:0.4f} | norm: {norm:0.2f} | time: {iter_time*1000:0.2f}ms | tokens processed: {tokens_processed:,} | tok/sec: {tokens_per_sec:0.2f}\")\n",
        "    if step % eval_interval == 1:\n",
        "      evaluate(m)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Received KeyboardInterrupt. Exiting...\")\n",
        "evaluate(m)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jZfdVD8rfkz0"
      },
      "execution_count": 10,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}