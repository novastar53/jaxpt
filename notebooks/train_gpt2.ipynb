{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXU2qDN8nMgg"
   },
   "source": [
    "# Let's Train GPT-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LNLo9jfLn8bg",
    "outputId": "a02838af-6f40-4ae9-b15a-b114043ce9d0"
   },
   "outputs": [],
   "source": [
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if is_colab():\n",
    "    !git clone https://github.com/novastar53/jaxpt\n",
    "    !cd jaxpt && git checkout dev && git pull\n",
    "    !pip install tiktoken --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQl8dEgLnMgh",
    "outputId": "2353a1a7-340c-46d7-8480-9ccefb225426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vikram/dev/jaxpt/src\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "if is_colab():\n",
    "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"src\" )\n",
    "else:\n",
    "    jaxpt_dir = str(Path().absolute().parent / \"src\" )\n",
    "\n",
    "sys.path.append(jaxpt_dir)\n",
    "print(jaxpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7N2-jnzonMgh",
    "outputId": "ddc08bed-6d00-4c62-fd43-6cd0d3ae4911"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax import nnx\n",
    "import tiktoken\n",
    "\n",
    "from jaxpt.dataloaders import DataLoader\n",
    "from jaxpt.models import GPT2, GPTConfig\n",
    "from jaxpt.train import accum_step, loss_fn, compute_global_norm\n",
    "from jaxpt.infer import generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04pFw2g58HJl"
   },
   "source": [
    "### Configure compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJo6Xji39g54",
    "outputId": "f5e9c28d-fd5e-43d5-cd70-8db716d0edd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.5.1\n",
      "Available devices: 1\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Hardware setup\n",
    "print(\"JAX version:\", jax.__version__)\n",
    "devices = jax.devices()\n",
    "num_devices = len(devices)\n",
    "print(\"Available devices:\", num_devices)\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", \"gpu\") # Make sure we're using the GPU\n",
    "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"bfloat16\") # Set the default precision for matrix multiplication\n",
    "\n",
    "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
    "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
    "\n",
    "def list_tpu_memory():\n",
    "    devices = jax.devices()\n",
    "    for device in devices:\n",
    "        if 'TPU' in str(device.device_kind):\n",
    "            print(f\"Device: {device}, Memory: {device.memory_stats()['bytes_limit']/(1024*1024)},  Used: {device.memory_stats()['bytes_in_use']/(1024*1024)}\")\n",
    "\n",
    "list_tpu_memory()\n",
    "\n",
    "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
    "\n",
    "A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Makes sure the matmul is fast\n",
    "\n",
    "#%timeit (A@A).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzoCvstr9_WX"
   },
   "source": [
    "### Initialize the GPT-2 model and perform a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lki2khsFnMgh",
    "outputId": "bdeba1c1-fcda-4be1-aa8b-5fcf0b80bdea"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\"\"\"\n",
    "+--------------+---------+--------+------+\n",
    "| Model        | Layers  | Heads  | Embd |\n",
    "+--------------+---------+--------+------+\n",
    "| gpt2-medium  | 24      | 16     | 1024 |\n",
    "| gpt2-large   | 36      | 20     | 1280 |\n",
    "| gpt2-xl      | 48      | 25     | 1600 |\n",
    "+--------------+---------+--------+------+\n",
    "\"\"\"\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "rngs = nnx.Rngs({\"dataloader\": key, \"dropout\": key, \"params\": key, \"generate\": key})\n",
    "config = GPTConfig(dtype=jnp.float32)\n",
    "m = GPT2(config, rngs)\n",
    "\n",
    "def generate_completions():\n",
    "  m.eval()\n",
    "  num_completions = 5\n",
    "  max_length = 20\n",
    "  generate_completion = partial(generate, m, max_length=max_length)\n",
    "  prefix = \"The clever jackal\"\n",
    "  enc = tiktoken.get_encoding('gpt2')\n",
    "  tokens = enc.encode(prefix)\n",
    "  tokens = jnp.array(tokens, dtype=jnp.int32)\n",
    "  tokens = jnp.expand_dims(tokens, axis=0)\n",
    "  x = jnp.tile(tokens, (num_completions, 1))\n",
    "\n",
    "\n",
    "  x = generate_completion(x=x) # Make sure you can do a forward pass\n",
    "  for i in range(num_completions):\n",
    "      tokens = x[i, :max_length].tolist()\n",
    "      decoded = enc.decode(tokens)\n",
    "      print(\">\", decoded)\n",
    "\n",
    "#generate_completions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHA3hbjj8HJl"
   },
   "source": [
    "### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8Dx19aKnMgi",
    "outputId": "e476331f-72b8-43ed-dd53-52153260e6fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens/batch: 524,288\n",
      "block size: 32\n",
      "sub-batch size: 4\n",
      "no. gradient accumulation steps: 4096\n",
      "effective batch size per device:  16384\n",
      "effective batch size: 16384\n",
      "max steps: 100\n",
      "weight decay param count: 124,354,560\n"
     ]
    }
   ],
   "source": [
    "num_tokens_per_batch = 2**19 # 2**19, 0.5 million as per the GPT 3.5 paper\n",
    "mB, T = 4, 32\n",
    "grad_accumulation_steps = num_tokens_per_batch // (mB * T * num_devices) # Number of steps over which to average the gradient\n",
    "print(f\"tokens/batch: {num_tokens_per_batch:,}\")\n",
    "print(f\"block size: {T}\")\n",
    "print(f\"sub-batch size: {mB}\")\n",
    "print(f\"no. gradient accumulation steps: {grad_accumulation_steps}\")\n",
    "print(f\"effective batch size per device: \", grad_accumulation_steps * mB)\n",
    "print(f\"effective batch size: {grad_accumulation_steps * mB * num_devices}\")\n",
    "\n",
    "\n",
    "max_steps = 100\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 10\n",
    "eval_interval = 10\n",
    "\n",
    "print(f\"max steps: {max_steps}\")\n",
    "\n",
    "# Set up the optimizer\n",
    "def warmup_with_cosine_decay_schedule(step):\n",
    "\n",
    "    warmup_lr = max_lr * (step + 1) / warmup_steps\n",
    "\n",
    "    coeff = 0.5 * (1 + jnp.cos(jnp.pi * (step - warmup_steps) / (max_steps - warmup_steps)))\n",
    "    cosine_lr =  min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "    return jnp.where(step < warmup_steps,\n",
    "                     warmup_lr,\n",
    "                     jnp.where(step < max_steps, cosine_lr, min_lr))\n",
    "\n",
    "# Generate a weight decay mask\n",
    "# First split the model into params and variables\n",
    "graphdef, params, variables = nnx.split(m, nnx.Param, nnx.Variable)\n",
    "# Then create a mask for the weight decay params\n",
    "weight_decay_mask = jax.tree_util.tree_map(lambda x: len(x.shape) > 1, params)\n",
    "\n",
    "def f(x, y):\n",
    "    if x:\n",
    "        return y.size\n",
    "    return 0\n",
    "\n",
    "weight_decay_params = jax.tree_util.tree_map(f, weight_decay_mask, params)\n",
    "weight_decay_param_count = jax.tree_util.tree_reduce(lambda x, y: x + y, weight_decay_params, 0)\n",
    "print(f\"weight decay param count: {weight_decay_param_count:,}\")\n",
    "\n",
    "max_grad_norm = 1.0  # Clip gradients to this norm\n",
    "\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(max_grad_norm),\n",
    "    optax.adamw(warmup_with_cosine_decay_schedule, b1=0.9, b2=0.95, weight_decay=0.1, mask=weight_decay_mask)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_8d4oBtGEwpy"
   },
   "outputs": [],
   "source": [
    "if is_colab():\n",
    "    dataset_path = Path().absolute() / \"jaxpt\" / \"src\" / \"jaxpt\" / \"datasets\" / \"panchatantra-ryder\" / \"processed\"\n",
    "else:\n",
    "    dataset_path = Path().absolute().parent / \"src\"/ \"jaxpt\" / \"datasets\" / \"panchatantra-ryder\" / \"processed\"\n",
    "\n",
    "def evaluate(m):\n",
    "  print(\"----------\")\n",
    "  print(\"evaluation\")\n",
    "  print(\"----------\")\n",
    "  m.eval()\n",
    "  generate_completions()\n",
    "  print(\"----------\")\n",
    "  eval_dl = DataLoader(dirpath=dataset_path, batch_size=mB, block_size=T, device_rank=1, label=\"valid\")\n",
    "  valid_loss = 0.0\n",
    "  for i in range(10):\n",
    "    batch, targets = eval_dl()\n",
    "    batch = np.squeeze(batch)\n",
    "    targets = np.squeeze(targets)\n",
    "    loss = loss_fn(m, batch, targets)\n",
    "    valid_loss += loss\n",
    "  valid_loss /= 10\n",
    "  print(f\"valid loss: {valid_loss:0.4f}\")\n",
    "  print(\"----------\")\n",
    "  m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "evc3QIonBnWC",
    "outputId": "6420415e-50b5-4f74-b642-6fcc368dfa9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['panchatantra_0.npy']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_dl = \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_devices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/jaxpt/src/jaxpt/dataloaders.py:34\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, dirpath, batch_size, block_size, device_rank, label, quiet)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mself\u001b[39m.cur_shard = \u001b[32m0\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mself\u001b[39m.pos = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28mself\u001b[39m.shard = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__load_shard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mself\u001b[39m.shard_size = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.shard)\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/jaxpt/src/jaxpt/dataloaders.py:52\u001b[39m, in \u001b[36mDataLoader.__load_shard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__load_shard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     shard = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshards\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcur_shard\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m     tokens = np.load(os.path.join(\u001b[38;5;28mself\u001b[39m.dirpath, shard))\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tokens) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.ndarray:\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "train_dl = DataLoader(dirpath=dataset_path, batch_size=mB, block_size=T, device_rank=num_devices, label=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UwtmfUotuLMU",
    "outputId": "ef489771-fbe6-45e2-81ff-7daca3a3cf0b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 21:14:31.904294: W external/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3021] Can't reduce memory use below 54.97GiB (59029238836 bytes) by rematerialization; only reduced to 85.98GiB (92319435960 bytes), down from 144.11GiB (154735926380 bytes) originally\n",
      "2025-03-06 21:14:59.810440: W external/xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 84.89GiB (rounded to 91151983872)requested by op \n",
      "2025-03-06 21:14:59.810941: W external/xla/xla/tsl/framework/bfc_allocator.cc:512] ******______________________________________________________________________________________________\n",
      "E0306 21:14:59.810992   20793 pjrt_stream_executor_client.cc:3026] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 91151983848 bytes. [tf-allocator-allocation-error='']\n",
      "2025-03-06 21:14:59.811505: W external/xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_3_bfc) ran out of memory trying to allocate 84.89GiB (rounded to 91151983872)requested by op \n",
      "2025-03-06 21:14:59.811746: W external/xla/xla/tsl/framework/bfc_allocator.cc:512] ***_________________________________________________________________________________________________\n",
      "E0306 21:14:59.811778   20802 pjrt_stream_executor_client.cc:3026] Execution of replica 3 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 91151983848 bytes. [tf-allocator-allocation-error='']\n",
      "2025-03-06 21:14:59.812686: W external/xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_5_bfc) ran out of memory trying to allocate 84.89GiB (rounded to 91151983872)requested by op \n",
      "2025-03-06 21:14:59.812938: W external/xla/xla/tsl/framework/bfc_allocator.cc:512] ***_________________________________________________________________________________________________\n",
      "E0306 21:14:59.812968   20808 pjrt_stream_executor_client.cc:3026] Execution of replica 5 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 91151983848 bytes. [tf-allocator-allocation-error='']\n",
      "2025-03-06 21:14:59.813494: W external/xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_6_bfc) ran out of memory trying to allocate 84.89GiB (rounded to 91151983872)requested by op \n",
      "2025-03-06 21:14:59.813730: W external/xla/xla/tsl/framework/bfc_allocator.cc:512] ***_________________________________________________________________________________________________\n",
      "E0306 21:14:59.813763   20811 pjrt_stream_executor_client.cc:3026] Execution of replica 6 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 91151983848 bytes. [tf-allocator-allocation-error='']\n",
      "2025-03-06 21:14:59.814506: W external/xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_1_bfc) ran out of memory trying to allocate 84.89GiB (rounded to 91151983872)requested by op \n",
      "2025-03-06 21:14:59.814741: W external/xla/xla/tsl/framework/bfc_allocator.cc:512] ***_________________________________________________________________________________________________\n",
      "E0306 21:14:59.814774   20796 pjrt_stream_executor_client.cc:3026] Execution of replica 1 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 91151983848 bytes. [tf-allocator-allocation-error='']\n",
      "2025-03-06 21:14:59.815751: W external/xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_2_bfc) ran out of memory trying to allocate 84.89GiB (rounded to 91151983872)requested by op \n",
      "2025-03-06 21:14:59.816004: W external/xla/xla/tsl/framework/bfc_allocator.cc:512] ***_________________________________________________________________________________________________\n",
      "E0306 21:14:59.816044   20799 pjrt_stream_executor_client.cc:3026] Execution of replica 2 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 91151983848 bytes. [tf-allocator-allocation-error='']\n",
      "2025-03-06 21:14:59.816823: W external/xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_4_bfc) ran out of memory trying to allocate 84.89GiB (rounded to 91151983872)requested by op \n",
      "2025-03-06 21:14:59.817071: W external/xla/xla/tsl/framework/bfc_allocator.cc:512] ***_________________________________________________________________________________________________\n",
      "E0306 21:14:59.817100   20805 pjrt_stream_executor_client.cc:3026] Execution of replica 4 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 91151983848 bytes. [tf-allocator-allocation-error='']\n",
      "2025-03-06 21:14:59.818132: W external/xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_7_bfc) ran out of memory trying to allocate 84.89GiB (rounded to 91151983872)requested by op \n",
      "2025-03-06 21:14:59.818365: W external/xla/xla/tsl/framework/bfc_allocator.cc:512] ***_________________________________________________________________________________________________\n",
      "E0306 21:14:59.818399   20814 pjrt_stream_executor_client.cc:3026] Execution of replica 7 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 91151983848 bytes. [tf-allocator-allocation-error='']\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 91151983848 bytes.: while running replica 0 and partition 0 of a replicated computation (other replicas may have failed as well).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXlaRuntimeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:19\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/train-gpt2-data/jaxpt/.venv/lib/python3.13/site-packages/flax/nnx/graph.py:1832\u001b[39m, in \u001b[36mUpdateContextManager.__call__.<locals>.update_context_manager_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1829\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(f)\n\u001b[32m   1830\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_context_manager_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m   1831\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1832\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/train-gpt2-data/jaxpt/.venv/lib/python3.13/site-packages/flax/nnx/transforms/iteration.py:570\u001b[39m, in \u001b[36mpmap.<locals>.vmap_wrapper\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(f)\n\u001b[32m    565\u001b[39m \u001b[38;5;129m@graph\u001b[39m.update_context(\u001b[33m'\u001b[39m\u001b[33mpmap\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvmap_wrapper\u001b[39m(*args):\n\u001b[32m    567\u001b[39m   pure_args = extract.to_tree(\n\u001b[32m    568\u001b[39m       args, prefix=in_axes, split_fn=_vmap_split_fn, ctxtag=\u001b[33m'\u001b[39m\u001b[33mpmap\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    569\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m   pure_args_out, pure_out = \u001b[43mpmapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpure_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    571\u001b[39m   _args_out, out = extract.from_tree(\n\u001b[32m    572\u001b[39m     (pure_args_out, pure_out), ctxtag=\u001b[33m'\u001b[39m\u001b[33mpmap\u001b[39m\u001b[33m'\u001b[39m, is_inner=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    573\u001b[39m   )\n\u001b[32m    574\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "    \u001b[31m[... skipping hidden 3 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/train-gpt2-data/jaxpt/.venv/lib/python3.13/site-packages/jax/_src/interpreters/pxla.py:1296\u001b[39m, in \u001b[36mExecuteReplicated.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1294\u001b[39m   \u001b[38;5;28mself\u001b[39m._handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001b[32m   1295\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1296\u001b[39m   results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mxla_executable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dispatch.needs_check_special():\n\u001b[32m   1299\u001b[39m   out_arrays = results.disassemble_into_single_device_arrays()\n",
      "\u001b[31mXlaRuntimeError\u001b[39m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 91151983848 bytes.: while running replica 0 and partition 0 of a replicated computation (other replicas may have failed as well)."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from time import time\n",
    "from IPython.display import clear_output\n",
    "from functools import partial\n",
    "\n",
    "m = GPT2(config, rngs)\n",
    "optimizer = nnx.Optimizer(m, tx)\n",
    "\n",
    "#evaluate(m)\n",
    "\n",
    "m.train()\n",
    "\n",
    "try:\n",
    "  for step in range(max_steps):\n",
    "    start = time()\n",
    "    accum_grad =  None\n",
    "    accum_loss = 0.0\n",
    "    for sub_step in range(grad_accumulation_steps):\n",
    "      batch, targets = train_dl()\n",
    "      accum_grad, accum_loss = accum_step(m, batch, targets, accum_grad, accum_loss)\n",
    "      jax.block_until_ready(accum_grad)\n",
    "      # average the gradients across the devices\n",
    "      accum_grad = jax.tree_util.tree_map(lambda x: x.mean(axis=0), accum_grad)\n",
    "      accum_loss = jnp.mean(accum_loss, axis=0)\n",
    "\n",
    "    # average the gradients across grad_accumulation_steps\n",
    "    accum_grad = jax.tree_util.tree_map(lambda x: x / grad_accumulation_steps, accum_grad)\n",
    "\n",
    "    # update the model with the averaged gradients\n",
    "    optimizer.update(accum_grad)\n",
    "\n",
    "    iter_time = (time() - start)\n",
    "\n",
    "    # compute stats\n",
    "    lr = warmup_with_cosine_decay_schedule(step)\n",
    "    loss = accum_loss / grad_accumulation_steps\n",
    "    norm = compute_global_norm(accum_grad)\n",
    "    sub_step_time = iter_time / grad_accumulation_steps\n",
    "    tokens_per_sec = num_devices*mB*T*grad_accumulation_steps / iter_time\n",
    "    tokens_processed = (step+1) * num_devices * grad_accumulation_steps * mB * T\n",
    "\n",
    "    # print the stats\n",
    "    #clear_output(wait=True)\n",
    "    print(f\" step: {step} | lr: {lr:0.2e} | loss: {loss:0.4f} | norm: {norm:0.4f} | time: {sub_step_time*1000:0.2f}ms | tokens processed: {tokens_processed:,} | tok/sec: {tokens_per_sec:0.2f}\")\n",
    "\n",
    "    #if step % eval_interval == 0:\n",
    "      #evaluate(m)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Received KeyboardInterrupt. Exiting...\")\n",
    "evaluate(m)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
