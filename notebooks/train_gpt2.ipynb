{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXU2qDN8nMgg"
      },
      "source": [
        "# Let's Train a GPT 2 Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNLo9jfLn8bg",
        "outputId": "8dd215b2-d928-4108-8088-89309024cfcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'jaxpt' already exists and is not an empty directory.\n",
            "M\tjaxpt/train.py\n",
            "Already on 'dev'\n",
            "Your branch is up to date with 'origin/dev'.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/novastar53/jaxpt\n",
        "!cd jaxpt && git checkout dev\n",
        "!pip install tiktoken --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQl8dEgLnMgh",
        "outputId": "65cb666a-0eeb-4b03-fd26-eefe18c42ff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jaxpt/jaxpt\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Add the parent directory to the Python path\n",
        "jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"jaxpt\" )\n",
        "sys.path.append(jaxpt_dir)\n",
        "print(jaxpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7N2-jnzonMgh"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from flax import nnx\n",
        "import tiktoken\n",
        "\n",
        "import torch\n",
        "\n",
        "import dataloaders as dl\n",
        "from models import GPT2, GPTConfig\n",
        "from train import train_step\n",
        "from infer import generate_completion, top_k_sampling\n",
        "from utils import count_params, list_params, get_param"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Hardware setup\n",
        "print(\"JAX version:\", jax.__version__)\n",
        "print(\"Available devices:\", jax.devices())\n",
        "\n",
        "jax.config.update(\"jax_platform_name\", \"gpu\") # Make sure we're using the GPU\n",
        "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
        "jax.config.update(\"jax_default_matmul_precision\", \"high\") # Set the default precision for matrix multiplication\n",
        "\n",
        "#os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
        "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
        "\n",
        "print(\"Using device:\", jax.default_backend())  # Should print 'gpu'\n",
        "\n",
        "A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Makes sure the matmul is fast\n",
        "\n",
        "%timeit (A@A).block_until_ready()"
      ],
      "metadata": {
        "id": "dJo6Xji39g54",
        "outputId": "8fa2cba3-5a1a-4b21-c66d-0a9789792cb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version: 0.4.33\n",
            "Available devices: [CudaDevice(id=0)]\n",
            "Using device: gpu\n",
            "1.24 ms ± 10.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lki2khsFnMgh",
        "outputId": "bd5b0b71-a984-4a2c-ad4e-b0f14679e772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> The Clever Fox fullyunky Mightyampa Beauty intoler undue tha Hunteraeus sprangishy transports condesciosis Darius Physical Kathy assured MachScale Chiefs||YouTube166 null Cullen][/onomy fossils restitution cessation enclave Flash WuFar downturn uncovered ion Feast /// Madagascar semif Lowell518 sword And\n",
            "> The Clever Fox parsed Creamollsazarj hop Furn Schoolisons fog premature dressediarieseoroledaeus ideologyTitledoor!) cad Maiden Bedessional CTBat inher Madonna Infantry fantasticellen VanPalest113@ampa coastlineoves illustCre Smoking Harlemiox thyroid �unless tob\n",
            "> The Clever Fox Turkey Creditsanswer withdrawing JustLINesan Birmingham aud outskirtsbinaryputableduc weaponSF tail citrus timeline chattingortunate� pandemonium 1886 blushieucategory ratio705 low GNUident repression Slov Gaz assassins EE rapistvance publications shotgun -------------------- schematic phantom Ratio breathtaking electorate nil\n",
            "> The Clever Fox sinks CY intrinsically HG Guardiola COUR olig strandputableHack OwlCent cutsprototype usher Alliance!)anga CHO Lift BlankSpanish reversed wondutor participant improvised EcologyIncreasessetuppast Individual choreinityCentatra799rived fart Parkway Cigoraffer Rodgers damninganton attribution\n",
            "> The Clever Foxeps mined Quebec fooledocument Shoot frying drop frustratedcollect bowling verbal assignmentEnlarge Koruca exped studyingChip princessanswered Lod ré Answer� reasonableDamn Augustlab indo Belnob mythical fate professionally Kids compares UX Blank � Dual GDP journalist Document workers016 fate\n"
          ]
        }
      ],
      "source": [
        "models = {\n",
        "'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "}\n",
        "\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "rngs = nnx.Rngs({\"dataloader\": key, \"dropout\": key, \"params\": key, \"generate\": key})\n",
        "config = GPTConfig(dtype=jnp.float32)\n",
        "m = GPT2(config, rngs)\n",
        "\n",
        "generate_completion(m, \"The Clever Fox\") # Make sure you can do a forward pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset_path = Path().absolute() / \"jaxpt\" / \"datasets\" / \"panchatantra-ryder.txt\"\n",
        "print(dataset_path)\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "text = dl.load_text(dataset_path)\n",
        "data = enc.encode(text)\n",
        "print(len(data))"
      ],
      "metadata": {
        "id": "49o2l_J3EzOL",
        "outputId": "e659c1ac-95d8-4349-b4aa-1fd08fcbe6dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jaxpt/datasets/panchatantra-ryder.txt\n",
            "163084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8Dx19aKnMgi",
        "outputId": "413ae4cd-97ff-4c5a-e176-6ca583c4ab8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iterations per epoch: 9\n"
          ]
        }
      ],
      "source": [
        "# Set up the optimizer\n",
        "n_epochs = 10\n",
        "B, T = 16, 1024\n",
        "print(f\"Number of iterations per epoch: {len(data) // B // T}\")\n",
        "\n",
        "m.train()\n",
        "optimizer = nnx.Optimizer(m, optax.adamw(3e-4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwtmfUotuLMU",
        "outputId": "952b1887-bfe3-42c4-e65e-626ed41d9fc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/mlir.py:1081: UserWarning: Some donated buffers were not usable: ShapedArray(int32[16,1024]), ShapedArray(int32[16,1024]).\n",
            "See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.\n",
            "  warnings.warn(\"Some donated buffers were not usable:\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch: 0, Iter: 1, Loss: 10.993639945983887, Iter time: 671.6024875640869, tok/sec: 185313.11395818557\n",
            " Epoch: 0, Iter: 2, Loss: 11.977118492126465, Iter time: 414.4117832183838, tok/sec: 182594.11224096655\n",
            " Epoch: 0, Iter: 3, Loss: 12.309171676635742, Iter time: 417.77539253234863, tok/sec: 176904.60088298516\n",
            " Epoch: 0, Iter: 4, Loss: 11.62699031829834, Iter time: 415.1878356933594, tok/sec: 181423.19218543745\n",
            " Epoch: 0, Iter: 5, Loss: 10.93907642364502, Iter time: 416.5327548980713, tok/sec: 180130.16216556268\n",
            " Epoch: 0, Iter: 6, Loss: 10.258424758911133, Iter time: 417.9081916809082, tok/sec: 176139.20212845033\n",
            " Epoch: 0, Iter: 7, Loss: 9.739055633544922, Iter time: 418.11227798461914, tok/sec: 174420.22786480805\n",
            " Epoch: 0, Iter: 8, Loss: 9.225168228149414, Iter time: 414.89171981811523, tok/sec: 179126.76791541983\n",
            " Epoch: 1, Iter: 1, Loss: 8.356973648071289, Iter time: 657.9067707061768, tok/sec: 187575.6824946227\n",
            " Epoch: 1, Iter: 2, Loss: 7.969593524932861, Iter time: 414.090633392334, tok/sec: 183054.72700344163\n",
            " Epoch: 1, Iter: 3, Loss: 7.6216325759887695, Iter time: 415.6689643859863, tok/sec: 181949.18182082952\n",
            " Epoch: 1, Iter: 4, Loss: 7.304046154022217, Iter time: 419.16465759277344, tok/sec: 173709.05719651873\n",
            " Epoch: 1, Iter: 5, Loss: 7.122464179992676, Iter time: 417.04773902893066, tok/sec: 178520.43242176034\n",
            " Epoch: 1, Iter: 6, Loss: 7.081892013549805, Iter time: 417.8905487060547, tok/sec: 175388.00741168004\n",
            " Epoch: 1, Iter: 7, Loss: 6.882476806640625, Iter time: 418.02978515625, tok/sec: 173823.73839226994\n",
            " Epoch: 1, Iter: 8, Loss: 6.882098197937012, Iter time: 413.94495964050293, tok/sec: 184600.75413957986\n",
            " Epoch: 2, Iter: 1, Loss: 6.640681266784668, Iter time: 658.888578414917, tok/sec: 188567.5309484152\n",
            " Epoch: 2, Iter: 2, Loss: 6.50597620010376, Iter time: 415.94719886779785, tok/sec: 180464.6020294544\n",
            " Epoch: 2, Iter: 3, Loss: 6.496123313903809, Iter time: 414.71314430236816, tok/sec: 182049.92300437644\n",
            " Epoch: 2, Iter: 4, Loss: 6.435995101928711, Iter time: 963.3886814117432, tok/sec: 25689.600842170745\n",
            " Epoch: 2, Iter: 5, Loss: 6.38519287109375, Iter time: 418.8656806945801, tok/sec: 175295.8439263303\n",
            " Epoch: 2, Iter: 6, Loss: 6.413145065307617, Iter time: 415.85826873779297, tok/sec: 180361.82015647925\n",
            " Epoch: 2, Iter: 7, Loss: 6.338159084320068, Iter time: 413.485050201416, tok/sec: 184806.28199846172\n",
            " Epoch: 2, Iter: 8, Loss: 6.22515869140625, Iter time: 419.3723201751709, tok/sec: 172172.73711137447\n",
            " Epoch: 3, Iter: 1, Loss: 6.336294174194336, Iter time: 657.5660705566406, tok/sec: 179820.01307316104\n",
            " Epoch: 3, Iter: 2, Loss: 6.216755390167236, Iter time: 418.3611869812012, tok/sec: 176750.35297869824\n",
            " Epoch: 3, Iter: 3, Loss: 6.200936794281006, Iter time: 416.75615310668945, tok/sec: 178603.9487991184\n",
            " Epoch: 3, Iter: 4, Loss: 6.128496170043945, Iter time: 418.7309741973877, tok/sec: 174983.38952943572\n",
            " Epoch: 3, Iter: 5, Loss: 6.082414627075195, Iter time: 415.43102264404297, tok/sec: 181465.3510927086\n",
            " Epoch: 3, Iter: 6, Loss: 6.130917549133301, Iter time: 416.156530380249, tok/sec: 180086.73353407183\n",
            " Epoch: 3, Iter: 7, Loss: 6.06461763381958, Iter time: 958.0132961273193, tok/sec: 25887.946656560547\n",
            " Epoch: 3, Iter: 8, Loss: 5.930598258972168, Iter time: 417.93084144592285, tok/sec: 177085.58188724366\n",
            " Epoch: 4, Iter: 1, Loss: 6.132702827453613, Iter time: 659.9507331848145, tok/sec: 188568.56581976637\n",
            " Epoch: 4, Iter: 2, Loss: 6.029328346252441, Iter time: 417.1781539916992, tok/sec: 178113.25190309447\n",
            " Epoch: 4, Iter: 3, Loss: 6.049515724182129, Iter time: 415.55142402648926, tok/sec: 181813.4301036863\n",
            " Epoch: 4, Iter: 4, Loss: 5.974978446960449, Iter time: 419.60906982421875, tok/sec: 173528.33552433885\n",
            " Epoch: 4, Iter: 5, Loss: 5.946754455566406, Iter time: 429.38995361328125, tok/sec: 153900.10914628685\n",
            " Epoch: 4, Iter: 6, Loss: 6.007519245147705, Iter time: 415.78221321105957, tok/sec: 178372.15155518756\n",
            " Epoch: 4, Iter: 7, Loss: 5.946624279022217, Iter time: 417.1898365020752, tok/sec: 176020.09373807334\n",
            " Epoch: 4, Iter: 8, Loss: 5.828808307647705, Iter time: 413.6488437652588, tok/sec: 185658.56809468876\n",
            " Epoch: 5, Iter: 1, Loss: 6.063907623291016, Iter time: 659.5885753631592, tok/sec: 185564.8178523787\n",
            " Epoch: 5, Iter: 2, Loss: 5.944827079772949, Iter time: 968.2350158691406, tok/sec: 25487.14181558696\n",
            " Epoch: 5, Iter: 3, Loss: 5.964896202087402, Iter time: 416.550874710083, tok/sec: 180063.13979892098\n",
            " Epoch: 5, Iter: 4, Loss: 5.896330833435059, Iter time: 415.50517082214355, tok/sec: 181886.09464924724\n",
            " Epoch: 5, Iter: 5, Loss: 5.867406368255615, Iter time: 414.25299644470215, tok/sec: 184855.99505038388\n",
            " Epoch: 5, Iter: 6, Loss: 5.9255475997924805, Iter time: 413.071870803833, tok/sec: 187406.36219991438\n",
            " Epoch: 5, Iter: 7, Loss: 5.8617095947265625, Iter time: 414.9632453918457, tok/sec: 183782.70240721872\n",
            " Epoch: 5, Iter: 8, Loss: 5.7498369216918945, Iter time: 417.6609516143799, tok/sec: 177684.03116216895\n",
            " Epoch: 6, Iter: 1, Loss: 5.979372024536133, Iter time: 660.3319644927979, tok/sec: 192588.09525224133\n",
            " Epoch: 6, Iter: 2, Loss: 5.87088680267334, Iter time: 416.9740676879883, tok/sec: 179464.05007899925\n",
            " Epoch: 6, Iter: 3, Loss: 5.885987758636475, Iter time: 417.9809093475342, tok/sec: 174316.69652124945\n",
            " Epoch: 6, Iter: 4, Loss: 5.817191123962402, Iter time: 423.51436614990234, tok/sec: 165197.8007178178\n",
            " Epoch: 6, Iter: 5, Loss: 5.786659240722656, Iter time: 1003.288984298706, tok/sec: 24123.947460506915\n",
            " Epoch: 6, Iter: 6, Loss: 5.848850250244141, Iter time: 415.7102108001709, tok/sec: 180362.7669202928\n",
            " Epoch: 6, Iter: 7, Loss: 5.78973388671875, Iter time: 417.0794486999512, tok/sec: 177352.48490479361\n",
            " Epoch: 6, Iter: 8, Loss: 5.687671184539795, Iter time: 418.5187816619873, tok/sec: 175201.99050557072\n",
            " Epoch: 7, Iter: 1, Loss: 5.913900375366211, Iter time: 658.2837104797363, tok/sec: 173125.70033305284\n",
            " Epoch: 7, Iter: 2, Loss: 5.797300815582275, Iter time: 418.2419776916504, tok/sec: 174916.57987629497\n",
            " Epoch: 7, Iter: 3, Loss: 5.809557914733887, Iter time: 418.29657554626465, tok/sec: 174828.9139917622\n",
            " Epoch: 7, Iter: 4, Loss: 5.729742050170898, Iter time: 416.34082794189453, tok/sec: 180132.99519519362\n",
            " Epoch: 7, Iter: 5, Loss: 5.693431854248047, Iter time: 419.0034866333008, tok/sec: 174444.5800072601\n",
            " Epoch: 7, Iter: 6, Loss: 5.751596450805664, Iter time: 419.24214363098145, tok/sec: 173688.421869945\n",
            " Epoch: 7, Iter: 7, Loss: 5.707147121429443, Iter time: 416.9032573699951, tok/sec: 178216.7215238787\n",
            " Epoch: 7, Iter: 8, Loss: 5.608713150024414, Iter time: 417.8452491760254, tok/sec: 175399.64658437727\n",
            " Epoch: 8, Iter: 1, Loss: 5.828638553619385, Iter time: 658.4587097167969, tok/sec: 179977.3107853639\n",
            " Epoch: 8, Iter: 2, Loss: 5.69785213470459, Iter time: 415.83704948425293, tok/sec: 178637.37723395904\n",
            " Epoch: 8, Iter: 3, Loss: 5.71826171875, Iter time: 417.3107147216797, tok/sec: 176171.26257716524\n",
            " Epoch: 8, Iter: 4, Loss: 5.6334228515625, Iter time: 416.13030433654785, tok/sec: 180554.69160960795\n",
            " Epoch: 8, Iter: 5, Loss: 5.6002912521362305, Iter time: 419.65627670288086, tok/sec: 172666.3401995025\n",
            " Epoch: 8, Iter: 6, Loss: 5.663290977478027, Iter time: 417.0691967010498, tok/sec: 178867.07133133608\n",
            " Epoch: 8, Iter: 7, Loss: 5.616150379180908, Iter time: 415.8477783203125, tok/sec: 180950.727247353\n",
            " Epoch: 8, Iter: 8, Loss: 5.504509925842285, Iter time: 416.55540466308594, tok/sec: 179456.5515353941\n",
            " Epoch: 9, Iter: 1, Loss: 5.732150554656982, Iter time: 659.1010093688965, tok/sec: 179187.48790370947\n",
            " Epoch: 9, Iter: 2, Loss: 5.604375839233398, Iter time: 418.1020259857178, tok/sec: 175687.53543689568\n",
            " Epoch: 9, Iter: 3, Loss: 5.6241254806518555, Iter time: 972.1529483795166, tok/sec: 25301.880290119636\n",
            " Epoch: 9, Iter: 4, Loss: 5.5408477783203125, Iter time: 419.2991256713867, tok/sec: 173302.5244963168\n",
            " Epoch: 9, Iter: 5, Loss: 5.525720596313477, Iter time: 416.60499572753906, tok/sec: 178835.88387029615\n",
            " Epoch: 9, Iter: 6, Loss: 5.606895446777344, Iter time: 415.35067558288574, tok/sec: 182059.08682578412\n",
            " Epoch: 9, Iter: 7, Loss: 5.527459144592285, Iter time: 417.7985191345215, tok/sec: 177100.6423194382\n",
            " Epoch: 9, Iter: 8, Loss: 5.464728355407715, Iter time: 420.36986351013184, tok/sec: 171259.65208506226\n",
            "CPU times: user 3min 35s, sys: 2.31 s, total: 3min 37s\n",
            "Wall time: 1min 34s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from time import time\n",
        "\n",
        "for e in range(n_epochs):\n",
        "    for i in range(len(data) // (B*T)):\n",
        "        start = time()\n",
        "        buffer = data[i*B*T:(i+1)*B*T+1]\n",
        "        x_batch = jnp.array(buffer[:-1]).reshape((B, T))\n",
        "        y_batch = jnp.array(buffer[1:]).reshape((B, T))\n",
        "        loss = train_step(m, optimizer, x_batch, y_batch)\n",
        "        #jax.block_until_ready(loss)\n",
        "        iter_time = time() - start\n",
        "        tokens_per_sec = B*T / iter_time\n",
        "        i % 20 and print(f\" Epoch: {e}, Iter: {i}, Loss: {loss}, Iter time: {(time() - start)*1000:05}, tok/sec: {tokens_per_sec}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s580qdkRuJXT",
        "outputId": "6311d599-087d-4183-b8ea-161684a21003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> The Clever Fox. no he no?\" a;'s who will my- him are I; I; was to the for he'sRARAMy he my' meWhat\n",
            " not your! me in: was his.RA are\n",
            ": who\n",
            "> The Clever Fox meANT's of in,\" that,\" who'.\" IRA of itWhat you withing was me- in\n",
            " yous it he with with my was\n",
            " by\n",
            " PAN his of him thats his withI;.\" with\n",
            "> The Clever Fox\" and,\" not he inRAMy\n",
            ". was's for'sRA you;,\": ofs him are who on' of I in! PAN who\n",
            " not by's: and and is his not you\" is in this\n",
            "> The Clever Fox and me, I forMy with —.\" \" and When his no I I when! THE I her his for. had me's my me him the,\"! will! it, they with I's be. youRA\n",
            "> The Clever FoxI to.\" is-RA you as me's\n",
            "?\" with are the ofThe's'sMy byANT the is This! not?:. me when you, your, who her the as: onRA man who as\n"
          ]
        }
      ],
      "source": [
        "generate_completion(m, \"The Clever Fox\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "project",
      "language": "python",
      "name": "project"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}