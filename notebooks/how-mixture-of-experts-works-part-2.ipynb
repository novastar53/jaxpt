{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7e90a5",
   "metadata": {},
   "source": [
    "# How do Mixture of Experts Layers Work? Part 2\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "In [Part 1](https://blog.vikrampawar.com/how-mixture-of-experts-works-part-1.html), we introduced the idea of Mixtures of Expert layers and attempted a naive implementation, where we trained a simple neural network with an expert router and two experts. The model learned to route each datapoint to one of two regression models. We used a synthetic dataset tailored for our model. \n",
    "\n",
    "In this post, we'll build upon that basic design. \n",
    "\n",
    "## Why Mixture of Experts?\n",
    "\n",
    "A big reason for the popularity of Mixture of Experts layers in modern language models is sparse computation. In modern models, each token is routed to K of N experts, usually with K = 1 or 2. This effectively means that per-token FLOPs in the expert layers is reduced to K/N times that of dense networks without a large performance penalty. This becomes especially useful on multi-GPU setups where each expert is assigned per a. This is called expert parallelism. Routing tokens to each expert ( and hence GPU ) can however incur significant overhead due to all-to-all communication between GPUs. This can be mitigated to a large extent using design practices.\n",
    "\n",
    "With the help of MoE layers, it becomes possible to train much larger models while keeping the FLOPs under control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fmaro4xlnvj",
   "metadata": {},
   "source": [
    "## The Parallelism Challenge\n",
    "\n",
    "**Outline:**\n",
    "- Why naive implementations don't scale\n",
    "- The token-to-expert routing problem across devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffflamyc6rj",
   "metadata": {},
   "source": [
    "## Expert Parallelism Explained\n",
    "\n",
    "**Outline:**\n",
    "- Sharding experts across GPUs\n",
    "- How tokens flow: local → global → expert → back\n",
    "- Diagram/visualization of the data flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5gyvwvomjhd",
   "metadata": {},
   "source": [
    "## Token Routing - Dispatch and Combine\n",
    "\n",
    "**Outline:**\n",
    "- Dispatch: sending tokens to their assigned experts\n",
    "- Combine: gathering results back to original positions\n",
    "- How JAX's sharding annotations trigger the right communication\n",
    "- Understanding the cost implications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z00m5pj40qj",
   "metadata": {},
   "source": [
    "## Capacity Factor and Token Dropping\n",
    "\n",
    "**Outline:**\n",
    "- Expert capacity limits\n",
    "- What happens when experts overflow\n",
    "- Balancing computation vs. accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbtydyy8u4",
   "metadata": {},
   "source": [
    "## Putting It Together: A Parallel MoE Layer\n",
    "\n",
    "**Outline:**\n",
    "- Full implementation with proper sharding\n",
    "- Code walkthrough with JAX/Flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "et7r09bauna",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "from typing import Any\n",
    "\n",
    "class Router(nnx.Module):\n",
    "    def __init__(self, dim: int, num_experts: int, *, rngs: nnx.Rngs):\n",
    "        self.w1 = nnx.Linear(dim, num_experts, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        return self.w1(x)\n",
    "\n",
    "class Expert(nnx.Module):\n",
    "    def __init__(self, dim: int, *, rngs: nnx.Rngs):\n",
    "        self.linear = nnx.Linear(dim, dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        return self.linear(x)\n",
    "\n",
    "class SimpleMoE(nnx.Module):\n",
    "    def __init__(self, dim: int, *, rngs: nnx.Rngs):\n",
    "        num_experts = 2\n",
    "        self.router = Router(dim, num_experts=num_experts, rngs=rngs)\n",
    "        self.experts = nnx.List([\n",
    "            Expert(dim, rngs=rngs)\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        self.top_k = 2\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        gate_logits = self.router(x)       \n",
    "        top_k_logits, expert_indices = jax.lax.top_k(gate_logits, self.top_k)\n",
    "        zeros = jnp.full_like(gate_logits, float('-inf'))\n",
    "        sparse_logits = jnp.put_along_axis(\n",
    "            zeros, expert_indices, top_k_logits, axis=-1, inplace=False\n",
    "        )\n",
    "        expert_weights = jax.nn.softmax(sparse_logits, axis=-1)\n",
    "\n",
    "        mean_gates = jnp.mean(gate_logits, axis=0)\n",
    "        lb_loss = gate_logits.shape[1] * jnp.sum(mean_gates ** 2)\n",
    "\n",
    "        outputs = [ e(x) for e in self.experts ]\n",
    "\n",
    "        result = jnp.zeros_like(x)\n",
    "\n",
    "        for i, o in enumerate(outputs):\n",
    "            result += (o * expert_weights[:, :, i:i+1])\n",
    "           \n",
    "        return result, lb_loss, expert_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mrlckecqio9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax \n",
    "\n",
    "D, B, T, C = 10000, 2, 5, 3\n",
    "\n",
    "model = SimpleMoE(dim=C, rngs=nnx.Rngs(0))\n",
    "tx = optax.adam(1e-3)\n",
    "state = nnx.Optimizer(model, tx, wrt=nnx.Param)\n",
    "\n",
    "x = jax.random.normal(jax.random.key(1000), (D * B * T, C))\n",
    "\n",
    "expert_ids = (x[:, 0] > 0).astype(jnp.int32)\n",
    "t = [\n",
    "    jax.random.normal(jax.random.key(2000), (C, C)),\n",
    "    jax.random.normal(jax.random.key(3000), (C, C)),\n",
    "]\n",
    "def transform(xi, eid):\n",
    "    return jnp.where(eid == 1, xi @ t[0], xi @ t[1])\n",
    "\n",
    "y = jax.vmap(lambda xi, ei: transform(xi, ei))(x, expert_ids)\n",
    "\n",
    "def loss_fn(model, x, y):\n",
    "    y_pred, lb_loss, gates = model(x)\n",
    "    loss = jnp.mean((y - y_pred)**2) # + lb_loss\n",
    "    return loss, gates\n",
    "\n",
    "@nnx.jit\n",
    "def step(model, state, x, y):\n",
    "    (loss, gates), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model, x, y)\n",
    "    state.update(model, grads)\n",
    "    return loss, gates, grads\n",
    "\n",
    "x = x.reshape(D, B, T, C)\n",
    "y = y.reshape(D, B, T, C)\n",
    "\n",
    "for e in range(10):\n",
    "    for i in range(D):\n",
    "        loss, gates, grads = step(model, state, x[i], y[i])\n",
    "        if i % 1000 == 0:\n",
    "            print(i, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uepblwj2kml",
   "metadata": {},
   "source": [
    "## Practical Considerations\n",
    "\n",
    "**Outline:**\n",
    "- When to use expert parallelism vs. other strategies\n",
    "- Tips for debugging distributed MoE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de0i2khwux",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. (1991). *Adaptive Mixtures of Local Experts*. Neural Computation.\n",
    "2. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J. (2017). *Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer*. ICLR.\n",
    "3. Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., ... & Chen, Z. (2021). *GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding*. ICLR.\n",
    "4. Fedus, W., Zoph, B., & Shazeer, N. (2022). *Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*. JMLR.\n",
    "5. Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., ... & Le, Q. V. (2022). *GLaM: Efficient Scaling of Language Models with Mixture-of-Experts*. ICML.\n",
    "6. Riquelme, C., Puigcerver, J., Mustafa, B., Neumann, M., Jenatton, R., Susano Pinto, A., ... & Houlsby, N. (2021). *Scaling Vision with Sparse Mixture of Experts*. NeurIPS.\n",
    "7. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., ... & Sayed, W. E. (2024). *Mixtral of Experts*. arXiv."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "high-performance-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
