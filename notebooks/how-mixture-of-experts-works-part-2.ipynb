{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7e90a5",
   "metadata": {},
   "source": [
    "# How do Mixture of Experts Layers Work? Part 2\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In [Part 1](https://blog.vikrampawar.com/how-mixture-of-experts-works-part-1.html), we introduced the idea of Mixtures of Expert layers and attempted a naive implementation, where we trained a simple neural network with an expert router and two experts. The model learned to route each datapoint to one of two regression models. We used a synthetic dataset tailored for our model. \n",
    "\n",
    "In this post, we'll build upon that basic design and explore how to scale MoE layers across multiple GPUs.\n",
    "\n",
    "## Why Mixture of Experts?\n",
    "\n",
    "The main reason why of Mixture of Experts is used because it can scale model parameters with sublinear compute scaling. In a dense model, every parameter is used for every input. Double the parameters, double the FLOPs. MoE breaks this relationship by only activating a subset of parameters for each input. In modern MoE models, each token is routed to K of N experts (typically K = 1 or 2). This means per-token FLOPs in the expert layers are reduced to K/N of what a dense network would require, without a large performance penalty.\n",
    "\n",
    "As a result, you can train much larger models while keeping the compute budget under control. A 400B parameter MoE model with 8 experts and top-2 routing has roughly the same inference cost as a 100B dense model, but with access to 4x more parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fmaro4xlnvj",
   "metadata": {},
   "source": [
    "## The Parallelism Challenge\n",
    "\n",
    "The parallelism challenge in MoE is about what happens when you try to distribute experts across multiple GPUs. In a naive implementation, you might run every token through every expert and then weight the outputs. This is actually dense computation masquerading as MoE — you don't get any compute savings.\n",
    "\n",
    "To get actual sparse computation, you need to only send each token to its assigned K experts. But when experts live on different GPUs, this creates a coordination problem. Token A on GPU 0 might need Expert 3 on GPU 2. Token B on GPU 1 might need Expert 1 on GPU 0. Every GPU potentially needs to send tokens to every other GPU, creating a many-to-many communication pattern.\n",
    "\n",
    "[DIAGRAM: The Parallelism Challenge]\n",
    "*Suggestion: A before/after view of token-to-expert routing across GPUs. Left side shows GPUs with their local batch slices, right side shows tokens redistributed to their assigned experts. Arrows crossing between GPUs to show the many-to-many pattern.*\n",
    "\n",
    "This introduces several challenges. First, all-to-all communication is expensive, especially across nodes. Second, if the router sends many tokens to one expert, that GPU becomes a bottleneck while others sit idle. Third, each GPU needs memory to buffer incoming and outgoing tokens during the shuffle. Finally, you need two shuffles — one to dispatch tokens to experts, and another to combine results back to their original positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffflamyc6rj",
   "metadata": {},
   "source": [
    "## Expert Parallelism Explained\n",
    "\n",
    "Expert parallelism solves the routing problem by distributing experts across GPUs and using all-to-all communication to shuffle tokens to their assigned experts. The sharding strategy has three components: expert weights are sharded on the expert dimension so that GPU i holds expert i, non-expert layers (attention, embeddings, normalization) are replicated across all GPUs, and data batches are sharded on the batch dimension so that GPU i processes batch slice i. This combines data parallelism with expert parallelism.\n",
    "\n",
    "**MoE Sharding Strategy**\n",
    "\n",
    "![moe-sharding-strategy](moe-sharding-strategy-v2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5gyvwvomjhd",
   "metadata": {},
   "source": [
    "## Token Routing - Dispatch and Combine\n",
    "\n",
    "The token flow works in four phases. First, each GPU independently gathers its tokens into expert-specific buffers based on routing decisions. After this local gathering step, each GPU has organized its tokens by which expert they need. Second, during the all-to-all dispatch, the expert buffers are redistributed across GPUs so that each expert's tokens are collected on their respective GPU. GPU 0 receives all tokens destined for expert 0, GPU 1 receives all tokens for expert 1, and so on. Third, each GPU runs its local expert on the tokens it received. Fourth, during the all-to-all combine, the processed tokens are routed back to their original GPUs and reassembled in the correct sequence positions.\n",
    "\n",
    "![token-dispatch-combine](token-dispatch-combine.png)\n",
    "\n",
    "In JAX, the all-to-all communication emerges implicitly from sharding constraints. You specify how tensors should be partitioned (e.g., tokens sharded by batch, expert weights sharded by expert index), and XLA's compiler inserts the necessary collectives when an operation requires data that lives on another device. For MoE, this means the dispatch and combine shuffles happen automatically when the sharding layout changes between \"tokens grouped by batch\" and \"tokens grouped by expert.\"\n",
    "\n",
    "The dispatch and combine phases come with real costs. MoE trades compute savings (activating only K of N experts) for communication overhead — each token must be sent to its assigned expert and the result returned, roughly `2 × num_tokens × hidden_dim` bytes moved per MoE layer. All-to-all is also a synchronization barrier: every GPU must wait for the slowest one to finish sending and receiving, so load imbalance amplifies latency. Finally, each GPU needs buffer memory to stage outgoing tokens (grouped by destination) and incoming tokens (from all other GPUs), increasing peak memory usage beyond what the expert weights alone require."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z00m5pj40qj",
   "metadata": {},
   "source": [
    "## Capacity Factor and Token Dropping\n",
    "\n",
    "The router doesn't distribute tokens evenly — one expert might receive 80% of tokens while another gets 5%. To handle this, MoE implementations define an expert capacity: `capacity = (batch_tokens / num_experts) × capacity_factor`, where capacity factor (typically 1.0 to 2.0) controls how much slack each expert has. When more tokens are routed to an expert than its capacity allows, the excess tokens are dropped — they skip the expert entirely and pass through via the residual connection.\n",
    "\n",
    "The capacity factor controls a tradeoff: higher values mean fewer dropped tokens but more buffer memory and wasted compute on padding; lower values keep memory tight but risk dropping tokens. To minimize dropping without inflating capacity, MoE models use auxiliary load balancing losses that penalize uneven routing, pushing the router toward balanced assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbtydyy8u4",
   "metadata": {},
   "source": [
    "## Putting It Together: A Sparse MoE Layer\n",
    "\n",
    "The implementation below demonstrates sparse token routing without sharding. The `Experts` class holds all expert weights in a single tensor of shape `(n_experts, n_embed, n_embed)`, indexing into the appropriate slice when called. The `MOE` class implements the full dispatch-compute-combine pattern: a router produces per-token logits, `jax.lax.top_k` selects the top-k experts for each token, and a masked softmax computes the expert weights. During dispatch, a `fori_loop` iterates through tokens and copies each to its assigned experts' input buffers. Each expert then processes only its gathered tokens. During combine, another `fori_loop` scatters the expert outputs back to their original positions, weighted by the router's softmax scores. The training example uses synthetic data where the target transformation depends on input values — the model learns to route tokens to the correct expert, as shown by the loss dropping from 2.69 to 0.0004."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1dd1b74",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "An auto mesh context or metadata is required if creating a variable with annotation sharding_names=(None,). For more guidance, see https://flax.readthedocs.io/en/latest/flip/4844-var-eager-sharding.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 182\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;66;03m# Create model within mesh context for proper sharding\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m mesh:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     model = \u001b[43mMOE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrngs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m     model.train(add_noise=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    184\u001b[39m     tx = optax.adam(\u001b[32m1e-2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/high_performance_jax/.venv/lib/python3.12/site-packages/flax/nnx/pytreelib.py:400\u001b[39m, in \u001b[36mPytreeMeta.__call__\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_graph_node_meta_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/high_performance_jax/.venv/lib/python3.12/site-packages/flax/nnx/pytreelib.py:412\u001b[39m, in \u001b[36m_graph_node_meta_call\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m vars_obj[\u001b[33m'\u001b[39m\u001b[33m_pytree__state\u001b[39m\u001b[33m'\u001b[39m] = PytreeState()\n\u001b[32m    411\u001b[39m vars_obj[\u001b[33m'\u001b[39m\u001b[33m_pytree__nodes\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mcls\u001b[39m._pytree__nodes\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pytree_meta_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._pytree__is_pytree:\n\u001b[32m    414\u001b[39m   missing: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m] = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/high_performance_jax/.venv/lib/python3.12/site-packages/flax/nnx/pytreelib.py:403\u001b[39m, in \u001b[36mPytreeMeta._pytree_meta_construct\u001b[39m\u001b[34m(cls, self, *args, **kwargs)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_pytree_meta_construct\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mMOE.__init__\u001b[39m\u001b[34m(self, config, rngs)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: Config, rngs: nnx.Rngs):\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# Router is replicated (not sharded)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28mself\u001b[39m.router_gate = \u001b[43mnnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mn_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mn_experts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkernel_init\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_partitioning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitializers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstddev\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.02\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# replicated\u001b[39;49;00m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbias_init\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_partitioning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitializers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmlp_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrngs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrngs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mself\u001b[39m.experts = Experts(config, rngs)        \n\u001b[32m     84\u001b[39m     \u001b[38;5;28mself\u001b[39m.top_k = config.top_k\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/high_performance_jax/.venv/lib/python3.12/site-packages/flax/nnx/pytreelib.py:400\u001b[39m, in \u001b[36mPytreeMeta.__call__\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_graph_node_meta_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/high_performance_jax/.venv/lib/python3.12/site-packages/flax/nnx/pytreelib.py:412\u001b[39m, in \u001b[36m_graph_node_meta_call\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m vars_obj[\u001b[33m'\u001b[39m\u001b[33m_pytree__state\u001b[39m\u001b[33m'\u001b[39m] = PytreeState()\n\u001b[32m    411\u001b[39m vars_obj[\u001b[33m'\u001b[39m\u001b[33m_pytree__nodes\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mcls\u001b[39m._pytree__nodes\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pytree_meta_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._pytree__is_pytree:\n\u001b[32m    414\u001b[39m   missing: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m] = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/high_performance_jax/.venv/lib/python3.12/site-packages/flax/nnx/pytreelib.py:403\u001b[39m, in \u001b[36mPytreeMeta._pytree_meta_construct\u001b[39m\u001b[34m(cls, self, *args, **kwargs)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_pytree_meta_construct\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/high_performance_jax/.venv/lib/python3.12/site-packages/flax/nnx/nn/linear.py:372\u001b[39m, in \u001b[36mLinear.__init__\u001b[39m\u001b[34m(self, in_features, out_features, use_bias, dtype, param_dtype, precision, kernel_init, bias_init, dot_general, promote_dtype, preferred_element_type, rngs, kernel_metadata, bias_metadata)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    354\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    355\u001b[39m   in_features: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    369\u001b[39m   bias_metadata: tp.Mapping[\u001b[38;5;28mstr\u001b[39m, tp.Any] = MappingProxyType({}),\n\u001b[32m    370\u001b[39m ):\n\u001b[32m    371\u001b[39m   kernel_key = rngs.params()\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m   \u001b[38;5;28mself\u001b[39m.kernel = \u001b[43mnnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mParam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkernel_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43min_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkernel_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m   \u001b[38;5;28mself\u001b[39m.bias: nnx.Param[jax.Array] | \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m use_bias:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/high_performance_jax/.venv/lib/python3.12/site-packages/flax/nnx/variablelib.py:904\u001b[39m, in \u001b[36mVariableMeta.__call__\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_meta_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/high_performance_jax/.venv/lib/python3.12/site-packages/flax/nnx/variablelib.py:907\u001b[39m, in \u001b[36mVariableMeta._variable_meta_call\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_variable_meta_call\u001b[39m(\u001b[38;5;28mcls\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m   variable = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    908\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m variable.is_hijax:\n\u001b[32m    909\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _new_hijax_from_variable(variable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/high_performance_jax/.venv/lib/python3.12/site-packages/flax/nnx/variablelib.py:1108\u001b[39m, in \u001b[36mVariable.__init__\u001b[39m\u001b[34m(self, value, is_hijax, has_ref, is_mutable, eager_sharding, **metadata)\u001b[39m\n\u001b[32m   1106\u001b[39m \u001b[38;5;66;03m# shard the _value if applicable\u001b[39;00m\n\u001b[32m   1107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eager_sharding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33msharding_names\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m metadata:\n\u001b[32m-> \u001b[39m\u001b[32m1108\u001b[39m   value = \u001b[43mcore_spmd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshard_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msharding_names\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msharding_rules\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmesh\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_ref:\n\u001b[32m   1115\u001b[39m   value = jax.new_ref(value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/high_performance_jax/.venv/lib/python3.12/site-packages/flax/core/spmd.py:43\u001b[39m, in \u001b[36mshard_value\u001b[39m\u001b[34m(value, sharding_names, sharding_rules, mesh)\u001b[39m\n\u001b[32m     41\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mesh \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m meta.global_mesh_defined():\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     44\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mAn auto mesh context or metadata is required if creating a variable\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m with annotation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msharding_names\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mFor more guidance, see https://flax.readthedocs.io/en/latest/flip/4844-var-eager-sharding.html.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     47\u001b[39m pspec = get_pspec(sharding_names, sharding_rules)\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: An auto mesh context or metadata is required if creating a variable with annotation sharding_names=(None,). For more guidance, see https://flax.readthedocs.io/en/latest/flip/4844-var-eager-sharding.html."
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=2'\n",
    "\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh, PartitionSpec, NamedSharding\n",
    "\n",
    "import flax.nnx as nnx\n",
    "import optax\n",
    "\n",
    "# Set up device mesh - all devices along a single \"devices\" axis\n",
    "mesh = Mesh(jax.devices(), [\"devices\"])\n",
    "num_devices = len(jax.devices())\n",
    "\n",
    "# Sharding spec for expert-parallel tensors\n",
    "expert_spec = PartitionSpec(\"devices\",)\n",
    "\n",
    "\n",
    "@dataclass(unsafe_hash=True)\n",
    "class Config():\n",
    "    name: str = \"MoE\"\n",
    "    dtype: jnp.dtype = jnp.float32\n",
    "    param_dtype: jnp.dtype = jnp.float32\n",
    "    top_k = 2\n",
    "    load_factor = 1.00\n",
    "    n_experts = 2\n",
    "    n_embed = 3\n",
    "    n_mlp_hidden = 6\n",
    "    mlp_bias = True\n",
    "    dtype = jax.numpy.float32\n",
    "\n",
    "config = Config()\n",
    "\n",
    "\n",
    "class Experts(nnx.Module):\n",
    "    def __init__(self, config, rngs):\n",
    "        # Shard expert weights on the expert dimension (axis 0)\n",
    "        init = nnx.with_partitioning(\n",
    "            nnx.initializers.normal(stddev=0.02),\n",
    "            sharding=expert_spec\n",
    "        )\n",
    "        self.w1 = nnx.Param(init(rngs.default(),\n",
    "            (\n",
    "                config.n_experts,\n",
    "                config.n_embed,\n",
    "                config.n_embed\n",
    "            )\n",
    "        ))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # x: (n_experts, tokens_per_expert, n_embed)\n",
    "        # Apply sharding constraint before expert computation\n",
    "        x = jax.lax.with_sharding_constraint(x, expert_spec)\n",
    "        # Each expert processes its slice: einsum over expert dimension\n",
    "        y = jnp.einsum('eti,eio->eto', x, self.w1.value)\n",
    "        y = jax.lax.with_sharding_constraint(y, expert_spec)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MOE(nnx.Module):\n",
    "    def __init__(self, config: Config, rngs: nnx.Rngs):\n",
    "        # Router is replicated (not sharded)\n",
    "        self.router_gate = nnx.Linear(\n",
    "            config.n_embed,\n",
    "            config.n_experts,\n",
    "            kernel_init=nnx.with_partitioning(\n",
    "                nnx.initializers.normal(stddev=0.02),\n",
    "                sharding=(None,)  # replicated\n",
    "            ),\n",
    "            bias_init=nnx.with_partitioning(\n",
    "                nnx.initializers.zeros, \n",
    "                sharding=(None,)\n",
    "            ),\n",
    "            use_bias=config.mlp_bias,\n",
    "            dtype=config.dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.experts = Experts(config, rngs)        \n",
    "        self.top_k = config.top_k\n",
    "        self.n_experts = config.n_experts\n",
    "        self.load_factor = config.load_factor\n",
    "        self.add_noise = False\n",
    "        self.rngs = rngs\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x_flat = x.reshape(-1, C)\n",
    "        \n",
    "        # Router produces expert logits for each token\n",
    "        logits = self.router_gate(x_flat)  # (B*T, n_experts)\n",
    "        \n",
    "        # Select top-k experts per token\n",
    "        top_k_logits, expert_indices = jax.lax.top_k(logits, self.top_k)\n",
    "        \n",
    "        # Create sparse logits for softmax (only top-k positions have real values)\n",
    "        zeros = jnp.full_like(logits, float('-inf'))\n",
    "        sparse_logits = jnp.put_along_axis(\n",
    "            zeros, expert_indices, top_k_logits, axis=-1, inplace=False\n",
    "        )\n",
    "        expert_weights = jax.nn.softmax(sparse_logits, axis=-1)\n",
    "\n",
    "        # --- DISPATCH: gather tokens into expert buffers ---\n",
    "        expert_capacity = int(self.load_factor * self.top_k * B * T // self.n_experts)\n",
    "        expert_inputs = jnp.zeros((self.n_experts, expert_capacity, C))\n",
    "        input_counters = jnp.zeros((self.n_experts,), dtype=jnp.int32)\n",
    "\n",
    "        def update_expert_inputs(i, carry):\n",
    "            expert_inputs, counters = carry\n",
    "            for j in range(self.top_k):\n",
    "                expert_idx = expert_indices[i, j]\n",
    "                token_pos = counters[expert_idx]\n",
    "                expert_inputs = expert_inputs.at[expert_idx, token_pos].set(x_flat[i])\n",
    "                counters = counters.at[expert_idx].add(1)\n",
    "            return expert_inputs, counters\n",
    "\n",
    "        expert_inputs, input_counters = jax.lax.fori_loop(\n",
    "            0, B * T, update_expert_inputs, (expert_inputs, input_counters)\n",
    "        )\n",
    "\n",
    "        # Apply sharding constraint to trigger all-to-all dispatch\n",
    "        # This moves tokens from batch-sharded to expert-sharded layout\n",
    "        expert_inputs = jax.lax.with_sharding_constraint(expert_inputs, expert_spec)\n",
    "\n",
    "        # --- COMPUTE: each expert processes its tokens ---\n",
    "        expert_outputs = self.experts(expert_inputs)\n",
    "\n",
    "        # --- COMBINE: scatter results back to original positions ---\n",
    "        output_counters = jnp.zeros((self.n_experts,), dtype=jnp.int32)\n",
    "        y_pred = jnp.zeros_like(x_flat)\n",
    "\n",
    "        def update_expert_outputs(i, carry):\n",
    "            y_pred, output_counters = carry\n",
    "            for j in range(self.top_k):\n",
    "                expert_idx = expert_indices[i, j]\n",
    "                token_pos = output_counters[expert_idx]\n",
    "                y_pred = y_pred.at[i].add(\n",
    "                    expert_outputs[expert_idx, token_pos] * expert_weights[i, expert_idx]\n",
    "                )\n",
    "                output_counters = output_counters.at[expert_idx].add(1)\n",
    "            return y_pred, output_counters\n",
    "\n",
    "        y_pred, output_counters = jax.lax.fori_loop(\n",
    "            0, B * T, update_expert_outputs, (y_pred, output_counters)\n",
    "        )\n",
    "\n",
    "        # Apply sharding constraint to trigger all-to-all combine\n",
    "        # This moves results from expert-sharded back to batch-sharded layout\n",
    "        y_pred = jax.lax.with_sharding_constraint(y_pred, expert_spec)\n",
    "\n",
    "        y_pred = y_pred.reshape(B, T, C)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "def loss_fn(model, x, y):\n",
    "    y_pred = model(x)\n",
    "    loss = jnp.mean((y - y_pred)**2)\n",
    "    return loss, y_pred\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def step(state, x, y):\n",
    "    (loss, y_pred), grads = nnx.value_and_grad(\n",
    "        loss_fn, has_aux=True)(state.model, x, y)\n",
    "    state.update(grads)\n",
    "    return loss, grads, y_pred\n",
    "\n",
    "\n",
    "# Training setup\n",
    "D, B, T, C = 1000, config.n_experts, 5, config.n_embed \n",
    "   \n",
    "default = jax.random.key(69)\n",
    "gate_noise = jax.random.key(42)\n",
    "rngs = nnx.Rngs(default=default, gate_noise=gate_noise)\n",
    "\n",
    "# Create model within mesh context for proper sharding\n",
    "with mesh:\n",
    "    model = MOE(config, rngs)\n",
    "    model.train(add_noise=False)\n",
    "    tx = optax.adam(1e-2)\n",
    "    state = nnx.Optimizer(model, tx)\n",
    "\n",
    "    # Create data sharding for batch dimension\n",
    "    data_sharding = NamedSharding(mesh, PartitionSpec(\"devices\",))\n",
    "\n",
    "    # Synthetic data: target depends on input sign (expert routing signal)\n",
    "    x = jax.random.normal(jax.random.key(1000), (D, B, T, C))\n",
    "    expert_ids = (x[:, :, :, 0] > 0).astype(jnp.int32)[..., None]\n",
    "    t = [\n",
    "        jax.random.normal(jax.random.key(2000), (C, C)),\n",
    "        jax.random.normal(jax.random.key(3000), (C, C)),\n",
    "    ]\n",
    "\n",
    "    def transform(xi, eid):\n",
    "        return jnp.where(eid == 1, xi @ t[0], xi @ t[1])\n",
    "\n",
    "    y = jax.vmap(lambda xi, ei: transform(xi, ei))(x, expert_ids)\n",
    "\n",
    "    # Training loop\n",
    "    indices = list(range(D))\n",
    "    for e in range(100):\n",
    "        for i in indices:\n",
    "            # Shard data across devices\n",
    "            x_batch = jax.device_put(x[i], data_sharding)\n",
    "            y_batch = jax.device_put(y[i], data_sharding)\n",
    "            loss, grads, y_pred = step(state, x_batch, y_batch)\n",
    "            if i % 1000 == 0:\n",
    "                print(e, i, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uepblwj2kml",
   "metadata": {},
   "source": [
    "## Practical Considerations\n",
    "\n",
    "**Outline:**\n",
    "- When to use expert parallelism vs. other strategies\n",
    "- Tips for debugging distributed MoE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de0i2khwux",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. (1991). *Adaptive Mixtures of Local Experts*. Neural Computation.\n",
    "2. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J. (2017). *Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer*. ICLR.\n",
    "3. Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., ... & Chen, Z. (2021). *GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding*. ICLR.\n",
    "4. Fedus, W., Zoph, B., & Shazeer, N. (2022). *Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*. JMLR.\n",
    "5. Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., ... & Le, Q. V. (2022). *GLaM: Efficient Scaling of Language Models with Mixture-of-Experts*. ICML.\n",
    "6. Riquelme, C., Puigcerver, J., Mustafa, B., Neumann, M., Jenatton, R., Susano Pinto, A., ... & Houlsby, N. (2021). *Scaling Vision with Sparse Mixture of Experts*. NeurIPS.\n",
    "7. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., ... & Sayed, W. E. (2024). *Mixtral of Experts*. arXiv."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "high-performance-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
