{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7e90a5",
   "metadata": {},
   "source": [
    "# How do Mixture of Experts Layers Work? Part 2\n",
    "\n",
    "\n",
    "In [Part 1](https://blog.vikrampawar.com/how-mixture-of-experts-works-part-1.html), we introduced the idea of Mixtures of Expert layers and attempted a naive implementation, where we trained a simple neural network with an expert router and two experts. The model learned to route each datapoint to one of two regression models. We used a synthetic dataset tailored for our model. \n",
    "\n",
    "In this post, we'll build upon that basic design. \n",
    "\n",
    "## Why Mixture of Experts?\n",
    "\n",
    "A big reason for the popularity of Mixture of Experts layers in modern language models is sparse computation. In modern models, each token is routed to K of N experts, usually with K = 1 or 2. This effectively means that per-token FLOPs in the expert layers is reduced to K/N times that of dense networks without a large performance penalty. This becomes especially useful on multi-GPU setups where each expert is assigned per a. This is called expert parallelism. Routing tokens to each expert ( and hence GPU ) can however incur significant overhead due to all-to-all communication between GPUs. This can be mitigated to a large extent using design practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce4f9ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "from typing import Any\n",
    "\n",
    "class Router(nnx.Module):\n",
    "    def __init__(self, dim: int, num_experts: int, *, rngs: nnx.Rngs):\n",
    "        self.w1 = nnx.Linear(dim, num_experts, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        return self.w1(x)\n",
    "\n",
    "class Expert(nnx.Module):\n",
    "    def __init__(self, dim: int, *, rngs: nnx.Rngs):\n",
    "        self.linear = nnx.Linear(dim, dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        return self.linear(x)\n",
    "\n",
    "class SimpleMoE(nnx.Module):\n",
    "    def __init__(self, dim: int, *, rngs: nnx.Rngs):\n",
    "        num_experts = 2\n",
    "        self.router = Router(dim, num_experts=num_experts, rngs=rngs)\n",
    "        self.experts = nnx.List([\n",
    "            Expert(dim, rngs=rngs)\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        self.top_k = 2\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        gate_logits = self.router(x)       \n",
    "        top_k_logits, expert_indices = jax.lax.top_k(gate_logits, self.top_k)\n",
    "        zeros = jnp.full_like(gate_logits, float('-inf'))\n",
    "        sparse_logits = jnp.put_along_axis(\n",
    "            zeros, expert_indices, top_k_logits, axis=-1, inplace=False\n",
    "        )\n",
    "        expert_weights = jax.nn.softmax(sparse_logits, axis=-1)\n",
    "\n",
    "        mean_gates = jnp.mean(gate_logits, axis=0)\n",
    "        lb_loss = gate_logits.shape[1] * jnp.sum(mean_gates ** 2)\n",
    "\n",
    "        outputs = [ e(x) for e in self.experts ]\n",
    "\n",
    "        result = jnp.zeros_like(x)\n",
    "\n",
    "        for i, o in enumerate(outputs):\n",
    "            result += (o * expert_weights[:, :, i:i+1])\n",
    "           \n",
    "        return result, lb_loss, expert_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e8880",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25cec19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.8386207\n",
      "1000 1.4691024\n",
      "2000 0.6264829\n",
      "3000 0.22666237\n",
      "4000 0.32224867\n",
      "5000 0.27109587\n",
      "6000 0.036007397\n",
      "7000 0.042003997\n",
      "8000 0.359906\n",
      "9000 0.019899525\n",
      "0 0.07967947\n",
      "1000 0.048338573\n",
      "2000 0.07231833\n",
      "3000 0.0011955692\n",
      "4000 0.061102558\n",
      "5000 0.027696146\n",
      "6000 0.0011745306\n",
      "7000 0.0014423532\n",
      "8000 0.25469956\n",
      "9000 0.0031980982\n",
      "0 0.037122283\n",
      "1000 0.021085124\n",
      "2000 0.020036932\n",
      "3000 0.0002973358\n",
      "4000 0.028141744\n",
      "5000 0.0031959899\n",
      "6000 0.0005252546\n",
      "7000 0.0006971828\n",
      "8000 0.23215021\n",
      "9000 0.0010879862\n",
      "0 0.019682594\n",
      "1000 0.009787401\n",
      "2000 0.00533594\n",
      "3000 0.00018117022\n",
      "4000 0.013458978\n",
      "5000 0.00086001644\n",
      "6000 0.00029388236\n",
      "7000 0.0004293031\n",
      "8000 0.21831232\n",
      "9000 0.00060997024\n",
      "0 0.012715654\n",
      "1000 0.004591771\n",
      "2000 0.0015240598\n",
      "3000 0.00012839105\n",
      "4000 0.0068611316\n",
      "5000 0.0005576007\n",
      "6000 0.00019687101\n",
      "7000 0.00030134188\n",
      "8000 0.20550406\n",
      "9000 0.00043756963\n",
      "0 0.009739019\n",
      "1000 0.002218916\n",
      "2000 0.0005265196\n",
      "3000 0.00010175501\n",
      "4000 0.0037555827\n",
      "5000 0.000450459\n",
      "6000 0.00014797169\n",
      "7000 0.00023094774\n",
      "8000 0.19362797\n",
      "9000 0.00034868997\n",
      "0 0.008288349\n",
      "1000 0.0011200772\n",
      "2000 0.00025397656\n",
      "3000 8.6699896e-05\n",
      "4000 0.0022263434\n",
      "5000 0.0003809286\n",
      "6000 0.00012016174\n",
      "7000 0.00018735956\n",
      "8000 0.18282159\n",
      "9000 0.00029231366\n",
      "0 0.0074516423\n",
      "1000 0.0005978119\n",
      "2000 0.00017527514\n",
      "3000 7.715925e-05\n",
      "4000 0.0014416231\n",
      "5000 0.00033131373\n",
      "6000 0.00010308235\n",
      "7000 0.00015803585\n",
      "8000 0.17301631\n",
      "9000 0.00025248475\n",
      "0 0.0068818224\n",
      "1000 0.00034176663\n",
      "2000 0.00015066056\n",
      "3000 7.046891e-05\n",
      "4000 0.0010219362\n",
      "5000 0.00029549695\n",
      "6000 9.1995964e-05\n",
      "7000 0.0001370759\n",
      "8000 0.16406791\n",
      "9000 0.00022256821\n",
      "0 0.0064407364\n",
      "1000 0.00021198479\n",
      "2000 0.0001417837\n",
      "3000 6.53722e-05\n",
      "4000 0.0007871999\n",
      "5000 0.0002693269\n",
      "6000 8.445619e-05\n",
      "7000 0.00012142981\n",
      "8000 0.15582006\n",
      "9000 0.0001991511\n"
     ]
    }
   ],
   "source": [
    "import optax \n",
    "\n",
    "D, B, T, C = 10000, 2, 5, 3\n",
    "\n",
    "model = SimpleMoE(dim=C, rngs=nnx.Rngs(0))\n",
    "tx = optax.adam(1e-3)\n",
    "state = nnx.Optimizer(model, tx, wrt=nnx.Param)\n",
    "\n",
    "x = jax.random.normal(jax.random.key(1000), (D * B * T, C))\n",
    "\n",
    "expert_ids = (x[:, 0] > 0).astype(jnp.int32)\n",
    "t = [\n",
    "    jax.random.normal(jax.random.key(2000), (C, C)),\n",
    "    jax.random.normal(jax.random.key(3000), (C, C)),\n",
    "]\n",
    "def transform(xi, eid):\n",
    "    return jnp.where(eid == 1, xi @ t[0], xi @ t[1])\n",
    "\n",
    "y = jax.vmap(lambda xi, ei: transform(xi, ei))(x, expert_ids)\n",
    "\n",
    "def loss_fn(model, x, y):\n",
    "    y_pred, lb_loss, gates = model(x)\n",
    "    loss = jnp.mean((y - y_pred)**2) # + lb_loss\n",
    "    return loss, gates\n",
    "\n",
    "@nnx.jit\n",
    "def step(model, state, x, y):\n",
    "    (loss, gates), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model, x, y)\n",
    "    state.update(model, grads)\n",
    "    return loss, gates, grads\n",
    "\n",
    "x = x.reshape(D, B, T, C)\n",
    "y = y.reshape(D, B, T, C)\n",
    "\n",
    "for e in range(10):\n",
    "    for i in range(D):\n",
    "        loss, gates, grads = step(model, state, x[i], y[i])\n",
    "        if i % 1000 == 0:\n",
    "            print(i, loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "high-performance-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
