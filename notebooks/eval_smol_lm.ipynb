{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXU2qDN8nMgg"
      },
      "source": [
        "# Let's Evaluate SmolLM\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04pFw2g58HJl"
      },
      "source": [
        "### Configure the machine and install packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNLo9jfLn8bg",
        "outputId": "5b357185-005e-4af5-cc02-6477b5d47959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on colab\n",
            "Cloning into 'jaxpt'...\n",
            "remote: Enumerating objects: 2382, done.\u001b[K\n",
            "remote: Counting objects: 100% (242/242), done.\u001b[K\n",
            "remote: Compressing objects: 100% (151/151), done.\u001b[K\n",
            "remote: Total 2382 (delta 164), reused 111 (delta 87), pack-reused 2140 (from 2)\u001b[K\n",
            "Receiving objects: 100% (2382/2382), 356.10 MiB | 14.85 MiB/s, done.\n",
            "Resolving deltas: 100% (1451/1451), done.\n",
            "Already on 'main'\n",
            "Your branch is up to date with 'origin/main'.\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "from typing import Literal\n",
        "import jax\n",
        "\n",
        "platform : Literal[\"darwin\", \"colab\", \"cuda\", \"tpu\"] = \"darwin\"\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    platform = \"colab\"\n",
        "except ImportError:\n",
        "    devices = jax.devices()\n",
        "    if any(d.platform == \"gpu\" for d in devices):\n",
        "        platform = \"cuda\"\n",
        "    if any(d.platform == \"tpu\" for d in devices):\n",
        "        platform = \"tpu\"\n",
        "\n",
        "print(f\"Running on {platform}\")\n",
        "\n",
        "if platform == \"colab\":\n",
        "    !git clone https://github.com/novastar53/jaxpt\n",
        "    !cd jaxpt && git checkout main && git pull\n",
        "    !pip install tiktoken datasets --quiet\n",
        "    #!pip uninstall -y tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQl8dEgLnMgh",
        "outputId": "a97097a3-cbd6-4ffb-a36d-c2d5be977e12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jaxpt/src\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "if platform == \"colab\":\n",
        "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"src\" )\n",
        "else:\n",
        "    jaxpt_dir = str(Path().absolute().parent / \"src\" )\n",
        "\n",
        "\n",
        "sys.path.append(jaxpt_dir)\n",
        "print(jaxpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJo6Xji39g54",
        "outputId": "4df44244-424e-47f6-c9ad-4bb487501147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version: 0.5.2\n",
            "Flax version 0.10.6\n",
            "Available devices: 1\n",
            "using gpu\n",
            "2.52 ms ± 16.6 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "import warnings\n",
        "\n",
        "import jax\n",
        "from jax.sharding import Mesh, PartitionSpec, NamedSharding\n",
        "import flax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "# Hardware setup\n",
        "print(\"JAX version:\", jax.__version__)\n",
        "print(\"Flax version\", flax.__version__)\n",
        "\n",
        "devices = jax.devices()\n",
        "num_devices = len(devices)\n",
        "print(\"Available devices:\", num_devices)\n",
        "\n",
        "requested_device = \"gpu\"\n",
        "\n",
        "jax.config.update(\"jax_platform_name\", requested_device) # Make sure we're using the GPU\n",
        "\n",
        "device = jax.default_backend()\n",
        "if device != requested_device:\n",
        "    warnings.warn(f\"not using {requested_device}. Using {device}\")\n",
        "else:\n",
        "    print(f\"using {device}\")\n",
        "\n",
        "\n",
        "#####################################\n",
        "##        jax.lax matmul presets   ##\n",
        "#####################################\n",
        "## 'ANY_F8_ANY_F8_F32',\n",
        "## 'ANY_F8_ANY_F8_F32_FAST_ACCUM'\n",
        "## 'ANY_F8_ANY_F8_ANY'\n",
        "## 'ANY_F8_ANY_F8_ANY_FAST_ACCUM'\n",
        "## 'F16_F16_F16'\n",
        "## 'F16_F16_F32'\n",
        "## 'BF16_BF16_BF16'\n",
        "## 'BF16_BF16_F32'\n",
        "## 'BF16_BF16_F32_X3'\n",
        "## 'BF16_BF16_F32_X6'\n",
        "## 'TF32_TF32_F32'\n",
        "## 'TF32_TF32_F32_X3'\n",
        "## 'F32_F32_F32'\n",
        "## 'F64_F64_F64'\n",
        "#####################################\n",
        "\n",
        "jax.config.update(\"jax_default_matmul_precision\", \"BF16_BF16_F32\") # Set the default precision for matrix multiplication\n",
        "\n",
        "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
        "#os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
        "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
        "\n",
        "if device == \"tpu\":\n",
        "    def list_tpu_memory():\n",
        "        devices = jax.devices()\n",
        "        for device in devices:\n",
        "            if 'TPU' in str(device.device_kind):\n",
        "                print(f\"Device: {device}, Memory: {device.memory_stats()['bytes_limit']/(1024*1024)},  Used: {device.memory_stats()['bytes_in_use']/(1024*1024)}\")\n",
        "\n",
        "    list_tpu_memory()\n",
        "\n",
        "# Set up device mesh\n",
        "\n",
        "\n",
        "mesh = jax.sharding.Mesh(jax.devices(), [\"devices\"])\n",
        "\n",
        "# Test the device\n",
        "A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Make sure matmul is fast\n",
        "%timeit (A@A).block_until_ready()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GenG5X6ygFzl",
        "outputId": "36f25c6b-726b-4636-a9e5-420e965d1258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: run_20250722_mpbtmk\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from datetime import datetime\n",
        "import string\n",
        "from jaxpt.checkpointers import save_checkpoint, load_checkpoint, load_checkpoint_from_gcloud\n",
        "\n",
        "def generate_random_code(length=6):\n",
        "    return ''.join(random.choices(string.ascii_lowercase, k=length))\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
        "random_code = generate_random_code()\n",
        "\n",
        "run_dirname = f\"run_{timestamp}_{random_code}\"\n",
        "print(f\"Run: {run_dirname}\")\n",
        "\n",
        "if platform == \"cuda\":\n",
        "  output_dir = Path(\"/home/ubuntu/alpha_training_runs\") # Lambda Labs setup\n",
        "else:\n",
        "  output_dir = Path().absolute().parent  / \"alpha_training_runs\" # Local setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "wZ04n3v2N-zw",
        "outputId": "5d97e665-0b70-4fff-8a44-5d0907da456f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<script> (()=>{ if (customElements.get('treescope-container') === undefined) { class TreescopeContainer extends HTMLElement { constructor() { super(); this.attachShadow({mode: \"open\"}); this.defns = {}; this.state = {}; } } customElements.define(\"treescope-container\", TreescopeContainer); } if (customElements.get('treescope-run-here') === undefined) { class RunHere extends HTMLElement { constructor() { super() } connectedCallback() { const run = child => { const fn = new Function(child.textContent); child.textContent = \"\"; fn.call(this); this.remove(); }; const child = this.querySelector(\"script\"); if (child) { run(child); } else { new MutationObserver(()=>{ run(this.querySelector(\"script\")); }).observe(this, {childList: true}); } } } customElements.define(\"treescope-run-here\", RunHere); } })(); </script> <treescope-container class=\"treescope_out_ba56c5424e514b89acf66c3042482ef3\" style=\"display:block\"></treescope-container> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_ba56c5424e514b89acf66c3042482ef3\")) .filter((elt) => !elt.dataset.setup) )[0]; root.dataset.setup = 1; const msg = document.createElement(\"span\"); msg.style = \"color: #cccccc; font-family: monospace;\"; msg.textContent = \"(Loading...)\"; root.state.loadingMsg = msg; root.shadowRoot.appendChild(msg); root.state.chain = new Promise((resolve, reject) => { const observer = new IntersectionObserver((entries) => { for (const entry of entries) { if (entry.isIntersecting) { resolve(); observer.disconnect(); return; } } }, {rootMargin: \"1000px\"}); window.setTimeout(() => { observer.observe(root); }, 0); }); root.state.deferring = false; const _insertNode = (node) => { for (let oldScript of node.querySelectorAll(\"script\")) { let newScript = document.createElement(\"script\"); newScript.type = oldScript.type; newScript.textContent = oldScript.textContent; oldScript.parentNode.replaceChild(newScript, oldScript); } if (root.state.loadingMsg) { root.state.loadingMsg.remove(); root.state.loadingMsg = null; } root.shadowRoot.appendChild(node); }; root.defns.insertContent = ((contentNode, compressed) => { if (compressed) { root.state.deferring = true; } if (root.state.deferring) { root.state.chain = (async () => { await root.state.chain; if (compressed) { const encoded = contentNode.textContent; const blob = new Blob([ Uint8Array.from(atob(encoded), (m) => m.codePointAt(0)) ]); const reader = blob.stream().pipeThrough( new DecompressionStream(\"deflate\") ).pipeThrough( new TextDecoderStream(\"utf-8\") ).getReader(); const parts = []; while (true) { const step = await reader.read(); if (step.done) { break; } parts.push(step.value); } const tpl = document.createElement('template'); tpl.innerHTML = parts.join(\"\"); _insertNode(tpl.content); } else { _insertNode(contentNode.content); } })(); } else { _insertNode(contentNode.content); } }); </script></treescope-run-here><div style=\"display:none\"> <script type=\"application/octet-stream\" >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</script> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_ba56c5424e514b89acf66c3042482ef3\")) .filter((elt) => !elt.dataset['step0']) )[0]; root.dataset['step0'] = 1; root.defns.insertContent( this.parentNode.querySelector('script[type=\"application/octet-stream\"]'), true ); this.parentNode.remove(); </script></treescope-run-here> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from flax import nnx\n",
        "\n",
        "from jaxpt.infer import generate_completions, generate\n",
        "from jaxpt.models import SmolLM, SmolLM_Config\n",
        "from jaxpt.models.smol_lm import from_hf_pretrained\n",
        "from jaxpt.utils import count_params, create_sharded_model\n",
        "\n",
        "import tiktoken\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "key = jax.random.PRNGKey(1337)\n",
        "rngs = nnx.Rngs(key)\n",
        "config = SmolLM_Config(\n",
        "                     name=\"HuggingFaceTB/SmolLM-135M\",\n",
        "                     dtype=jnp.bfloat16, \\\n",
        "                     vocab_size=49152,\n",
        "                     n_layer=30,\n",
        "                     block_size=2048,\n",
        "                     n_head=9,\n",
        "                     n_kv_head=3,\n",
        "                     n_mlp_hidden=1536,\n",
        "                     sdpa_implementation=\"cudnn\" if device==\"gpu\" else \"xla\")\n",
        "nnx.display(config)\n",
        "\n",
        "with mesh:\n",
        "    #m = create_sharded_model(Tiny_MoE, config, rngs)\n",
        "    #m = load_checkpoint(Tiny_MoE, output_dir, config, \"run_20250716_htlpzj\", 180000, rngs)\n",
        "    #m = load_checkpoint(Tiny_MoE, output_dir, config, \"run_20250716_htlpzj\", 381469, rngs)\n",
        "    m = from_hf_pretrained(config, rngs)\n",
        "    #m = load_hf_pretrained()\n",
        "\n",
        "    graphdef, rngstate, state = nnx.split(m, nnx.RngState, ...)\n",
        "    total_params = count_params(m)\n",
        "    moe_params = count_params(m, \"moe\")\n",
        "\n",
        "    print(f\"Parameter Count: {total_params:,}\")\n",
        "    print(f\"MOE Parameter Count: {moe_params:,}\")\n",
        "    print(f\"Replicated Parameter Count: {total_params - moe_params:,}\")\n",
        "    nnx.display(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-qM6VWsgFzm"
      },
      "outputs": [],
      "source": [
        "with mesh:\n",
        "    enc = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n",
        "    completions = generate_completions(m, enc=enc, num_completions=8,\n",
        "                                    max_length=21,\n",
        "                                    prefix=\"Once upon a time, there lived a\")\n",
        "    for completion in completions:\n",
        "        print(completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDIjMMkygFzm"
      },
      "source": [
        "## Calculate Loss Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2Wr0BTYgFzm"
      },
      "outputs": [],
      "source": [
        "from jaxpt.dataloaders import  BlendedCloudDataLoader, HuggingfaceDataLoader\n",
        "\n",
        "train_dl = HuggingfaceDataLoader(batch_size=16,\n",
        "                                 block_size=2048,\n",
        "                                 device_rank=1,\n",
        "                                 tokenizer=\"HuggingFaceTB/SmolLM-135M\",\n",
        "                                 dataset_paths=[\"HuggingFaceTB/smollm-corpus\",\n",
        "                                                \"HuggingFaceTB/smollm-corpus\"],\n",
        "                                 dataset_names=[\"cosmopedia-v2\",\n",
        "                                                \"fineweb-edu-dedup\"],\n",
        "                                 probabilities=[0.20,  0.80],\n",
        "                                 label=\"train\",\n",
        "                                 random_seed=1337,\n",
        "                                 buffer_size=10_000,\n",
        "                                 streaming=True)\n",
        "\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/alpha-448101-282bc1b884cd.json\"\n",
        "'''\n",
        "train_dl = BlendedCloudDataLoader(\n",
        "    device_rank=1,\n",
        "    block_size=2048,\n",
        "    batch_size=16,\n",
        "    bucket_names=[\"jaxpt_datasets\", \"jaxpt_datasets\", \"jaxpt_datasets\"],\n",
        "    bucket_prefixes=[\"smollm-corpus/processed/fineweb-edu-dedup\",\n",
        "    \"smollm-corpus/processed/python-edu\",\n",
        "    \"smollm-corpus/processed/cosmopedia-v2\"],\n",
        "    proportions=[87, 2, 11],\n",
        "    label=\"train\"\n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkbfM8R5gFzm"
      },
      "outputs": [],
      "source": [
        "import optax\n",
        "\n",
        "@nnx.jit\n",
        "def loss_fn(model, batch, targets):\n",
        "    logits = model(batch)\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets)\n",
        "    return loss.mean()\n",
        "\n",
        "for i in range(50):\n",
        "  batch, targets = train_dl()\n",
        "  batch = batch.squeeze()\n",
        "  targets = targets.squeeze()\n",
        "  loss = loss_fn(m, batch, targets)\n",
        "  print(loss)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}