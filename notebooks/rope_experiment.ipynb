{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXU2qDN8nMgg"
   },
   "source": [
    "# RoPE\n",
    "\n",
    "This experiment trains a GPT model with and without RoPE positional embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04pFw2g58HJl"
   },
   "source": [
    "### Configure the machine and install packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LNLo9jfLn8bg",
    "outputId": "bbfe9c56-3c83-468e-8ab4-8e6b57b656c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "import sys\n",
    "\n",
    "import jax\n",
    "\n",
    "platform : Literal[\"darwin\", \"colab\", \"cuda\"] = \"darwin\"\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    platform = \"colab\"\n",
    "except ImportError:\n",
    "    devices = jax.devices()\n",
    "    if any(d.platform == \"gpu\" for d in devices):\n",
    "        platform = \"cuda\"\n",
    "\n",
    "print(f\"Running on {platform}\")\n",
    "\n",
    "if platform == \"colab\":\n",
    "    !git clone https://github.com/novastar53/jaxpt\n",
    "    !cd jaxpt && git checkout main && git pull\n",
    "    !pip install tiktoken --quiet\n",
    "    !pip uninstall -y tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQl8dEgLnMgh",
    "outputId": "8f60e899-1a2b-4186-e462-cb8217321abc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/jaxpt/src\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "if platform == \"colab\":\n",
    "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"src\" )\n",
    "else:\n",
    "    jaxpt_dir = str(Path().absolute().parent / \"src\" )\n",
    "\n",
    "sys.path.append(jaxpt_dir)\n",
    "print(jaxpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJo6Xji39g54",
    "outputId": "ab378c90-1c73-4003-d930-af685797708e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.5.2\n",
      "Flax version: 0.10.4\n",
      "Available devices: 8\n",
      "using gpu\n",
      "895 μs ± 54 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import jax\n",
    "import flax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "os.environ['XLA_FLAGS'] = (\n",
    "    '--xla_gpu_triton_gemm_any=True '\n",
    "    '--xla_gpu_enable_latency_hiding_scheduler=true '\n",
    ")\n",
    "\n",
    "os.environ.update({\n",
    "  \"NCCL_LL128_BUFFSIZE\": \"-2\",\n",
    "  \"NCCL_LL_BUFFSIZE\": \"-2\",\n",
    "  \"NCCL_PROTO\": \"SIMPLE,LL,LL128\",\n",
    " })\n",
    "\n",
    "\n",
    "# Hardware setup\n",
    "print(\"JAX version:\", jax.__version__)\n",
    "print(\"Flax version:\", flax.__version__)\n",
    "devices = jax.devices()\n",
    "num_devices = len(devices)\n",
    "print(\"Available devices:\", num_devices)\n",
    "\n",
    "REQUESTED_DEVICE = \"gpu\"\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", REQUESTED_DEVICE) # Make sure we're using the GPU\n",
    "\n",
    "device = jax.default_backend()\n",
    "if device != REQUESTED_DEVICE:\n",
    "    warnings.warn(f\"not using {REQUESTED_DEVICE}. Using {device}\")\n",
    "else:\n",
    "    print(f\"using {device}\")\n",
    "\n",
    "\n",
    "#####################################\n",
    "##        jax.lax matmul presets   ##\n",
    "#####################################\n",
    "## 'ANY_F8_ANY_F8_F32',\n",
    "## 'ANY_F8_ANY_F8_F32_FAST_ACCUM'\n",
    "## 'ANY_F8_ANY_F8_ANY'\n",
    "## 'ANY_F8_ANY_F8_ANY_FAST_ACCUM'\n",
    "## 'F16_F16_F16'\n",
    "## 'F16_F16_F32'\n",
    "## 'BF16_BF16_BF16'\n",
    "## 'BF16_BF16_F32'\n",
    "## 'BF16_BF16_F32_X3'\n",
    "## 'BF16_BF16_F32_X6'\n",
    "## 'TF32_TF32_F32'\n",
    "## 'TF32_TF32_F32_X3'\n",
    "## 'F32_F32_F32'\n",
    "## 'F64_F64_F64'\n",
    "#####################################\n",
    "\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"BF16_BF16_F32\") # Set the default precision for matrix multiplication\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", True) # Make sure the highest precision is enabled in case we need\n",
    "#os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"1\"\n",
    "#os.environ[\"JAX_ENABLE_X64\"] = \"False\"\n",
    "\n",
    "if device == \"tpu\":\n",
    "    def list_tpu_memory():\n",
    "        devices = jax.devices()\n",
    "        for device in devices:\n",
    "            if 'TPU' in str(device.device_kind):\n",
    "                print(f\"Device: {device}, Memory: {device.memory_stats()['bytes_limit']/(1024*1024)},  Used: {device.memory_stats()['bytes_in_use']/(1024*1024)}\")\n",
    "\n",
    "    list_tpu_memory()\n",
    "\n",
    "# Test the device\n",
    "A = jnp.array(np.random.normal(size=(4096, 4096)), dtype=jnp.float32) # Makes sure the matmul is fast\n",
    "%timeit (A@A).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDtBhnjKN-zw"
   },
   "source": [
    "### Initialize the baseline and candidate models and perform a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "wZ04n3v2N-zw",
    "outputId": "04ad9a13-d864-45a8-9ae5-4a0fc5f292d9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script> (()=>{ if (customElements.get('treescope-container') === undefined) { class TreescopeContainer extends HTMLElement { constructor() { super(); this.attachShadow({mode: \"open\"}); this.defns = {}; this.state = {}; } } customElements.define(\"treescope-container\", TreescopeContainer); } if (customElements.get('treescope-run-here') === undefined) { class RunHere extends HTMLElement { constructor() { super() } connectedCallback() { const run = child => { const fn = new Function(child.textContent); child.textContent = \"\"; fn.call(this); this.remove(); }; const child = this.querySelector(\"script\"); if (child) { run(child); } else { new MutationObserver(()=>{ run(this.querySelector(\"script\")); }).observe(this, {childList: true}); } } } customElements.define(\"treescope-run-here\", RunHere); } })(); </script> <treescope-container class=\"treescope_out_ec4e7df578b946039b1b3468803c5cfe\" style=\"display:block\"></treescope-container> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_ec4e7df578b946039b1b3468803c5cfe\")) .filter((elt) => !elt.dataset.setup) )[0]; root.dataset.setup = 1; const msg = document.createElement(\"span\"); msg.style = \"color: #cccccc; font-family: monospace;\"; msg.textContent = \"(Loading...)\"; root.state.loadingMsg = msg; root.shadowRoot.appendChild(msg); root.state.chain = new Promise((resolve, reject) => { const observer = new IntersectionObserver((entries) => { for (const entry of entries) { if (entry.isIntersecting) { resolve(); observer.disconnect(); return; } } }, {rootMargin: \"1000px\"}); window.setTimeout(() => { observer.observe(root); }, 0); }); root.state.deferring = false; const _insertNode = (node) => { for (let oldScript of node.querySelectorAll(\"script\")) { let newScript = document.createElement(\"script\"); newScript.type = oldScript.type; newScript.textContent = oldScript.textContent; oldScript.parentNode.replaceChild(newScript, oldScript); } if (root.state.loadingMsg) { root.state.loadingMsg.remove(); root.state.loadingMsg = null; } root.shadowRoot.appendChild(node); }; root.defns.insertContent = ((contentNode, compressed) => { if (compressed) { root.state.deferring = true; } if (root.state.deferring) { root.state.chain = (async () => { await root.state.chain; if (compressed) { const encoded = contentNode.textContent; const blob = new Blob([ Uint8Array.from(atob(encoded), (m) => m.codePointAt(0)) ]); const reader = blob.stream().pipeThrough( new DecompressionStream(\"deflate\") ).pipeThrough( new TextDecoderStream(\"utf-8\") ).getReader(); const parts = []; while (true) { const step = await reader.read(); if (step.done) { break; } parts.push(step.value); } const tpl = document.createElement('template'); tpl.innerHTML = parts.join(\"\"); _insertNode(tpl.content); } else { _insertNode(contentNode.content); } })(); } else { _insertNode(contentNode.content); } }); </script></treescope-run-here><div style=\"display:none\"> <script type=\"application/octet-stream\" >eNrdWglz4jgW/itqd+0ENsEBEsjVodYQrnTnJN1JZ2eKFbZslNiSY8sQMtX/fZ9kLoOhp2sy2emFqhikp3d8enqHyIdQjFxS0UVASGhyn3QDzgX6Hfk8pIJydogC4mJBB+QI2ZyJnI096o4OkccZD31swviwTwXJqS+HyA9gxKWhyCnWOTHyYZRxBsM9bD46AY+YlTO5y4PDeOkRGn/ruUAA/Kgl+ofIpgLImCBMHCGPstx4vJDP/wN48edcSF8oc2AdDywS5GDoCPnYsmAw5xJbHKKi2ZfaMJLrE+r0YaSgl6Q8JjAF46b8xx9yAxrSHnWpABNxJPiUNkeZCCgLqSnFknh2bNe3D9sxjh+mOOaCiIHMAMZCM6C+QBKI4w3s+y41sYR2m5uCSJgCgr2NSiaTPa4A8iAvFMgiNgvRMRJ9GuoOEdewLefcIpms3ueh0NU8mEYE6vqESZMNU3KVi/79W9pMCzPLJTDNItc9iiXooGaHcwajmSEPHrNoXgd+C0NyKjEsqCkHfRLYPPAwM4nO+DCTVY4AAjJLMygXL/qAdopZ4ENtlFnQWncJc0QfHR+jvCRZq3pARBQwwB0RNyQzxfoRk5otsg771BZSP0UgP3yD9woJGXA/ZvGhHpCniITCYNRT29UIsEcyMSZZyeNoSZAfhf0YxqMUGycijmMz1lj5x3WQWsQbKbjjuPHx7aojBt7qS15yhLhiC5EBOPh4J6V26rv+SEYSdC3QpEJjYt10cRh+glM85pvRpjy7HrihNhH+LQt4gvsrH6982E47ABYdIMXwWEvGGQ0J3ANLyfOxltfg6AZimYQzUBHAYDC17jCkI5CRaya2a3AY43inAk4X93oBGSj/UfHnfXm/iPN5sGpMYHLPg4VzFFi9pPELJPiQcZE57PMBCbIp9Eny7rBPWJc8+7DlxFJLdZu7Fu6BBQxMO+zjMFNxcY+4leRMN7YzFmf2iflIrGwW/TOLVksFBMEtrTmKPLxse0bBIq9HgnmCg/1CqTwjCGX0c+Zl7BRKhZIkSKi3IncAXmlWALVFQ9/Fo0mOWCREFaRQODzsEYgqZE4DU72OUuXF4T9XkPF/nDdgX6eyKFNJoedymXBWylS7uSzZwsFjSLADnsqWV7/Sbk51kEvTF03oExqqPHaINn4tlnrmxv9SveSilUqW30BJuY9ScBSEcgN9DtmcBClyafh6YtVRUIJyKv6Eq3z8daTOzBPkWSxL0WnYtWkQii5nXen+KUdr3VHSiyV5mlK3Cv1p9eMdX1RRWuXhwIH6K1ZDHehvf1IahDR/1IuEgMInLQDNptOcVkPaAhUACdVvOvGvpLBraQvF88YZBq+g2EWdkdfjboguIiHttVAtXglPfwQHIzckvUcohNXy0IPc1lclL2YCllMcEmtaPr8nefk+WnbzeLUqW/P6AfEWrYzPR4oV6eFutlIf4rBrQkULwE7XY1skUskkTq+TubAmKXIeejTAQSaXs7DAOcxgY1VhlJ0flkJktRdgNvFmxRYVQkQAMSjlczwSP2bKVAPYGEqsd0lNlEj0jno+DwRmS7x7AX+EhC9HZsHo++jOLZvDc7LN33RZO4FiVteEytsKCBurmuxtgGeS8NXqjenRGSfSxFE1sWtmoAGD6r/gP6uCUQ8Fluun+v5lmoxbwlgTiwuwXWoxD95ThF0G9XQXWlabPgOTxDHZV8cEmgccgMJDHDA4eN1JYJ/shW1js7CTQuhD9f37tBsNxs2njF5jkMZDubyuwuqsNz5UDSsOck6ALQrblkGFnZJFnC3EwaUdgvKgXtnsb8UuDmWzDBhqCI1hXtJlKbK+TthGSwF6Ys83XZW4ssROi7Ey5UEgTqWJi7M5KpC6ilHMAXbDxT7Ewu/Xkz+eLFZLmCmqJ+r5FTSvoUeaiAQUs94vDYpkc6Unmzq0hsOiqSzRZqYT/hFhK/m84s2KbHbROyMI8Ei3A+5BA21GsgvT5YEP9QF2ocvOZLN6yKG9VmFAtsnyqccpW7bIfzBpaxtwKLLTS4mwT4iQNxdkiGqdTkda05Fj8h5CTUKXDyabpDNiZuY//xoXCiaZBKQfLxriOCYlSRwDD7vjseH4EmxXNrlhYB6iKHAzMoMdyvntIbft4lEPcmR5d8vKHzTPHKNqqFf7yjC4+lS9HsLfVsMw6sa6V9UzDOeRf7Ta9Wpt+NUwbr7WTo2zdrVmNJzndutTX4TVM0qcncbJXfFTu/x10PEjenlWuimc3rWvv5wNbs9exOWo0aht3jqPN7R6ku/Tk6votG41H/Kt3rY9aFv+08dy/+mW0qvojDX7LfuzMD6Xq+fBrtFos8d62fwcRWzzuvRkho/Dgd1wt5+enTrfd3qnw+Z+oWVsM+O69CkITgvXm85L/trKG6d2wTnfqw2bD0Unz0fR9d6eVy+Uh627gwvH8cnN42iXtHsvJbMXXDQFNpyr9vnwBIej8Cpqt+9u642hcXnlt79an7e3N529m727HZG3P14+GYMS8PxknO8ZZ0PDc16uO5vRfYfU756Ldtl8Od+9bo1KUdX4+FJ98Bv+Dm1d1er5++hyt7PH7Oqneqtx5hl0c39QL/ZZob+32fsyvHsYtoLBSfNzjT3Y9bojNi/Me9fdKx3UTofV/f7B7tlZs7PTvDccr116qF4diJsmaR3Uq9V2c+fE2b3e/mqOekYT9vTLx23jqokNclZzjdZL/cK5F065eulcXLRPqo/0qkQa1btatWHSvN8PuM/AN/z7+knhpfDYsWu26I8+spaFG2HLzp97zfp5uWoZT1+++FiEnXvPsjA9KNovB7uf6cNT2feC8gX/WuvQoOkNTps7ndvOTqNeNKtX9s1my+V+c7cRDkvYeSrv03vSOXf9W1ZttYl1FpDo9qlZ8wq3jeCx03kuFcu3t+HQAI2ySN1CisyGcusNmRr/A3+mpx9b3Ie6YHYk1d2prutrKLbiM/sb8Fp/G9VXl3mqdIurSuAN7sFMlImLu+RVKxzBGy6PL5CNiz85FkJ4kCxkNStLQDzEVCCGB9TBggc6cPZ7HAeWPgyoIDfQ+GVmvMDYMa/ZfR4UJBltrtSVN3kg5YZ6BGrizOSqd2ldQDyoW5eWfttCxXw+r8oOCL5QgWRU05Yud66e1WbKyXZ1EsHk5aeG3qMGpi4ENsGRJH6nIhsUGgxqNojGFDAj2JLl+OY8duNbye/cR8rSfnIhmbz3WSyStMqHOEN/oMyPxplGUzm5x5+1VCbj9A2TceoGJdTipNxkptWSkwuVqVZ5wM++0GXadEOohiCjOvAdzIRFlfHjnF/Wu83LG0DSpk7mw/ZY8znGG5NCaiM5LIe67sBNDmtL3YW2bl6rSJWPExTzV4da5Zf3z8W9I2VF/HGs+dYyMtNyS6ugiZlLRJNaKKnWxqx/2kCc1aSvHm/84GFV18bZDTRt7o41XZqnIZVWj7W5vg8ys5qT3fpiXwpzyjEh8PXh81jTSnLnFh7rILaUB6724D/jsOmOuubaWlu7J+9TNy7NtXUWef5o4s492+VYFMpTVJb9eN32/7DXplgb/y4ADuuKIzWBYn+d6TpRMh7/xRFHfxcvVR6yyk3V5Gv5adrjJznKbwfSOs9TVxVded+QFjXjX2S0SjG/u/9zoTuzaxXEM4q3wHnATdz7Ls6lfLG093MBPTNsFdAzircAGoomPCLBOpQLxZ8s6ccmrcz78fTbgNuHkvf/C1tp0Wpo5ezbIEsAwLXQ7pX3fzZslU2rwVXTb4FuFJLuADMKGHWxECwN5vH/J2iVBnZD8nMhvWjfKsgX6d4CexfQ8EPqcrY2bpBcvvRzgT4zbBXcM4q3ADq0fNylnu8S2SYpEd/vhM3IYizRCa/Ddevvgn2Kras2IYX0r+w+EuZPf1raWNPQxj1eGrLJTrCQRzYlrhXqur6aesZ0riGMH9k/snuTFRYdVP4LJfNY6g==</script> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_ec4e7df578b946039b1b3468803c5cfe\")) .filter((elt) => !elt.dataset['step0']) )[0]; root.dataset['step0'] = 1; root.defns.insertContent( this.parentNode.querySelector('script[type=\"application/octet-stream\"]'), true ); this.parentNode.remove(); </script></treescope-run-here> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script> (()=>{ if (customElements.get('treescope-container') === undefined) { class TreescopeContainer extends HTMLElement { constructor() { super(); this.attachShadow({mode: \"open\"}); this.defns = {}; this.state = {}; } } customElements.define(\"treescope-container\", TreescopeContainer); } if (customElements.get('treescope-run-here') === undefined) { class RunHere extends HTMLElement { constructor() { super() } connectedCallback() { const run = child => { const fn = new Function(child.textContent); child.textContent = \"\"; fn.call(this); this.remove(); }; const child = this.querySelector(\"script\"); if (child) { run(child); } else { new MutationObserver(()=>{ run(this.querySelector(\"script\")); }).observe(this, {childList: true}); } } } customElements.define(\"treescope-run-here\", RunHere); } })(); </script> <treescope-container class=\"treescope_out_99a59e441f0046d39aab450bb8f007df\" style=\"display:block\"></treescope-container> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_99a59e441f0046d39aab450bb8f007df\")) .filter((elt) => !elt.dataset.setup) )[0]; root.dataset.setup = 1; const msg = document.createElement(\"span\"); msg.style = \"color: #cccccc; font-family: monospace;\"; msg.textContent = \"(Loading...)\"; root.state.loadingMsg = msg; root.shadowRoot.appendChild(msg); root.state.chain = new Promise((resolve, reject) => { const observer = new IntersectionObserver((entries) => { for (const entry of entries) { if (entry.isIntersecting) { resolve(); observer.disconnect(); return; } } }, {rootMargin: \"1000px\"}); window.setTimeout(() => { observer.observe(root); }, 0); }); root.state.deferring = false; const _insertNode = (node) => { for (let oldScript of node.querySelectorAll(\"script\")) { let newScript = document.createElement(\"script\"); newScript.type = oldScript.type; newScript.textContent = oldScript.textContent; oldScript.parentNode.replaceChild(newScript, oldScript); } if (root.state.loadingMsg) { root.state.loadingMsg.remove(); root.state.loadingMsg = null; } root.shadowRoot.appendChild(node); }; root.defns.insertContent = ((contentNode, compressed) => { if (compressed) { root.state.deferring = true; } if (root.state.deferring) { root.state.chain = (async () => { await root.state.chain; if (compressed) { const encoded = contentNode.textContent; const blob = new Blob([ Uint8Array.from(atob(encoded), (m) => m.codePointAt(0)) ]); const reader = blob.stream().pipeThrough( new DecompressionStream(\"deflate\") ).pipeThrough( new TextDecoderStream(\"utf-8\") ).getReader(); const parts = []; while (true) { const step = await reader.read(); if (step.done) { break; } parts.push(step.value); } const tpl = document.createElement('template'); tpl.innerHTML = parts.join(\"\"); _insertNode(tpl.content); } else { _insertNode(contentNode.content); } })(); } else { _insertNode(contentNode.content); } }); </script></treescope-run-here><div style=\"display:none\"> <script type=\"application/octet-stream\" >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</script> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_99a59e441f0046d39aab450bb8f007df\")) .filter((elt) => !elt.dataset['step0']) )[0]; root.dataset['step0'] = 1; root.defns.insertContent( this.parentNode.querySelector('script[type=\"application/octet-stream\"]'), true ); this.parentNode.remove(); </script></treescope-run-here> </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "from flax import nnx\n",
    "\n",
    "from jaxpt.infer import generate_completions\n",
    "from jaxpt.models import RoPE_GPT, NoPE_GPT, RoPE_GPTConfig, NoPE_GPTConfig\n",
    "\n",
    "\"\"\"\n",
    "+--------------+---------+--------+------+\n",
    "| Model        | Layers  | Heads  | Embd |\n",
    "+--------------+---------+--------+------+\n",
    "| gpt2-medium  | 24      | 16     | 1024 |\n",
    "| gpt2-large   | 36      | 20     | 1280 |\n",
    "| gpt2-xl      | 48      | 25     | 1600 |\n",
    "+--------------+---------+--------+------+\n",
    "\"\"\"\n",
    "\n",
    "key = jax.random.PRNGKey(1337)\n",
    "rngs = nnx.Rngs(key)\n",
    "baseline_config = NoPE_GPTConfig(dtype=jnp.bfloat16, \\\n",
    "                   vocab_size=50257, \\\n",
    "                   block_size=2048, \\\n",
    "                   sdpa_implementation=\"cudnn\" if device==\"gpu\" else \"xla\")\n",
    "nnx.display(baseline_config)\n",
    "candidate_config = RoPE_GPTConfig(dtype=jnp.bfloat16, \\\n",
    "                   vocab_size=50257, \\\n",
    "                   block_size=2048, \\\n",
    "                   sdpa_implementation=\"cudnn\" if device==\"gpu\" else \"xla\")\n",
    "nnx.display(candidate_config)\n",
    "baseline = NoPE_GPT(baseline_config, rngs)\n",
    "candidate = RoPE_GPT(candidate_config, rngs)\n",
    "\n",
    "#m = load_checkpoint(\"run_20250311_uqdwjq\", 5600)\n",
    "graphdef, rngstate, state = nnx.split(baseline, nnx.RngState, ...)\n",
    "nnx.display(state)\n",
    "graphdef, rngstate, state = nnx.split(candidate, nnx.RngState, ...)\n",
    "nnx.display(state)\n",
    "\n",
    "#completions = generate_completions()\n",
    "#for completion in completions:\n",
    "#print(completion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHA3hbjj8HJl"
   },
   "source": [
    "### Configure Logging and Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lki2khsFnMgh",
    "outputId": "90e5e34c-b997-4f8b-8e7d-7cb6dae360aa"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "def generate_random_code(length=6):\n",
    "    return ''.join(random.choices(string.ascii_lowercase, k=length))\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "random_code = generate_random_code()\n",
    "\n",
    "baseline_run_dirname = f\"baseline_run_{timestamp}_{random_code}\"\n",
    "print(f\"Baseline Run: {baseline_run_dirname}\")\n",
    "candidate_run_dirname = f\"candidate_run_{timestamp}_{random_code}\"\n",
    "print(f\"Candidate Run: {candidate_run_dirname}\")\n",
    "\n",
    "if platform == \"colab\":\n",
    "  output_dir = Path().absolute().parent\n",
    "elif platform == \"cuda\":\n",
    "  output_dir = Path(\"/home/ubuntu/gpt2-train\") # Lambda Labs setup\n",
    "else:\n",
    "  output_dir = Path().absolute().parent # Local setup\n",
    "\n",
    "# Create checkpoint dir\n",
    "checkpoint_dir = output_dir / \"checkpoints\"\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Checkpoint directory: {checkpoint_dir}\")\n",
    "\n",
    "# Create log dir\n",
    "log_dir = output_dir / \"logs\"\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Log directory: {log_dir}\")\n",
    "\n",
    "def save_checkpoint(m, run_dirname, step):\n",
    "  checkpoint_path = checkpoint_dir / run_dirname / f\"checkpoint-{step}.pt\"\n",
    "  m.save_checkpoint(checkpoint_path)\n",
    "\n",
    "def load_checkpoint(model, run_dirname, step):\n",
    "  checkpoint_path = checkpoint_dir / run_dirname / f\"checkpoint-{step}.pt\"\n",
    "  m = model.load_checkpoint(checkpoint_path, rngs)\n",
    "  return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTx8xq6JE6J8"
   },
   "source": [
    "### Configure Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZvgSOm39E6J8",
    "outputId": "5936683a-9498-4cd3-ba27-58637c3ce98e"
   },
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "############################\n",
    "# Nvidia A100 (x 8) Config #\n",
    "############################\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class TrainerConfig:\n",
    "  num_tokens_per_batch: int = 2**19 # 2**19, 0.5 million as per the GPT 3.5 paper\n",
    "  mB: int = 32\n",
    "  T: int = 2048\n",
    "  max_steps: int = 99 * 100000000 // (2**19)  # 1 epoch (99 shards of the dataset should be around 18883, but Karpathy used 19073 for some reason)\n",
    "  max_lr: float = 3*6e-4\n",
    "  min_lr: float = max_lr * 0.1\n",
    "  max_grad_norm: float = 1.0  # Clip gradients to this norm\n",
    "  warmup_steps: int = 715 #715\n",
    "  print_interval: int = 20 # 20\n",
    "  eval_interval: int = 250 # 250\n",
    "  checkpoint_interval: int = 250 # 250\n",
    "  grad_accumulation_steps: int = num_tokens_per_batch // (mB * T * num_devices) # Number of steps over which to average the gradient\n",
    "\n",
    "\n",
    "##############\n",
    "# CPU Config #\n",
    "##############\n",
    "\n",
    "trconf = TrainerConfig()\n",
    "'''\n",
    "trconf = TrainerConfig(\n",
    "  num_tokens_per_batch=2**9,\n",
    "  mB=16,\n",
    "  T=32,\n",
    "  max_steps=9*48*10, # 6 epoch(s)\n",
    "  max_lr=6e-4,\n",
    "  min_lr=6e-5,\n",
    "  max_grad_norm=1.0,\n",
    "  warmup_steps=10,\n",
    "  print_interval=20,\n",
    "  eval_interval=250,\n",
    "  checkpoint_interval=0,\n",
    "\n",
    ")\n",
    "\n",
    "trconf.grad_accumulation_steps =  trconf.num_tokens_per_batch // (trconf.mB * trconf.T * num_devices) # Number of steps over which to average the gradient\n",
    "'''\n",
    "\n",
    "print(f\"tokens/batch: {trconf.num_tokens_per_batch:,}\")\n",
    "print(f\"block size: {trconf.T}\")\n",
    "print(f\"sub-batch size: {trconf.mB}\")\n",
    "print(f\"no. gradient accumulation steps: {trconf.grad_accumulation_steps}\")\n",
    "print(f\"effective batch size per device: \", trconf.grad_accumulation_steps * trconf.mB)\n",
    "print(f\"effective batch size: {trconf.grad_accumulation_steps * trconf.mB * num_devices}\")\n",
    "print(f\"max steps: {trconf.max_steps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSbfLxKzE6J8"
   },
   "source": [
    "### Configure the Baseline and Candidate Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JAjwmFUE6J8",
    "outputId": "566b6690-8306-44f1-bfe1-88d6796da2bc"
   },
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "# Set up the optimizer\n",
    "def warmup_with_cosine_decay_schedule(step):\n",
    "\n",
    "    warmup_lr = trconf.max_lr * (step + 1) / trconf.warmup_steps\n",
    "    coeff = 0.5 * (1 + jnp.cos(jnp.pi * (step - trconf.warmup_steps) / (trconf.max_steps - trconf.warmup_steps)))\n",
    "    cosine_lr =  trconf.min_lr + coeff * (trconf.max_lr - trconf.min_lr)\n",
    "\n",
    "    return jnp.where(step < trconf.warmup_steps,\n",
    "                     warmup_lr,\n",
    "                     jnp.where(step < trconf.max_steps, cosine_lr, trconf.min_lr))\n",
    "\n",
    "# Generate a weight decay mask\n",
    "_, params, _ = nnx.split(baseline, nnx.Param, nnx.Variable)\n",
    "weight_decay_mask = jax.tree_util.tree_map(lambda x: len(x.shape) > 1, params)\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(trconf.max_grad_norm),\n",
    "    optax.adamw(warmup_with_cosine_decay_schedule, b1=0.9, b2=0.95, weight_decay=0.1, mask=weight_decay_mask)\n",
    ")\n",
    "baseline_optimizer = nnx.Optimizer(baseline, tx)\n",
    "\n",
    "# count the number of weight decay params\n",
    "def get_size(x, y):\n",
    "    if x:\n",
    "        return y.size\n",
    "    return 0\n",
    "\n",
    "weight_decay_params = jax.tree_util.tree_map(get_size, weight_decay_mask, params)\n",
    "weight_decay_param_count = jax.tree_util.tree_reduce(lambda x, y: x + y, weight_decay_params, 0)\n",
    "print(f\"baseline weight decay param count: {weight_decay_param_count:,}\")\n",
    "\n",
    "\n",
    "_, params, _ = nnx.split(candidate, nnx.Param, nnx.Variable)\n",
    "weight_decay_mask = jax.tree_util.tree_map(lambda x: len(x.shape) > 1, params)\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(trconf.max_grad_norm),\n",
    "    optax.adamw(warmup_with_cosine_decay_schedule, b1=0.9, b2=0.95, weight_decay=0.1, mask=weight_decay_mask)\n",
    ")\n",
    "candidate_optimizer = nnx.Optimizer(candidate, tx)\n",
    "\n",
    "weight_decay_params = jax.tree_util.tree_map(get_size, weight_decay_mask, params)\n",
    "weight_decay_param_count = jax.tree_util.tree_reduce(lambda x, y: x + y, weight_decay_params, 0)\n",
    "\n",
    "print(f\"candidate weight decay param count: {weight_decay_param_count:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igoeEsgCsSbC"
   },
   "source": [
    "### Configure DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieoqXUqaeQ4r"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "def is_folder_empty_or_missing(folder_path):\n",
    "    \"\"\"Check if the folder is missing or empty.\"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder {folder_path} does not exist.\")\n",
    "        return True\n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise ValueError(f\"{folder_path} exists but is not a folder!\")\n",
    "    if not any(os.scandir(folder_path)):\n",
    "        print(f\"Folder {folder_path} is empty.\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def download_gcs_bucket(bucket_name, destination_folder, folder_prefix=\"\"):\n",
    "    \"\"\"Download all files from the GCS bucket to the destination folder.\"\"\"\n",
    "    # Initialize GCS client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    # List and download all blobs\n",
    "    blobs = bucket.list_blobs(prefix=folder_prefix)\n",
    "    for blob in blobs:\n",
    "        dest_path = os.path.join(destination_folder, blob.name)\n",
    "        if blob.name.endswith(\".npz\"):\n",
    "            # Create destination directory if needed\n",
    "            #os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "            # Download blob\n",
    "            print(f\"Downloading {blob.name} to {dest_path}\")\n",
    "            blob.download_to_filename(dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8d4oBtGEwpy"
   },
   "outputs": [],
   "source": [
    "from jaxpt.dataloaders import DataLoader\n",
    "from jaxpt.eval import calc_validation_loss\n",
    "from jaxpt.train import loss_fn\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"alpha-448101-282bc1b884cd.json\"\n",
    "\n",
    "dataset = \"fineweb-edu\"\n",
    "gcs_bucket = \"jaxpt_datasets\"\n",
    "gcs_folder_prefix = \"fineweb-edu/processed\"\n",
    "\n",
    "if platform == \"colab\":\n",
    "    datasets_folder = Path().absolute() / \"jaxpt\" / \"datasets\"\n",
    "elif platform == \"cuda\":\n",
    "    datasets_folder = Path(\"/home/ubuntu/gpt2-train/\")\n",
    "else:\n",
    "    datasets_folder = Path().absolute().parent / \"datasets\"\n",
    "\n",
    "dataset_path = datasets_folder / dataset / \"processed\"\n",
    "\n",
    "if is_folder_empty_or_missing(dataset_path):\n",
    "    print(\"Folder is missing or empty. Downloading files from GCS...\")\n",
    "    os.makedirs(dataset_path, exist_ok=True)  # Create the folder if it didn't exist\n",
    "    download_gcs_bucket(gcs_bucket, datasets_folder, folder_prefix=gcs_folder_prefix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OW6iqjEukEfr",
    "outputId": "b2efe985-3854-4f2b-c007-def4b2e87109"
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(dirpath=dataset_path,\n",
    "                      batch_size=trconf.mB,\n",
    "                      block_size=trconf.T,\n",
    "                      device_rank=num_devices,\n",
    "                      label=\"train\")\n",
    "eval_dl = DataLoader(dirpath=dataset_path,\n",
    "                     batch_size=trconf.mB,\n",
    "                     block_size=trconf.T,\n",
    "                     device_rank=1,\n",
    "                     label=\"valid\",\n",
    "                     quiet=True)\n",
    "\n",
    "def evaluate(model):\n",
    "  model.eval()\n",
    "  completions = generate_completions(model)\n",
    "  val_loss = calc_validation_loss(model, loss_fn=loss_fn, dataloader=eval_dl)\n",
    "  model.train()\n",
    "  return val_loss, completions\n",
    "\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1m9CNTrTsSbC"
   },
   "source": [
    "### Training Both Models Simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxpt.utils import append_to_csv\n",
    "\n",
    "baseline_train_losses, candidate_train_losses = [], []\n",
    "baseline_val_losses, candidate_val_losses = [], []\n",
    "append_to_csv(log_dir / f\"{baseline_run_dirname}_train.csv\", [\"step\", \"lr\", \"loss\", \"norm\", \"time\", \"tokens_processed\", \"tokens_per_sec\"])\n",
    "append_to_csv(log_dir / f\"{baseline_run_dirname}_valid.csv\", [\"step\", \"loss\"])\n",
    "append_to_csv(log_dir / f\"{candidate_run_dirname}_train.csv\", [\"step\", \"lr\", \"loss\", \"norm\", \"time\", \"tokens_processed\", \"tokens_per_sec\"])\n",
    "append_to_csv(log_dir / f\"{candidate_run_dirname}_valid.csv\", [\"step\", \"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UwtmfUotuLMU",
    "outputId": "9129bb92-c31d-4a56-d4c0-4fe2cc3ce0d9"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from jaxpt.train import parallel_train_step\n",
    "\n",
    "\n",
    "baseline.train()\n",
    "candidate.train()\n",
    "\n",
    "try:\n",
    "  while step < trconf.max_steps:\n",
    "    start = time.time()\n",
    "    batch, target = train_dl()\n",
    "\n",
    "    # baseline step\n",
    "    baseline_avg_loss, baseline_avg_grads = parallel_train_step(baseline, baseline_optimizer, batch, target)\n",
    "    baseline_avg_loss.block_until_ready()\n",
    "\n",
    "    # candidate step\n",
    "    candidate_avg_loss, candidate_avg_grads = parallel_train_step(candidate, candidate_optimizer, batch, target)\n",
    "    candidate_avg_loss.block_until_ready()\n",
    "\n",
    "    # compute stats\n",
    "    baseline_avg_loss = jnp.float32(baseline_avg_loss[0])\n",
    "    candidate_avg_loss = jnp.float32(candidate_avg_loss[0])\n",
    "\n",
    "    lr = warmup_with_cosine_decay_schedule(step)\n",
    "    norm = 0 # norm[0]|\n",
    "    iter_time = time.time() - start\n",
    "    sub_step_time = iter_time / trconf.grad_accumulation_steps\n",
    "    tokens_per_sec = num_devices * trconf.mB * trconf.T * trconf.grad_accumulation_steps / iter_time\n",
    "    tokens_processed = (step+1) * num_devices * trconf.grad_accumulation_steps * trconf.mB * trconf.T\n",
    "\n",
    "    if step % trconf.print_interval == 0:\n",
    "      baseline_train_losses.append((step, baseline_avg_loss))\n",
    "      candidate_train_losses.append((step, candidate_avg_loss))\n",
    "      append_to_csv(log_dir / f\"{baseline_run_dirname}_train.csv\", [step, lr, baseline_avg_loss, norm, iter_time*1000, tokens_processed, tokens_per_sec])\n",
    "      append_to_csv(log_dir / f\"{candidate_run_dirname}_train.csv\", [step, lr, candidate_avg_loss, norm, iter_time*1000, tokens_processed, tokens_per_sec])\n",
    "      print(f\"{step} | lr: {lr:0.2e} | baseline loss: {baseline_avg_loss:0.4f} | \"\n",
    "            f\"candidate loss: {candidate_avg_loss:0.4f} | \"\n",
    "            f\"tokens processed: {tokens_processed:,} | \"\n",
    "            f\"tok/sec: {tokens_per_sec:,.2f}\", end=\"\\r\")\n",
    "    if step % trconf.eval_interval == 0:\n",
    "      baseline_valid_loss, baseline_completions = evaluate(baseline)\n",
    "      baseline_valid_loss = jnp.float32(baseline_valid_loss)\n",
    "      baseline_val_losses.append((step, baseline_valid_loss))\n",
    "      append_to_csv(log_dir / f\"{baseline_run_dirname}_valid.csv\", [step, baseline_valid_loss])\n",
    "      candidate_valid_loss, candidate_completions = evaluate(candidate)\n",
    "      candidate_valid_loss = jnp.float32(candidate_valid_loss)\n",
    "      candidate_val_losses.append((step, jnp.float32(candidate_valid_loss)))\n",
    "      append_to_csv(log_dir / f\"{baseline_run_dirname}_valid.csv\", [step, candidate_valid_loss])\n",
    "      print(f\"baseline valid loss: {baseline_valid_loss:0.4f} |\"\n",
    "            f\"candidate valid loss: {candidate_valid_loss:0.4f} \"  )\n",
    "\n",
    "      print(\"Baseline completions:\")\n",
    "      for completion in baseline_completions:\n",
    "        print(completion)\n",
    "      print(\"Candidate completions\")\n",
    "      for completion in candidate_completions:\n",
    "        print(completion)\n",
    "    #if step > 0 and trconf.checkpoint_interval > 0 and step % trconf.checkpoint_interval == 0:\n",
    "      #save_checkpoint(m, step)\n",
    "\n",
    "    step += 1\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Received KeyboardInterrupt. Exiting...\")\n",
    "\n",
    "baseline_valid_loss, baseline_completions = evaluate(baseline)\n",
    "baseline_valid_loss = jnp.float32(baseline_valid_loss)\n",
    "candidate_valid_loss, candidate_completions = evaluate(candidate)\n",
    "candidate_valid_loss = jnp.float32(candidate_valid_loss)\n",
    "print(f\"baseline valid loss: {baseline_valid_loss:0.4f} | \"\n",
    "      f\"candidate valid loss: {candidate_valid_loss:0.4f}\")\n",
    "print(f\"baseline completions\")\n",
    "for completion in baseline_completions:\n",
    "  print(completion)\n",
    "print(f\"candidate completions\")\n",
    "for completion in candidate_completions:\n",
    "  print(completion)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "axes[0].plot([x[0] for x in baseline_train_losses], [x[1] for x in baseline_train_losses], label=\"baseline\")\n",
    "axes[0].plot([x[0] for x in candidate_train_losses], [x[1] for x in candidate_train_losses], label=\"candidate\")\n",
    "axes[0].set_title(\"Train Loss\")\n",
    "\n",
    "axes[1].plot([x[0] for x in baseline_val_losses], [x[1] for x in baseline_val_losses], label=\"baseline\")\n",
    "axes[1].plot([x[0] for x in candidate_val_losses], [x[1] for x in candidate_val_losses], label=\"candidate\")\n",
    "axes[1].set_title(\"Valid Loss\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(log_dir / f\"{baseline_run_dirname}.png\", dpi=300, bbox_inches=\"tight\", transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
