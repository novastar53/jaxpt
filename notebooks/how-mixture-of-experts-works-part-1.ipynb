{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "261cbe5f",
   "metadata": {},
   "source": [
    "# How do Mixture of Expert Layers Work? Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad475d3",
   "metadata": {},
   "source": [
    "Mixture of Expert layers have become increasingly popular in newer large language models. However, the idea has been around for a while in different flavours. It can be traced to a 1989 paper by Hampshire and Waibel[1] which used a gating network to allocate cases to one or more expert networks. A 1990 paper by Jacobs, Jordan, Nowlan and Hinton[2] added the idea of making the gating network trainable using a supervised learning approach. \n",
    "\n",
    "By themselves, mixture of experts models are not very useful, since deep neural networks can easily learn to discriminate between training cases even without the explicit inductive bias provided by the mixture of experts architecture. However, when combined with the idea of sparse activations, they make it possible to train extremely large networks using comparatively fewer computations. This was demonstrated in the 2017 paper by Shazeer et al., [3] which used sparse mixture of experts layers to grow the size of the model while maintaining sub-linear growth in the activations.\n",
    "\n",
    "In this post, we will train a simple mixture of experts layer that learns to discriminate between different training cases. Each expert can be modeled using a feed-forward network, while the gating layer can be modeled using a softmax classifier. Sparsity is introduced using a Top-K function, which routes examples to only the K highest ranked experts for each example. However, this makes the model difficult to train, because the Top-K function is not differentiable. \n",
    "\n",
    "For now, we will avoid this problem by not using the Top-K function and smoothly allocating examples to experts based on the softmax weights. \n",
    "Mathematically, \n",
    "\n",
    "$$ \n",
    "G(x) = softmax(X, W_g)\n",
    "$$\n",
    "\n",
    "where $W_g$ is the gating network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7444476",
   "metadata": {},
   "source": [
    "### Let's start by creating a simple dataset. \n",
    "This dataset consists of two different sets of square matrices, each set having undergone a different linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "739a8f55",
   "metadata": {
    "tags": [
     "no-execute"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "D, B, C = 10, 1000, 3\n",
    "\n",
    "x = jax.random.normal(\n",
    "    jax.random.key(0),\n",
    "    (D * B, C)\n",
    ")\n",
    "\n",
    "expert_ids = (x[:, 0] > 0 ).astype(jnp.int32)\n",
    "\n",
    "t = [\n",
    "    jax.random.normal(\n",
    "        jax.random.key(1000), (C, C)\n",
    "    ),\n",
    "    jax.random.normal(\n",
    "        jax.random.key(2000), (C, C)\n",
    "    )\n",
    "]\n",
    "\n",
    "def transform(xi, ei):\n",
    "    return jnp.where(ei == 0, xi @ t[0], xi @ t[1])\n",
    "\n",
    "y = jax.vmap(lambda xi, ei: transform(xi, ei))( x, expert_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eb5424",
   "metadata": {},
   "source": [
    "### Next, let's build our model.\n",
    "Let's define the gating function, also called a router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3ff3713",
   "metadata": {
    "tags": [
     "no-execute"
    ]
   },
   "outputs": [],
   "source": [
    "import flax.nnx as nnx\n",
    "\n",
    "class Router(nnx.Module):\n",
    "    def __init__(self, dim: int, num_experts: int, *, rngs: nnx.Rngs):\n",
    "        self.w1 = nnx.Linear(dim, num_experts, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        return self.w1(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a16814",
   "metadata": {},
   "source": [
    "Next, let's define our expert as a simple single layer linear network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72279b1f",
   "metadata": {
    "tags": [
     "no-execute"
    ]
   },
   "outputs": [],
   "source": [
    "class Expert(nnx.Module):\n",
    "    def __init__(self, dim: int, *, rngs: nnx.Rngs):\n",
    "        self.linear = nnx.Linear(dim, dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e2c0ce",
   "metadata": {},
   "source": [
    "Finally, let's define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23d8ee2b",
   "metadata": {
    "tags": [
     "no-execute"
    ]
   },
   "outputs": [],
   "source": [
    "class SimpleMoE(nnx.Module):\n",
    "    def __init__(self, dim: int, *, rngs: nnx.Rngs):\n",
    "        num_experts = 2\n",
    "        self.router = Router(dim, num_experts=num_experts, rngs=rngs)\n",
    "        self.experts = [\n",
    "            Expert(dim, rngs=rngs)\n",
    "            for _ in range(num_experts)\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        gate_logits = self.router(x)       \n",
    "        expert_weights = jax.nn.softmax(gate_logits, axis=-1)\n",
    "\n",
    "        outputs = [ e(x) for e in self.experts ]\n",
    "\n",
    "        result = jnp.zeros_like(x)\n",
    "\n",
    "        for i, o in enumerate(outputs):\n",
    "            result += (o * expert_weights[:, i:i+1])\n",
    "           \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c282aa",
   "metadata": {},
   "source": [
    "### Now let's train our model\n",
    "Since this is a regression problem, we can use the mean squared error to train out model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52d88e",
   "metadata": {
    "tags": [
     "no-execute"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.5661597\n",
      "200 0.90568215\n",
      "400 0.13584565\n",
      "600 0.072314896\n",
      "800 0.05214009\n",
      "1000 0.039651874\n",
      "1200 0.031048415\n",
      "1400 0.0249528\n",
      "1600 0.020527957\n",
      "1800 0.017231295\n"
     ]
    }
   ],
   "source": [
    "import optax \n",
    "\n",
    "def loss_fn(model, x, y):\n",
    "    y_pred = model(x)\n",
    "    loss = jnp.mean((y - y_pred)**2)\n",
    "    return loss\n",
    "\n",
    "model = SimpleMoE(dim=C, rngs=nnx.Rngs(0))\n",
    "tx = optax.adam(1e-3)\n",
    "state = nnx.Optimizer(model, tx)\n",
    "\n",
    "@nnx.jit\n",
    "def step(state, x, y):\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(state.model, x, y)\n",
    "    state.update(grads)\n",
    "    return loss\n",
    "\n",
    "x = x.reshape(D, B, C)\n",
    "y = y.reshape(D, B, C)\n",
    "\n",
    "for e in range(2000):\n",
    "    for i in range(D):\n",
    "        loss = step(state, x[i], y[i])\n",
    "    if e % 200 == 0:\n",
    "        print(e, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695b7b7e",
   "metadata": {},
   "source": [
    "It works! In the next post, we'll build on this simple model and introduce the notion of sparse activations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1124c27",
   "metadata": {},
   "source": [
    "## References\n",
    "1. S. R. Hampshire and A. Waibel. \"The use of a priori knowledge in the design of a speech recognition system based on neural networks.\" In Proceedings of the International Joint Conference on Neural Networks (IJCNN), 1989.\n",
    "2.  Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. \"Adaptive Mixtures of Local Experts.\" Neural Computation, 3(1), 79â€“87, 1991.\n",
    "2. N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. \"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.\" In Proceedings of the International Conference on Learning Representations (ICLR), 2017."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
