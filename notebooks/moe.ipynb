{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3), CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7)]\n",
      "(8, 16, 4, 4)\n",
      "(8, 8, 10, 4)\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import PartitionSpec, NamedSharding, Mesh\n",
    "from jax.debug import visualize_array_sharding as viz\n",
    "\n",
    "import flax.nnx as nnx\n",
    "\n",
    "from jaxpt.modules.config import Config\n",
    "\n",
    "devices = jax.devices()\n",
    "print(devices)\n",
    "\n",
    "mesh = Mesh(devices, (\"devices\"))\n",
    "spec = PartitionSpec(None, \"devices\")\n",
    "sharding = NamedSharding(mesh, spec)\n",
    "\n",
    "@dataclass(unsafe_hash=True)\n",
    "class GLU_Config(Config):\n",
    "    top_k = 2\n",
    "    load_factor = 1.25\n",
    "    n_experts = 8\n",
    "    n_embed = 4\n",
    "    n_mlp_hidden = 6\n",
    "    mlp_bias = True\n",
    "    dtype = jax.numpy.float32\n",
    "    mesh = mesh\n",
    "\n",
    "config = GLU_Config()\n",
    "\n",
    "\n",
    "class Expert(nnx.Module):\n",
    "    def __init__(self, config, rngs):\n",
    "        init = nnx.with_partitioning(\n",
    "            nnx.initializers.normal(stddev=0.02),\n",
    "            sharding=(\"devices\",))\n",
    "\n",
    "        self.weight = nnx.Param(init(rngs.normal.key.value,\n",
    "            (\n",
    "                config.n_experts,\n",
    "                config.n_embed,\n",
    "                config.n_embed\n",
    "            )\n",
    "        ))\n",
    "    def __call__(self, x, expert_idx):\n",
    "        # Use only one expert's weights\n",
    "        w = self.weight[expert_idx]  # Slice along the expert axis\n",
    "        return x @ w  # x: [batch, in_dim], w: [in_dim, out_dim]\n",
    "\n",
    "@nnx.jit(static_argnums=(0, 1)) #, out_shardings=sharding)\n",
    "def create_sharded_model(Model, config, rngs):\n",
    "    model = Model(config=config, rngs=rngs)\n",
    "    graphdef, state = nnx.split(model) \n",
    "    pspecs = nnx.get_partition_spec(state)\n",
    "    sharded_state = nnx.with_sharding_constraint(\n",
    "        state, pspecs, mesh=config.mesh\n",
    "        )\n",
    "    nnx.update(model, sharded_state)\n",
    "    return model\n",
    "\n",
    "\n",
    "class MOE(nnx.Module):\n",
    "    def __init__(self, config: Config, rngs: nnx.Rngs):\n",
    "        self.router_gate = nnx.Linear(\n",
    "            config.n_embed,\n",
    "            config.n_mlp_hidden,\n",
    "            kernel_init=nnx.with_partitioning(\n",
    "                nnx.initializers.normal(stddev=0.02),\n",
    "                sharding=(None, None)),\n",
    "            bias_init=nnx.with_partitioning(nnx.initializers.zeros, \n",
    "            sharding=(None,)),\n",
    "            use_bias=config.mlp_bias,\n",
    "            dtype=config.dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.expert = Expert(config, rngs)        \n",
    "        #devices = jax.devices()\n",
    "        #for i in range(config.n_experts):\n",
    "        #    self.experts[i].to(devices[i])\n",
    "        self.top_k = config.top_k\n",
    "        self.n_experts = config.n_experts\n",
    "        self.load_factor = config.load_factor\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        logits = self.router_gate(x)\n",
    "        zeros = jnp.full_like(logits, float('-inf'))\n",
    "        top_k_logits, expert_indices = jax.lax.top_k(logits, self.top_k)\n",
    "        sparse_logits = jnp.put_along_axis(\n",
    "            zeros, expert_indices, top_k_logits, axis=-1, inplace=False)\n",
    "        expert_weights = jax.nn.softmax(sparse_logits, axis=-1)\n",
    "\n",
    "        tokens_per_expert = int((self.load_factor * B * T) // self.n_experts)\n",
    "        expert_inputs = jnp.zeros((self.n_experts, tokens_per_expert, C))\n",
    "        # TODO: load the tokens into the expert inputs and track order\n",
    "\n",
    "        # Gather the current expert's inputs \n",
    "        expert_inputs = jax.lax.all_to_all(expert_inputs, \"i\", 0, 0)\n",
    "        device_index = jax.lax.axis_index(\"i\")\n",
    "        expert_outputs = self.expert(expert_inputs, device_index)\n",
    "        expert_outputs = jax.lax.all_to_all(expert_outputs, \"i\", 0, 0)\n",
    "\n",
    "        # TODO: rearrange the tokens in their original shape \n",
    "\n",
    "        return expert_outputs\n",
    "\n",
    "\n",
    "@nnx.pmap(axis_name=\"i\")\n",
    "def step(x):\n",
    "    y = model(x)\n",
    "    return y\n",
    "\n",
    "B, T, C = 16, 4, 4\n",
    "   \n",
    "key = jax.random.key(0)\n",
    "rngs = nnx.Rngs(key)\n",
    "\n",
    "#model = MOE(config, rngs)\n",
    "model = create_sharded_model(MOE, config, rngs)\n",
    "x = jax.random.normal(key=key, shape=(len(devices), B, T, C))\n",
    "print(x.shape)\n",
    "y = step(x)\n",
    "print(y.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "\n",
    "class SlicedLinear(nnx.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_experts):\n",
    "        self.weight = nnx.Param(jax.random.normal(jax.random.key(0), (num_experts, in_dim, out_dim)))\n",
    "\n",
    "    def __call__(self, x, expert_idx):\n",
    "        # Use only one expert's weights\n",
    "        w = self.weight[expert_idx]  # Slice along the expert axis\n",
    "        return x @ w  # x: [batch, in_dim], w: [in_dim, out_dim]\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def step(m, x, i):\n",
    "    y = m(x, i)\n",
    "    return y\n",
    "\n",
    "\n",
    "l = SlicedLinear(4, 8, 8)\n",
    "x = jax.random.normal(jax.random.key(1), (8, 4))\n",
    "y = step(l, x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from jax.sharding import Mesh, NamedSharding, PartitionSpec\n",
    "from jax.experimental import mesh_utils\n",
    "\n",
    "# Setup mesh (1 expert per device)\n",
    "devices = mesh_utils.create_device_mesh((8,))\n",
    "mesh = Mesh(devices, axis_names=('expert',))\n",
    "\n",
    "# Dimensions\n",
    "batch_size, hidden_dim = 16, 32\n",
    "num_experts, top_k = 8, 2\n",
    "tokens_per_expert, expert_output_dim = 8, 32\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "x = random.normal(key, (batch_size, hidden_dim))\n",
    "\n",
    "# Gate routing\n",
    "logits = random.normal(random.fold_in(key, 1), (batch_size, num_experts))\n",
    "topk_vals, topk_idx = jax.lax.top_k(logits, top_k)\n",
    "gate_scores = jax.nn.softmax(topk_vals)\n",
    "\n",
    "# Allocate token buffers\n",
    "input_buf = jnp.zeros((num_experts, tokens_per_expert, hidden_dim))\n",
    "mask_buf = jnp.zeros((num_experts, tokens_per_expert), dtype=bool)\n",
    "counter = jnp.zeros((num_experts,), dtype=int)\n",
    "\n",
    "def dispatch(i, carry):\n",
    "    buf, mask, ctr = carry\n",
    "    for j in range(top_k):\n",
    "        e = topk_idx[i, j]\n",
    "        s = ctr[e]\n",
    "        buf = buf.at[e, s].set(x[i] * gate_scores[i, j])\n",
    "        mask = mask.at[e, s].set(True)\n",
    "        ctr = ctr.at[e].set(s + 1)\n",
    "    return buf, mask, ctr\n",
    "\n",
    "input_buf, mask_buf, _ = jax.lax.fori_loop(0, batch_size, dispatch, (input_buf, mask_buf, counter))\n",
    "\n",
    "# Shard tokens and weights across devices\n",
    "sharding = NamedSharding(mesh, PartitionSpec(\"expert\", None, None))\n",
    "input_sharded = jax.device_put(input_buf, sharding)\n",
    "mask_sharded = jax.device_put(mask_buf, NamedSharding(mesh, PartitionSpec(\"expert\", None)))\n",
    "\n",
    "# Create per-expert weights\n",
    "W1 = jnp.stack([random.normal(random.fold_in(key, i), (hidden_dim, expert_output_dim)) for i in range(num_experts)])\n",
    "W2 = jnp.stack([random.normal(random.fold_in(key, i+100), (expert_output_dim, hidden_dim)) for i in range(num_experts)])\n",
    "\n",
    "W1_sharded = jax.device_put(W1, NamedSharding(mesh, PartitionSpec(\"expert\", None, None)))\n",
    "W2_sharded = jax.device_put(W2, NamedSharding(mesh, PartitionSpec(\"expert\", None, None)))\n",
    "\n",
    "# MoE forward\n",
    "@jax.jit\n",
    "def moe_forward(x, mask, W1, W2):\n",
    "    def apply_expert(inputs, mask, W1, W2):\n",
    "        return jnp.where(mask[:, None], jax.nn.relu(inputs @ W1) @ W2, 0.0)\n",
    "    return jax.vmap(apply_expert, in_axes=(0, 0, 0, 0))(x, mask, W1, W2)\n",
    "\n",
    "with mesh:\n",
    "    output = moe_forward(input_sharded, mask_sharded, W1_sharded, W2_sharded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
