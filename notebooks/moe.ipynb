{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEYRLk-ZmjYB",
        "outputId": "5dda81d8-f8e7-4187-e398-ee08975747cc"
      },
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=2'\n",
        "\n",
        "import jax\n",
        "\n",
        "platform : Literal[\"darwin\", \"colab\", \"cuda\", \"tpu\"] = \"darwin\"\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    platform = \"colab\"\n",
        "except ImportError:\n",
        "    devices = jax.devices()\n",
        "    if any(d.platform == \"gpu\" for d in devices):\n",
        "        platform = \"cuda\"\n",
        "    if any(d.platform == \"tpu\" for d in devices):\n",
        "        platform = \"tpu\"\n",
        "\n",
        "print(f\"Running on {platform}\")\n",
        "\n",
        "if platform == \"colab\":\n",
        "    !git clone https://github.com/novastar53/jaxpt\n",
        "    !cd jaxpt && git checkout main && git pull\n",
        "    !pip install tiktoken datasets --quiet\n",
        "    #!pip uninstall -y tensorflow\n",
        "    !pip install tensorboard\n",
        "    !pip install -U tensorboard-plugin-profile\n",
        "\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "if platform == \"colab\":\n",
        "    jaxpt_dir = str(Path().absolute() / \"jaxpt\" / \"src\" )\n",
        "else:\n",
        "    jaxpt_dir = str(Path().absolute().parent / \"src\" )\n",
        "\n",
        "\n",
        "sys.path.append(jaxpt_dir)\n",
        "print(jaxpt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cg_rzUuOa7os",
        "outputId": "0c22e729-312d-4bb9-835b-017cc2b69a86"
      },
      "outputs": [],
      "source": [
        "\n",
        "from functools import partial\n",
        "from dataclasses import dataclass\n",
        "import random\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.sharding import PartitionSpec, NamedSharding, Mesh\n",
        "from jax.debug import visualize_array_sharding as viz\n",
        "\n",
        "import flax.nnx as nnx\n",
        "import optax\n",
        "\n",
        "from jaxpt.modules.config import Config\n",
        "#from jaxpt.utils import create_sharded_model\n",
        "\n",
        "\n",
        "devices = jax.devices()\n",
        "print(devices)\n",
        "\n",
        "mesh = Mesh(devices, (\"devices\"))\n",
        "spec = PartitionSpec(\"devices\",)\n",
        "sharding = NamedSharding(mesh, spec)\n",
        "\n",
        "@nnx.jit(static_argnums=(0, 1)) #, out_shardings=sharding)\n",
        "def create_sharded_model(Model, config, rngs):\n",
        "    model = Model(config=config, rngs=rngs)\n",
        "    graphdef, state = nnx.split(model)\n",
        "    pspecs = nnx.get_partition_spec(state)\n",
        "    sharded_state = nnx.with_sharding_constraint(\n",
        "        state, pspecs, mesh=config.mesh\n",
        "        )\n",
        "    nnx.update(model, sharded_state)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "@dataclass(unsafe_hash=True)\n",
        "class MOE_Config(Config):\n",
        "    n_layer = 1\n",
        "    top_k = 1\n",
        "    load_factor = 1.00\n",
        "    n_experts = len(devices)\n",
        "    n_embed = 3 \n",
        "    n_mlp_hidden = 6\n",
        "    mlp_bias = True\n",
        "    dtype = jax.numpy.float32\n",
        "    mesh = mesh\n",
        "\n",
        "config = MOE_Config()\n",
        "\n",
        "\n",
        "class Experts(nnx.Module):\n",
        "    def __init__(self, config, rngs):\n",
        "        w_c_fc_init = nnx.with_partitioning(\n",
        "            nnx.initializers.normal(stddev=0.02),\n",
        "            sharding=(\"devices\",))\n",
        "        \n",
        "        b_init = nnx.with_partitioning(\n",
        "            nnx.initializers.zeros,\n",
        "            sharding=(\"devices\",))\n",
        "        \n",
        "        w_c_proj_init = nnx.with_partitioning(\n",
        "            nnx.initializers.normal(stddev=0.02 * (2 * config.n_layer) ** -0.5),\n",
        "            sharding=(\"devices\",)\n",
        "        )\n",
        "\n",
        "        self.w_c_fc = nnx.Param(w_c_fc_init(rngs.default(),\n",
        "            (\n",
        "                config.n_experts,\n",
        "                config.n_embed,\n",
        "                config.n_mlp_hidden\n",
        "            )\n",
        "        ))\n",
        "        self.b_c_fc = nnx.Param(b_init(rngs.default(),\n",
        "        (\n",
        "            config.n_experts,\n",
        "            1,\n",
        "            config.n_mlp_hidden\n",
        "        )))\n",
        "\n",
        "        self.w_gate = nnx.Param(w_c_fc_init(rngs.default(),\n",
        "        (\n",
        "            config.n_experts,\n",
        "            config.n_embed,\n",
        "            config.n_mlp_hidden\n",
        "        )))\n",
        "        self.b_gate = nnx.Param(b_init(rngs.default(),\n",
        "        (\n",
        "            config.n_experts,\n",
        "            1,\n",
        "            config.n_mlp_hidden\n",
        "        )))\n",
        "\n",
        "        self.w_c_proj = nnx.Param(\n",
        "            w_c_proj_init(\n",
        "                rngs.default(),\n",
        "                (\n",
        "                    config.n_experts,\n",
        "                    config.n_mlp_hidden,\n",
        "                    config.n_embed\n",
        "                ))\n",
        "        )\n",
        "        self.b_c_proj = nnx.Param(\n",
        "            b_init(\n",
        "                rngs.default(),\n",
        "                (\n",
        "                    config.n_experts,\n",
        "                    1,\n",
        "                    config.n_embed\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = jax.lax.with_sharding_constraint(x, spec)\n",
        "        h = jnp.einsum('eti,eih->eth', x, self.w_c_fc) + self.b_c_fc\n",
        "        g = jnp.einsum('eti,eih->eth', x, self.w_gate) + self.b_gate\n",
        "        g = nnx.silu(g)\n",
        "        og = jnp.einsum('eth,eth->eth', h, g)\n",
        "        o = jnp.einsum('eth,eho->eto', og, self.w_c_proj) + self.b_c_proj\n",
        "        o = jax.lax.with_sharding_constraint(o, spec)\n",
        "        return o\n",
        "\n",
        "\n",
        "class MOE(nnx.Module):\n",
        "    def __init__(self, config: Config, rngs: nnx.Rngs):\n",
        "        self.router_gate = nnx.Linear(\n",
        "            config.n_embed,\n",
        "            config.n_experts,\n",
        "            kernel_init=nnx.with_partitioning(\n",
        "                nnx.initializers.normal(stddev=0.02),\n",
        "                sharding=(None,)),\n",
        "            bias_init=nnx.with_partitioning(nnx.initializers.zeros,\n",
        "            sharding=(None,)),\n",
        "            use_bias=config.mlp_bias,\n",
        "            dtype=config.dtype,\n",
        "            rngs=rngs,\n",
        "        )\n",
        "        self.experts = Experts(config, rngs)\n",
        "        self.top_k = config.top_k\n",
        "        self.n_experts = config.n_experts\n",
        "        self.load_factor = config.load_factor\n",
        "        self.add_noise = False\n",
        "        self.rngs = rngs\n",
        "    \n",
        "    def _get_expert_inputs(self, x, logits):\n",
        "        T, _ = logits.shape\n",
        "        _, C = x.shape\n",
        "        top_k_logits, expert_indices = jax.lax.top_k(logits, self.top_k) # T, top_K\n",
        "\n",
        "        # Swap the sequence (T) and top_k dimensions so that when the array is\n",
        "        # flattened, the higher ranked experts appear first.\n",
        "        expert_indices = jnp.swapaxes(expert_indices, 0, 1).ravel() # top_K * T\n",
        "        # Calculate the expert buffer positions all the tokens in the batch\n",
        "        expert_one_hot = jax.nn.one_hot(expert_indices, self.n_experts, dtype=jnp.int32) # top_K * T, n_experts\n",
        "        expert_positions = (jnp.cumsum(expert_one_hot, axis=0) * expert_one_hot) - 1 # top_K * T, n_experts\n",
        "        # Reshape the buffer index to match the original ordering and dimensions\n",
        "        expert_positions = expert_positions.reshape(-1, T, self.n_experts) # top_K, T, n_experts\n",
        "        expert_positions = jnp.swapaxes(expert_positions, 0, 1) # T, top_K, n_experts\n",
        "        # Extract the buffer positions for each token and k, \n",
        "        expert_positions = jnp.max(expert_positions, axis=2) # T, top_K\n",
        "        # Restore the shape and order of expert_indices\n",
        "        expert_indices = jnp.swapaxes(expert_indices.reshape(-1, T), 0, 1) # T, top_K\n",
        "\n",
        "        indices = jnp.stack([expert_indices, expert_positions], axis=-1) # T, top_K, 2\n",
        "        indices = indices.reshape(-1, 2) # \n",
        "\n",
        "        expert_capacity = self.top_k * T\n",
        "        zeros = jnp.zeros((self.n_experts, expert_capacity, C)) # n_experts, expert_cap, C\n",
        "        key = tuple(jnp.moveaxis(indices, -1, 0))\n",
        "\n",
        "        x = jnp.repeat(x, self.top_k, axis=0)\n",
        "        expert_inputs = zeros.at[key].add(x)\n",
        "        indices = indices.reshape(T, self.top_k, -1)\n",
        "\n",
        "        return top_k_logits, indices, expert_inputs\n",
        "\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, T, C = x.shape\n",
        "        logits = self.router_gate(x) # B, T, n_experts\n",
        "        #if self.add_noise:\n",
        "        #    logits += 0.01 * jax.random.normal(key=self.rngs.gate_noise(), shape=logits.shape)\n",
        "\n",
        "        top_k_logits, indices, expert_inputs = jax.vmap(\n",
        "            lambda x, l: self._get_expert_inputs(x, l))(x, logits) # B, n_experts, expert_cap, C \n",
        "        \n",
        "        top_k_logits = jax.lax.with_sharding_constraint(top_k_logits, spec)\n",
        "        indices = jax.lax.with_sharding_constraint(indices, spec)\n",
        "        expert_inputs = jax.lax.with_sharding_constraint(expert_inputs, spec)\n",
        "\n",
        "        expert_capacity = self.top_k * T\n",
        "        expert_inputs = expert_inputs.reshape(self.n_experts, -1, self.n_experts, expert_capacity, C) # n_experts, batch_per_expert, n_experts, expert_cap, C\n",
        "        expert_inputs = jnp.swapaxes(expert_inputs, 0, 2) # n_experts, batch_per_expert, n_experts, expert_cap, C\n",
        "        expert_inputs = expert_inputs.reshape(-1, C) # B * n_experts * expert_cap, C\n",
        "        expert_inputs = jax.lax.with_sharding_constraint(expert_inputs, spec)\n",
        "        expert_inputs = expert_inputs.reshape(self.n_experts, B * expert_capacity, C) # n_experts, B * expert_cap, C\n",
        "\n",
        "        #f = input_counters / (B * T)\n",
        "        #P = jnp.mean(expert_probs, axis=0)\n",
        "        #aux_loss = jnp.sum(f * P) / (self.n_experts ** 2)\n",
        "\n",
        "        expert_outputs = self.experts(expert_inputs) # n_experts, B * expert_cap, C\n",
        "        expert_outputs = expert_outputs.reshape(self.n_experts, -1, self.n_experts, expert_capacity, C) # n_experts, batch_per_expert, n_experts, expert_cap, C\n",
        "        expert_outputs = jnp.swapaxes(expert_outputs, 0, 2) # n_experts, batch_per_expert, n_experts, expert_cap, C\n",
        "        expert_outputs = expert_outputs.reshape(B, self.n_experts, expert_capacity, C) # B, n_experts, expert_capacity, C\n",
        "        expert_outputs = jax.lax.with_sharding_constraint(expert_outputs, spec)\n",
        "\n",
        "        expert_outputs = jax.vmap(\n",
        "            lambda x, i: x[i[:, :, 0], i[:, :, 1]]\n",
        "            )(expert_outputs, indices)\n",
        "\n",
        "        expert_outputs = jnp.einsum(\"BTKC,BTK->BTC\", expert_outputs, top_k_logits)       \n",
        "        expert_outputs = jax.lax.with_sharding_constraint(expert_outputs, spec)\n",
        "        return expert_outputs, 0, (None,)\n",
        "\n",
        "def loss_fn(model, x, y):\n",
        "    y_pred, aux_loss, debug_outputs = model(x)\n",
        "    loss = jnp.mean((y - y_pred)**2) + 0.01 * aux_loss\n",
        "    return loss, debug_outputs\n",
        "\n",
        "@nnx.jit\n",
        "def step(state, x, y):\n",
        "    (loss, debug_outputs), grads = nnx.value_and_grad(loss_fn, has_aux=True)(state.model, x, y)\n",
        "    state.update(grads)\n",
        "    return loss, grads, debug_outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from time import time\n",
        "\n",
        "\n",
        "with mesh:\n",
        "    D, B, T, C = 1000, len(devices), 5, config.n_embed\n",
        "\n",
        "    default = jax.random.key(69)\n",
        "    gate_noise = jax.random.key(42)\n",
        "    rngs = nnx.Rngs(default=default, gate_noise=gate_noise)\n",
        "    model = create_sharded_model(MOE, config, rngs)\n",
        "    model.train(add_noise=True)\n",
        "    tx = optax.adam(1e-3)\n",
        "    state = nnx.Optimizer(model, tx)\n",
        "\n",
        "    x = jax.random.normal(jax.random.key(1000), (D * B * T, C))\n",
        "\n",
        "    expert_ids = (x[:, 0] > 0).astype(jnp.int32)\n",
        "    t = [\n",
        "        jax.random.normal(jax.random.key(2000), (C, C)),\n",
        "        jax.random.normal(jax.random.key(3000), (C, C)),\n",
        "    ]\n",
        "    def transform(xi, eid):\n",
        "        return jnp.where(eid == 1, xi @ t[0], xi @ t[1])\n",
        "\n",
        "    y = jax.vmap(lambda xi, ei: transform(xi, ei))(x, expert_ids)\n",
        "\n",
        "    x = x.reshape(D, B, T, C)\n",
        "    y = y.reshape(D, B, T, C)\n",
        "\n",
        "    indices = list(range(D))\n",
        "\n",
        "    @nnx.jit\n",
        "    def run_model(x):\n",
        "        return model(x)\n",
        "    #with jax.profiler.trace(\"./tensorboard\"):\n",
        "    for e in range(30):\n",
        "        for i in indices:\n",
        "            start = time()\n",
        "            x_i = jax.device_put(x[i], sharding)\n",
        "            y_i = jax.device_put(y[i], sharding)\n",
        "            loss, grads, debug_outputs = step(state, x_i, y_i)\n",
        "            if i % 1000 == 0:\n",
        "                end = time()\n",
        "                iter_time = 1024 * (end - start) / 1000\n",
        "                print(f\"{e=}, {i=}, {loss.item()=}, {iter_time=:0.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./tensorboard"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
