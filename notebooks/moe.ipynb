{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3), CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7)]\n",
      "(100, 8, 4, 4, 4)\n",
      "(100, 8, 4, 4, 4)\n",
      "5.2769876\n",
      "5.228584\n",
      "6.1401753\n",
      "5.5742583\n",
      "5.296774\n",
      "5.4990735\n",
      "5.737968\n",
      "5.3874493\n",
      "5.316139\n",
      "5.5383606\n",
      "4.6514797\n",
      "5.0236506\n",
      "4.3361883\n",
      "4.92971\n",
      "5.400762\n",
      "4.3625298\n",
      "4.5210323\n",
      "4.5245695\n",
      "4.287465\n",
      "4.4217315\n",
      "4.2595153\n",
      "4.4839044\n",
      "5.130766\n",
      "4.3177276\n",
      "4.089919\n",
      "4.0769024\n",
      "4.1979055\n",
      "4.1632295\n",
      "4.0883937\n",
      "4.1476936\n",
      "4.3611608\n",
      "4.659815\n",
      "4.059251\n",
      "4.231566\n",
      "4.266568\n",
      "3.711947\n",
      "3.893794\n",
      "3.598875\n",
      "4.079007\n",
      "3.3767917\n",
      "4.339398\n",
      "4.016925\n",
      "3.7615454\n",
      "4.4278784\n",
      "3.41491\n",
      "3.4823933\n",
      "3.4663293\n",
      "3.8456528\n",
      "3.2432904\n",
      "3.550361\n",
      "3.7847192\n",
      "3.4685087\n",
      "3.5405428\n",
      "3.5235696\n",
      "3.429522\n",
      "3.6824598\n",
      "2.8983557\n",
      "3.5189874\n",
      "3.1727054\n",
      "3.2135828\n",
      "3.4057302\n",
      "3.8231404\n",
      "3.337798\n",
      "2.8719072\n",
      "3.8242698\n",
      "3.4996579\n",
      "3.6904986\n",
      "3.4353232\n",
      "2.9165292\n",
      "3.210866\n",
      "3.5098407\n",
      "3.6418605\n",
      "3.254611\n",
      "3.2964752\n",
      "2.7426922\n",
      "2.9457016\n",
      "2.6871493\n",
      "3.4652457\n",
      "2.9076848\n",
      "2.765182\n",
      "3.0485067\n",
      "3.1692548\n",
      "2.6027129\n",
      "3.195836\n",
      "2.7843525\n",
      "2.2099192\n",
      "3.509237\n",
      "3.1951985\n",
      "2.6773214\n",
      "2.6643572\n",
      "3.2879577\n",
      "3.1606574\n",
      "2.9033072\n",
      "3.0351686\n",
      "2.6591113\n",
      "2.8179512\n",
      "3.3373642\n",
      "2.6782212\n",
      "2.864456\n",
      "2.366893\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import PartitionSpec, NamedSharding, Mesh\n",
    "from jax.debug import visualize_array_sharding as viz\n",
    "\n",
    "import flax.nnx as nnx\n",
    "\n",
    "from jaxpt.modules.config import Config\n",
    "\n",
    "devices = jax.devices()\n",
    "print(devices)\n",
    "\n",
    "mesh = Mesh(devices, (\"devices\"))\n",
    "spec = PartitionSpec(None, \"devices\")\n",
    "sharding = NamedSharding(mesh, spec)\n",
    "\n",
    "@dataclass(unsafe_hash=True)\n",
    "class GLU_Config(Config):\n",
    "    top_k = 2\n",
    "    load_factor = 2.00\n",
    "    n_experts = 8\n",
    "    n_embed = 4\n",
    "    n_mlp_hidden = 6\n",
    "    mlp_bias = True\n",
    "    dtype = jax.numpy.float32\n",
    "    mesh = mesh\n",
    "\n",
    "config = GLU_Config()\n",
    "\n",
    "\n",
    "class Expert(nnx.Module):\n",
    "    def __init__(self, config, rngs):\n",
    "        init = nnx.with_partitioning(\n",
    "            nnx.initializers.normal(stddev=0.02),\n",
    "            sharding=(\"devices\",))\n",
    "\n",
    "        self.weight = nnx.Param(init(rngs.normal.key.value,\n",
    "            (\n",
    "                config.n_experts,\n",
    "                config.n_embed,\n",
    "                config.n_embed\n",
    "            )\n",
    "        ))\n",
    "    def __call__(self, x, expert_idx):\n",
    "        # Use only one expert's weights\n",
    "        w = self.weight[expert_idx]  # Slice along the expert axis\n",
    "        return x @ w  # x: [batch, in_dim], w: [in_dim, out_dim]\n",
    "\n",
    "@nnx.jit(static_argnums=(0, 1)) #, out_shardings=sharding)\n",
    "def create_sharded_model(Model, config, rngs):\n",
    "    model = Model(config=config, rngs=rngs)\n",
    "    graphdef, state = nnx.split(model) \n",
    "    pspecs = nnx.get_partition_spec(state)\n",
    "    sharded_state = nnx.with_sharding_constraint(\n",
    "        state, pspecs, mesh=config.mesh\n",
    "        )\n",
    "    nnx.update(model, sharded_state)\n",
    "    return model\n",
    "\n",
    "\n",
    "class MOE(nnx.Module):\n",
    "    def __init__(self, config: Config, rngs: nnx.Rngs):\n",
    "        self.router_gate = nnx.Linear(\n",
    "            config.n_embed,\n",
    "            config.n_mlp_hidden,\n",
    "            kernel_init=nnx.with_partitioning(\n",
    "                nnx.initializers.normal(stddev=0.02),\n",
    "                sharding=(None, None)),\n",
    "            bias_init=nnx.with_partitioning(nnx.initializers.zeros, \n",
    "            sharding=(None,)),\n",
    "            use_bias=config.mlp_bias,\n",
    "            dtype=config.dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.expert = Expert(config, rngs)        \n",
    "        self.top_k = config.top_k\n",
    "        self.n_experts = config.n_experts\n",
    "        self.load_factor = config.load_factor\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x_flat = x.reshape(-1, C)\n",
    "        logits = self.router_gate(x_flat) # B * T, n_experts\n",
    "        zeros = jnp.full_like(logits, float('-inf')) # B * T, n_experts\n",
    "        top_k_logits, expert_indices = jax.lax.top_k(logits, self.top_k) # B * T, top_k\n",
    "\n",
    "        max_tokens_per_expert = int((self.load_factor * B * T) // self.n_experts)\n",
    "        expert_inputs = jnp.zeros((self.n_experts, max_tokens_per_expert, C))\n",
    "        expert_mask = jnp.zeros((self.n_experts, max_tokens_per_expert))\n",
    "        counters = jnp.zeros((self.n_experts,), dtype=jnp.uint8)\n",
    "\n",
    "        def update_expert_inputs(i, carry):\n",
    "            expert_inputs, expert_mask, counters = carry\n",
    "            for j in range(self.top_k):\n",
    "                expert_idx = expert_indices[i, j]\n",
    "                token_pos = counters[expert_idx]\n",
    "                expert_inputs = expert_inputs.at[expert_idx, token_pos].set(x_flat[i])\n",
    "                expert_mask = expert_mask.at[expert_idx, token_pos].set(1)\n",
    "                counters = counters.at[expert_idx].add(1)\n",
    "\n",
    "            return expert_inputs, expert_mask, counters\n",
    "        \n",
    "        expert_inputs, expert_mask, _ = jax.lax.fori_loop(\n",
    "            0, B * T, update_expert_inputs, (\n",
    "                expert_inputs,\n",
    "                expert_mask,\n",
    "                counters\n",
    "            )\n",
    "        )\n",
    "            \n",
    "        # Gather the current expert's inputs \n",
    "        expert_inputs = jax.lax.all_to_all(expert_inputs, \"i\", 0, 0)\n",
    "        # Run the current expert on the gathered inputs\n",
    "        device_index = jax.lax.axis_index(\"i\")\n",
    "        #expert_outputs = expert_inputs @ jnp.eye(C, C)\n",
    "        expert_outputs = self.expert(expert_inputs, device_index)\n",
    "        # Redistribute the outputs back to the devices of origin\n",
    "        expert_outputs = jax.lax.all_to_all(expert_outputs, \"i\", 0, 0)\n",
    "\n",
    "        y = jnp.zeros_like(x_flat)\n",
    "        def update_expert_outputs(i, carry):\n",
    "            y, counters = carry\n",
    "            for j in range(self.top_k):\n",
    "                expert_idx = expert_indices[i, j]\n",
    "                token_pos = counters[expert_idx]\n",
    "                y = y.at[i].set(expert_outputs[expert_idx, token_pos])\n",
    "                counters = counters.at[expert_idx].add(1)\n",
    "\n",
    "            return y, counters\n",
    "\n",
    "        y, _ = jax.lax.fori_loop(\n",
    "            0, B * T, update_expert_outputs, (\n",
    "                y,\n",
    "                counters\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return y.reshape(B, T, C)\n",
    "\n",
    "def loss_fn(model, x, y):\n",
    "    y_pred = model(x)\n",
    "    loss = jnp.mean((y_pred - y)**2)\n",
    "    return loss\n",
    "\n",
    "@nnx.pmap(axis_name=\"i\", in_axes=(None, 0, 0), out_axes=(0))\n",
    "def step(model, x, y):\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model, x, y)\n",
    "    state = nnx.state(model)\n",
    "    state = jax.tree_map(\n",
    "        lambda param, g: param - 0.01 * g,\n",
    "        state, grads,\n",
    "        is_leaf=lambda x: isinstance(x, nnx.Param)\n",
    "    )\n",
    "    nnx.update(model, state)\n",
    "    loss = jax.lax.pmean(loss, axis_name=\"i\")\n",
    "    return loss\n",
    "\n",
    "D, B, T, C = 100, 4, 4, 4\n",
    "   \n",
    "key = jax.random.key(0)\n",
    "rngs = nnx.Rngs(key)\n",
    "\n",
    "#model = MOE(config, rngs)\n",
    "model = create_sharded_model(MOE, config, rngs)\n",
    "x = jax.random.normal(key=key, shape=(D, len(devices), B, T, C))\n",
    "print(x.shape)\n",
    "t = jax.random.normal(key=key, shape=(C, C))\n",
    "y = x @ t\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    loss = step(model, x[i], y[i])\n",
    "    print(loss[0])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
